library(quanteda)
library(readtext)
readtext("/Users/ekb5/Corpora/Saints/txt/")
library(tidyverse)
readtext("/Users/ekb5/Corpora/Saints/txt/") %>% corpus()
readtext("/Users/ekb5/Corpora/Saints/txt/") %>% corpus() -> saints
saints
readtext("/Users/ekb5/Corpora/Saints/txt/Volume01") %>% corpus() -> saints01
saints01
for (i in 1:3) {
}
to_str
for (i in 1:3) {
to_str <- str_glue("saints0{i} <- readtext('/Users/ekb5/Corpora/Saints/txt/Volume0{i}') %>% corpus()")
}
to_str
for (i in 1:3) {
to_str <- str_glue("saints0{i} <- readtext('/Users/ekb5/Corpora/Saints/txt/Volume0{i}') %>% corpus()")
eval(parse(text = to_str))
}
46+44+39
saints01+saints02+saints03
for (i in 1:3) {
to_str <- str_glue("saints0{i} <- readtext('/Users/ekb5/Corpora/Saints/txt/Volume0{i}') %>% corpus(); docvars(saints0{i}, 'Volume') <- '0{i}'")
eval(parse(text = to_str))
}
saints01
summary(saints01)
saints01+saints02+saints03
saints01+saints02+saints03 %>% summary
summary(saints01+saints02+saints03 )
saints <- saints01 + saints02 + saints03
saints %>%
tokens() %>%
kwic(pattern = "tree.*", valuetype = "regex")
saints %>%
tokens() %>%
kwic(pattern = "tree.*", valuetype = "regex") %>%
view()
saints %>%
tokens() %>%
kwic(pattern = "\\btree.*\\b", valuetype = "regex") %>%
view()
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{2,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
view()
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
view()
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
write_csv(file="/Users/ekb5/Downloads/delete.csv")
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
view()
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex")
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
names()
docvars(saints)
summary(saints)
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>% names()
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
left_join(summary(saints), by = c("docname" = "Text"))
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
left_join(summary(saints), by = c("docname" = "Text")) %>%
view
summary(saints)
nrow(saints)
ndoc(saints)
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
left_join(summary(saints, n = ndoc(saints)), by = c("docname" = "Text")) %>%
view
install.packages(c("quanteda", "quanteda.textmodels", "quanteda.textstats", "quanteda.textplots", "readtext", "spacyr"))
install.packages(c("quanteda", "quanteda.textmodels", "quanteda.textstats", "quanteda.textplots", "readtext", "spacyr"))
install.packages('remotes')
?install.packages
library(quanteda)
library(readtext)
library(tidyverse)
readtext("/Users/ekb5/Corpora/CAES/")
?readtext
readtext("/Users/ekb5/Corpora/Leyes_mexicanas/doc//")
filenames <- dir(path = "/Users/ekb5/Corpora/Leyes_mexicanas/doc/", pattern = "\\.docx$", full.names = T)
filenames
filenames <- dir(path = "/Users/ekb5/Corpora/Leyes_mexicanas/doc", pattern = "\\.docx$", full.names = T)
readtext(file = filenames)
readtext(file = filenames) %>% corpus() -> leyes
summary(leyes)
kwic(leyes, pattern = "libros?", valuetype = "regex")
?readtext
readtext(file = "/Users/ekb5/Corpora/Saints/txt/", docvarsfrom = "filepaths") %>% corpus() -> asdf
asdf
summary(asdf)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/", docvarsfrom = "filepaths", dvsep = "Volume\\d{2}") %>% corpus() -> asdf
readtext(file = "/Users/ekb5/Corpora/Saints/txt/", docvarsfrom = "filepaths", dvsep = "Volume\\d{2}") %>% corpus() -> asdf; summary(asdf)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "Volume\\d{2}") %>% corpus() -> asdf; summary(asdf)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*.txt", docvarsfrom = "filepaths", dvsep = "Volume\\d{2}") %>% corpus() -> asdf; summary(asdf)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "Volume(?=\\d{2})") %>% corpus() -> asdf; summary(asdf)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "\\") %>% corpus() -> asdf; summary(asdf)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "/") %>% corpus() -> asdf; summary(asdf)
adsf %>%
select(docvar7)
asdf %>%
select(docvar7)
class(asdf)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "/")
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "/") %>% select(docvar7)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "/")
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "/") %>% select(doc_id, text, docvar7)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "/") %>% select(doc_id, text, docvar7) %>% corpus()
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "/") %>% select(doc_id, text, docvar7) %>% corpus() %>% summary
setwd("/Users/ekb5/Corpora/Saints/txt")
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/") %>% select(doc_id, text, docvar7) %>% corpus()
readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/")
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/", docvarnames = c("Volume", "filename")) %>% select(doc_id, text, docvar7) %>% corpus()
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/", docvarnames = c("Volume", "filename")) %>%  corpus()
saints
summary(saints)
setwd("/Users/ekb5/Corpora/Saints/txt")
for (i in 1:3) {
to_str <- str_glue("saints0{i} <- readtext('/Volume0{i}') %>% corpus(); docvars(saints0{i}, 'Volume') <- '0{i}'")
eval(parse(text = to_str))
}
for (i in 1:3) {
to_str <- str_glue("saints0{i} <- readtext('/Volume0{i}/*.txt') %>% corpus(); docvars(saints0{i}, 'Volume') <- '0{i}'")
eval(parse(text = to_str))
}
for (i in 1:3) {
to_str <- str_glue("saints0{i} <- readtext('Volume0{i}/*.txt') %>% corpus(); docvars(saints0{i}, 'Volume') <- '0{i}'")
eval(parse(text = to_str))
}
saints01
saints <- saints01 + saints02 + saints03
saints
saints
corpus_subset(saints, Volume == "Volume01")
summary(saints)
corpus_subset(saints, Volume == "01")
corpus_subset(saints, Volume == "01") %>% corpus
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/", docvarnames = c("Volume", "filename")) %>%  corpus()
library(quanteda)
library(readtext)
library(tidyverse)
setwd("/Users/ekb5/Corpora/Saints/txt")
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/", docvarnames = c("Volume", "filename")) %>%  corpus()
saints
saints %>%
tokens()
library(quanteda.textstats)
saints %>%
tokens() %>%
textstat_collocations()
?textstat_collocations
saints %>%
tokens() %>%
textstat_collocations()  %>%
arrange(desc(lambda))
saints %>%
tokens() %>%
textstat_collocations()  %>%
arrange(desc(lambda)) %>%
head(10)
saints %>%
tokens() %>%
textstat_collocations()  %>%
arrange(desc(lambda)) %>%
ggplot(aes(lambda, z)) +
geom_point()+
geom_smooth(method=lm)
saints %>%
tokens() %>%
textstat_collocations()  %>%
arrange(desc(z))
saints %>%
tokens() %>%
tokens_remove(stopwords("en")) %>%
textstat_collocations() %>%
arrange(desc(lambda))
?textstat_collocations()
?textstat_collocations
saints %>%
tokens() %>%
textstat_collocations()  %>%
arrange(desc(z))
dictionary(list(temple = c("temple", "sealing", "spire", "open house"),
missionary = c("missionary", "mission", "labor")))
dict <- dictionary(list(temple = c("temple", "sealing", "spire", "open house"),
missionary = c("missionary", "mission", "labor")))
kwic(saints %>% tokens(), pattern = dict)
kwic(saints %>% tokens(), pattern = dict, valuetype = "regex")
library("quanteda.textstats")
dfmat_inaug_post1980 <- corpus_subset(data_corpus_inaugural, Year > 1980) |>
tokens(remove_punct = TRUE) |>
tokens_wordstem(language = "en") |>
tokens_remove(stopwords("en")) |>
dfm()
tstat_obama <- textstat_simil(dfmat_inaug_post1980,
dfmat_inaug_post1980[c("2009-Obama", "2013-Obama"), ],
margin = "documents", method = "cosine")
as.list(tstat_obama)
dotchart(as.list(tstat_obama)$"2013-Obama", xlab = "Cosine similarity", pch = 19)
data_corpus_sotu <- readRDS(url("https://quanteda.org/data/data_corpus_sotu.rds"))
dfmat_sotu <- corpus_subset(data_corpus_sotu, Date > as.Date("1980-01-01")) |>
tokens(remove_punct = TRUE) |>
tokens_wordstem(language = "en") |>
tokens_remove(stopwords("en")) |>
dfm()
dfmat_sotu <- dfm_trim(dfmat_sotu, min_termfreq = 5, min_docfreq = 3)
tstat_dist <- dfmat_sotu |>
dfm_weight(scheme = "prop") |>
textstat_dist()
# hiarchical clustering the distance object
pres_cluster <- hclust(as.dist(tstat_dist))
# label with document names
pres_cluster$labels <- docnames(dfmat_sotu)
# plot as a dendrogram
plot(pres_cluster, xlab = "", sub = "",
main = "Euclidean Distance on Normalized Token Frequency")
period <- ifelse(docvars(data_corpus_inaugural, "Year") < 1945, "pre-war", "post-war")
dfmat1 <- dfm(data_corpus_inaugural, groups = period)
head(dfmat1) # make sure 'post-war' is in the first row
dfmat1 <- dfm(tokens(data_corpus_inaugural), groups = period)
dfmat1 <- dfm(tokens(data_corpus_inaugural), dfm_group = period)
dfmat1 <- dfm(tokens(data_corpus_inaugural), dfm_group(period))
dfm_group()
?dfm_group()
dfmat1 <- dfm(tokens(data_corpus_inaugural) %>% dfm_group(groups = period)
)
dfm(tokens(data_corpus_inaugural)
dfm(tokens(data_corpus_inaugural))
data_corpus_inaugural
tokens(data_corpus_inaugural)
dfm(tokens(data_corpus_inaugural))
dfmat1 <- dfm(tokens(data_corpus_inaugural)) %>% dfm_group(groups = period))
dfmat1 <- dfm(tokens(data_corpus_inaugural)) %>% dfm_group(groups = period)
head(dfmat1) # make sure 'post-war' is in the first row
data_corpus_inaugural
docvars(data_corpus_inaugural)
period <- ifelse(docvars(data_corpus_inaugural, "Year") < 1945, "pre-war", "post-war")
period
dfm(tokens(data_corpus_inaugural))
dfmat1 <- dfm(tokens(data_corpus_inaugural)) %>% dfm_group(groups = period)
head(dfmat1) # make sure 'post-war' is in the first row
?textstat_keyness
library(rves)
library(rvest)
page <- read_html("https://eur-lex.europa.eu/legal-content/ES/TXT/HTML/?uri=CELEX:12016M/TXT")
page %>%
html_text2()
page %>%
html_text2() %>%
write_lines(file = "/Users/ekb5/Downloads/eu_leyes.txt")
2709/3
sessionInfo()
10^-15.005441665649414
install.packages("nycflights13")
library("nycflights13")
flights
flights %>% view
library(tidyverse)
flights %>% view
flights
1301/60
60*24
library(nycflights13)
flights
flights %>% view
library(tidyverse)
flights %>% view
library(nycflights13)
names(flights)
library(tidyverse)
?select
library("nycflights13")
flights
flights |>
summarize(mean(dep_delay))
library("tidyverse")
flights |>
summarize(mean(dep_delay))
clear()
flights |>
filter(!is.na(dep_delay)) |>
group_by(carrier) |>
summarize(mean(dep_delay))
library(nycflights13)
flights |> names
flights |> names()
library("readxl")
library("tidyverse")
setwd("/Users/ekb5/Documents/data_analysis/datasets/")
sibilants <- read_excel("data_FRC_spch_rate.xlsx", sheet = "data")
sibilants
sibilants |> pull(spch_rate)
?left_join
?separate
?unite
x <- "Bobby: I'm hungry!"
str_extract(x, "^.*?:")
str_extract(x, "^.*?(?!:)")
str_extract(x, ".*?(?!:)")
str_extract(x, ".*?")
str_extract(x, "^.*?")
str_extract(x, "^.*")
str_extract(x, "^[^:]+")
str_extract(x, "[^:]+4")
str_extract(x, "[^:]+$")
?unite
0.6436^4
0.3564 + 0.3564 - (0.3564 * 0.3564)
library("readxl")
library("tidyverse")
setwd("/Users/ekb5/Downloads/")
sibilants <- read_excel("data_L2s.xlsx", sheet = "data_hls_2017-10-25")
sibilants
?pivot_longer
library("tidyverse")
?pivot_longer
?tribble
library(tidyverse)
?pivot_wider
#| eval: true
library("tidyverse")
library("readxl")
rpm <- read_excel("/Users/ekb5/Documents/data_analysis/datasets/RPM_2019_for_Reliability.xlsx", sheet = "For Earl")
rpm |>
pivot_wider(names_from = "Page/Item", values_from = "Correct/incorrect")
library(readxl)
?readxl
?readxl()
rpm
tail(rpm)
rpm <- read_excel("/Users/ekb5/Documents/data_analysis/datasets/RPM_2019_for_Reliability.xlsx", sheet = "For Earl", range = "For Earl!A1:E3745")
rpm <- read_excel("/Users/ekb5/Documents/data_analysis/datasets/RPM_2019_for_Reliability.xlsx", range = "For Earl!A1:E3745")
rpm
tail(rpm)
rpm <- read_excel("/Users/ekb5/Documents/data_analysis/datasets/RPM_2019_for_Reliability.xlsx", range = "For Earl!A1:E3745")
rpm |>
pivot_wider(names_from = "Page/Item", values_from = "Correct/incorrect")
rpm
rpm |>
pivot_wider(names_from = "Page/Item", values_from = "Correct/incorrect")
?rep
rep(1:104, times = 36)
rep(1:104, length.out = 36)
rep_len(1:104, length.out = 36)
rep(1:104, each. = 36)
rep(1:104, each = 36)
rpm |>
mutate(id = rep(1:104, each = 36)) |>
pivot_wider(names_from = "Page/Item", values_from = "Correct/incorrect")
rpm |>
mutate(id = rep(1:104, each = 36)) |>
pivot_wider(names_from = "Page/Item", values_from = "Correct/incorrect") |>
relocate(id, .before = 1)
rpm |>
mutate(id = rep(1:104, each = 36)) |>
pivot_wider(names_from = "Page/Item", values_from = "Correct/incorrect") |>
relocate(id, .before = 1) |> view()
library(tidyverse)
library(joeystanley)
install.packages(joeystanley)
install.packages('joeystanley')
f <- factor(c("a", "b", "c", "d"), levels = c("b", "c", "d", "a"))
levels(f)
?pivot_wider
?pivot_longer
vowels
library(tidyverse)
?geom_mosaic
??geom_mosaic
q1=1970342.3066
q3=2135475.4842
iqr=q3-q1
iqr
q1
q3
q3+(1.5*iqr)
0.86^2
36.6/40
install.packages("ggmosaic")
#| eval: false
library("tidyverse")
library("readxl")
ptk <- read_excel("../../../data_analysis/datasets/Data_ptk.xlsx", sheet = "data")
glimpse(ptk)
head(ptk)
# install.packages("devtools")
devtools::install_github("haleyjeppson/ggmosaic")
install.packages("devtools")
# install.packages("devtools")
devtools::install_github("haleyjeppson/ggmosaic")
ptk
ptk |> ggplot(aes(x = LANG, fill = GENRE))+geom_mosaic()
library("ggmosaic")
ptk |> ggplot(aes(x = LANG, fill = GENRE))+geom_mosaic()
ptk |> ggplot()+geom_mosaic(aes(x = LANG, fill = GENRE))
ptk |> ggplot()+geom_mosaic(aes(x = LANG, fill = prePhonBin))
#| eval: false
ptk |>
ggplot(aes(x = LANG, fill=prePhonBin))+
geom_bar(position = "fill")
ptk |>
ggplot(aes(x = LANG, fill=prePhonBin))+
geom_bar(position = "fill")
ptk |> ggplot()+geom_mosaic(aes(x = LANG, fill = prePhonBin))
ptk |> ggplot()+geom_mosaic(aes(x = GENRE, fill = prePhonBin))
ptk |> ggplot()+geom_mosaic(aes(x = prePhonBin, fill = LANG))
ptk |> ggplot()+geom_mosaic(aes(x = prePhonBin, fill = LANG))
#| eval: false
library("tidyverse")
library("readxl")
ptk <- read_excel("../../../data_analysis/datasets/Data_ptk.xlsx", sheet = "data")
glimpse(ptk)
head(ptk)
ptk |>
ggplot()+
geom_mosaic(aes(x = prePhonBin, fill = LANG))
library("ggmosaic")
ptk |>
ggplot()+
geom_mosaic(aes(x = prePhonBin, fill = LANG))
ptk |>
ggplot()+
geom_mosaic(aes(x = prePhonBin, fill = LANG))
ptk |>
ggplot()+
geom_mosaic(aes(x = prePhonBin, fill = LANG))
ptk |> pull(prePhonBin)
ptk |>
ggplot(aes(x = prePhonBin, fill = LANG))+
geom_mosaic()
ptk %>%
ggplot(aes(x = prePhonBin, fill = LANG))+
geom_mosaic()
library("tidyverse")
ptk %>%
ggplot(mapping = aes(x = prePhonBin, fill = LANG))+
geom_mosaic()
ptk %>%
ggplot()+
geom_mosaic(mapping = aes(x = prePhonBin, fill = LANG))
library("ggmosaic")
# ptk |>
#   ggplot()+
#   geom_mosaic(aes(x = prePhonBin, fill = LANG))
ptk %>%
ggplot()+
geom_mosaic(mapping = aes(x = prePhonBin, fill = LANG))
see.m <- c(44,48,27,17)
see.nm <- c(26,135,98,19)
rbind(see.m, see.nm)
temp <- ptk |>
count(prePhonBin, LANG) |>
pivot_wider(names_from = LANG, values_from = n)
temp
temp |> data.matrix()
row_names
temp <- ptk |>
count(prePhonBin, LANG) |>
pivot_wider(names_from = LANG, values_from = n)
row_names <- temp |> pull(1)
temp |> data.matrix()[,2:3]
temp <- ptk |>
count(prePhonBin, LANG) |>
pivot_wider(names_from = LANG, values_from = n)
row_names <- temp |> pull(1)
temp %>% data.matrix()[,2:3]
temp
row_names
temp_m <- temp |> data.matrix()
temp_m
temp_m[,-1]
temp |> data.matrix()[,-1)
temp |> data.matrix()[,-1]
temp_m[,-1]
#| eval: true
install.packages("devtools")
devtools::install_github("joeystanley/joeysvowels")
install.packages("devtools")
#| eval: true
library("tidyverse")
midpoints <- joeysvowels::coronals
glimpse(midpoints)
head(midpoints)
midpoints <- midpoints|>
filter(percent == 50)  |>
select(-percent) |>
filter(!vowel %in% c("PRICE", "MOUTH", "CHOICE"))
midpoints |>
ggplot(aes(x = F2, y = F1))+
geom_point()
96/3
library(tidyverse)
?stat_summary
?ggproto
