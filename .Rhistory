dir(path="/Users/ekb5/Corpora/Saints/txt", pattern = "\\.txt$", full.names = TRUE, recursive = TRUE)
wds <- "I love to see the temple I'm going there someday"
wds %>% str_split(" ")
library("tidyverse")
wds %>% str_split(" ")
wds %>% str_split(" ") %>% unlist()
wds %>% str_split(" ") %>% unlist() -> wds
wds
tibble(wd = wds)
tibble(wd = wds) %>% count(wd)
wds <- "I love to see the temple I'm going there someday I love to see my wife in the temple"
wds %>% str_split(" ") %>% unlist() -> wds
wds
tibble(wd = wds)
tibble(wd = wds) %>% count(wd)
tibble(wd = wds) %>% count(wd) %>% arrange(desc(wd))
tibble(wd = wds) %>% count(wd) %>% arrange(desc(n))
tibble(wd = wds) %>% count(name = "freq")
tibble(wd = wds) %>% count(wd)
tibble(wd = wds) %>% count(wd) %>% arrange(desc(n))
tibble(wd = wds) %>% count(wd) %>% arrange(desc(n)) %>% rename(freq = n)
tibble(wd = wds) %>% count(wd) %>% arrange(desc(n)) %>% rename(freq = n) %>% write_csv(file = "/Users/ekb5/Downloads/freqs.csv")
names = c("Mouse", "Kiki", "Lynx")
cat_name = readline(prompt = "Enter a name for a cat : ");
good_cat_name = function(names) {
if(cat_name %in% names == T) { print("thats a good name for a cat")
}
else {
print("yikes, I can think of better names")}
}
good_cat_name(cat_name)
good_cat_name = function(names) {
if(cat_name %in% names) { print("thats a good name for a cat")
}
else {
print("yikes, I can think of better names")}
}
good_cat_name(cat_name)
cat_name
names
cat_name %in% names
good_cat_name = function(names) {
if(cat_name %in% names) { print("thats a good name for a cat")
}
else {
print("yikes, I can think of better names")}
}
good_cat_name(cat_name)
cat_name
names
good_cat_name = function(names) {
if(cat_name %in% names) {
print("thats a good name for a cat")
} else {
print("yikes, I can think of better names")
}
}
cat_name
good_cat_name(cat_name)
good_cat_name = function(cat_name, names) {
if(cat_name %in% names) {
print("thats a good name for a cat")
} else {
print("yikes, I can think of better names")
}
}
names = c("Mouse", "Kiki", "Lynx")
cat_name = readline(prompt = "Enter a name for a cat : ");
if(cat_name %in% names == T) { print("thats a good name for a cat")
}
else {
good_cat_name(cat_name)
cat_name %in% names == T
good_cat_name = function(names) {
if(cat_name %in% names == T) { print("thats a good name for a cat")
}
else {
print("yikes, I can think of better names")}
}
good_cat_name(cat_name)
source("~/.active-rstudio-document", echo=TRUE)
names
cat_name
good_cat_name = function(cat_name) {
if(cat_name %in% names == T) { print("thats a good name for a cat")
}
else {
print("yikes, I can think of better names")}
}
good_cat_name(cat_name)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
library("tidyverse")
?table
x=""
x=str_c(x, "asdf")
x
x=str_c(x, "fdsa")
x
x=str_c(x, "erty", sep = " ")
x
install.packages("stylo")
library(stylo)
clear
clear()
?html_table
?read_html
httr::set_config(httr::user_agent("me@example.com; +https://example.com/info.html"))
library("tidyverse")
?separate
library("tidyverse")
df <- tibble(one = c("Earl-Brown", "Kristi–Brown"))
df %>% separate_wider_delim(one, "-")
df %>% separate_wider_delim(one, "-", names = c("first", "second"))
df %>% separate_wider_delim(one, regex("[\\-\\–]"), names = c("first", "second"))
install.packages("quanteda")
install.packages("quanteda.textmodels")
install.packages("quanteda.textstats")
install.packages("quanteda.textplots")
install.packages("readtext")
install.packages("spacyr")
install.packages("quanteda.corpora")
version
remotes::install_github("kbenoit/quanteda.dictionaries")
install.packages("remotes")
remotes::install_github("kbenoit/quanteda.dictionaries")
library("quanteda")
data_char_ukimmig2010
data_char_ukimmig2010 %>% class()
data_char_ukimmig2010 %>% length()
corp_uk <- corpus(data_char_ukimmig2010)
corp_uk
library("require")
library("readtext")
readtext("social_media/zombies/tweets.json")
require(readtext)
readtext("social_media/zombies/tweets.json")
print(data_char_ukimmig2010)
print(corp_uk)
as.character(corp_uk)[1]
summary(corp_uk)
summary(corp_uk, n=4)
28*1.5
42+112
Name = c("Cal", "Ellie", "Ben")
Language = c("English", "English", "Dog")
# A numeric vector
Age = c(21, 20, 4)
# creates dataframe
df = data.frame(Name, Language, Age)
#prints dataframe
df
# creating vectors of information about Disney princesses
princesses <- c("Cinderella", "Belle", "Jasmine", "Sleeping Beauty")
hair_color <- c("blonde", "brunette", "black", "blonde")
dress_color <- c("blue", "yellow", "turquoise", "pink")
#create a data frame with the information about the Disney princesses
princess_data <- data.frame(Name = princesses, Hair = hair_color, Dress = dress_color)
# create a vector of more princesses
all_princesses <- c("Cinderella", "Belle", "Jasmine", "Sleeping Beauty", "Ariel", "Rapunzel", "Snow White", "Elsa", "Tiana", "Meg", "Mulan")
# Create empty vectors to store results
result1 <- character(length(all_princesses))
result2 <- logical(length(all_princesses))
#create a function that checks to see if a princess is in the list of all princesses
Check_princess <- function(princess){
#check princess in princess vector
result <- princess %in% princesses
return(result)
}
# Iterate through all_princesses vector and check for princesses
for (i in seq_along(all_princesses)) {
result1[i] <- ifelse(all_princesses[i] == "Cinderella", "Yay, Kyra's favorite!", "Not Kyra's favorite")
result2[i] <- Check_princess(all_princesses[i])
}
i
all_princesses[i]
all_princesses[i] == "Cinderella", "Yay, Kyra's favorite!", "Not Kyra's favorite"
library(quanteda)
library(readtext)
readtext("/Users/ekb5/Corpora/Saints/txt/")
library(tidyverse)
readtext("/Users/ekb5/Corpora/Saints/txt/") %>% corpus()
readtext("/Users/ekb5/Corpora/Saints/txt/") %>% corpus() -> saints
saints
readtext("/Users/ekb5/Corpora/Saints/txt/Volume01") %>% corpus() -> saints01
saints01
for (i in 1:3) {
}
to_str
for (i in 1:3) {
to_str <- str_glue("saints0{i} <- readtext('/Users/ekb5/Corpora/Saints/txt/Volume0{i}') %>% corpus()")
}
to_str
for (i in 1:3) {
to_str <- str_glue("saints0{i} <- readtext('/Users/ekb5/Corpora/Saints/txt/Volume0{i}') %>% corpus()")
eval(parse(text = to_str))
}
46+44+39
saints01+saints02+saints03
for (i in 1:3) {
to_str <- str_glue("saints0{i} <- readtext('/Users/ekb5/Corpora/Saints/txt/Volume0{i}') %>% corpus(); docvars(saints0{i}, 'Volume') <- '0{i}'")
eval(parse(text = to_str))
}
saints01
summary(saints01)
saints01+saints02+saints03
saints01+saints02+saints03 %>% summary
summary(saints01+saints02+saints03 )
saints <- saints01 + saints02 + saints03
saints %>%
tokens() %>%
kwic(pattern = "tree.*", valuetype = "regex")
saints %>%
tokens() %>%
kwic(pattern = "tree.*", valuetype = "regex") %>%
view()
saints %>%
tokens() %>%
kwic(pattern = "\\btree.*\\b", valuetype = "regex") %>%
view()
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{2,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
view()
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
view()
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
write_csv(file="/Users/ekb5/Downloads/delete.csv")
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
view()
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex")
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
names()
docvars(saints)
summary(saints)
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>% names()
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
left_join(summary(saints), by = c("docname" = "Text"))
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
left_join(summary(saints), by = c("docname" = "Text")) %>%
view
summary(saints)
nrow(saints)
ndoc(saints)
saints %>%
tokens() %>%
kwic(pattern = phrase("\\w+(\\w{3,})\\W+\\w+\\1\\b"), valuetype = "regex") %>%
left_join(summary(saints, n = ndoc(saints)), by = c("docname" = "Text")) %>%
view
install.packages(c("quanteda", "quanteda.textmodels", "quanteda.textstats", "quanteda.textplots", "readtext", "spacyr"))
install.packages(c("quanteda", "quanteda.textmodels", "quanteda.textstats", "quanteda.textplots", "readtext", "spacyr"))
install.packages('remotes')
?install.packages
library(quanteda)
library(readtext)
library(tidyverse)
readtext("/Users/ekb5/Corpora/CAES/")
?readtext
readtext("/Users/ekb5/Corpora/Leyes_mexicanas/doc//")
filenames <- dir(path = "/Users/ekb5/Corpora/Leyes_mexicanas/doc/", pattern = "\\.docx$", full.names = T)
filenames
filenames <- dir(path = "/Users/ekb5/Corpora/Leyes_mexicanas/doc", pattern = "\\.docx$", full.names = T)
readtext(file = filenames)
readtext(file = filenames) %>% corpus() -> leyes
summary(leyes)
kwic(leyes, pattern = "libros?", valuetype = "regex")
?readtext
readtext(file = "/Users/ekb5/Corpora/Saints/txt/", docvarsfrom = "filepaths") %>% corpus() -> asdf
asdf
summary(asdf)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/", docvarsfrom = "filepaths", dvsep = "Volume\\d{2}") %>% corpus() -> asdf
readtext(file = "/Users/ekb5/Corpora/Saints/txt/", docvarsfrom = "filepaths", dvsep = "Volume\\d{2}") %>% corpus() -> asdf; summary(asdf)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "Volume\\d{2}") %>% corpus() -> asdf; summary(asdf)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*.txt", docvarsfrom = "filepaths", dvsep = "Volume\\d{2}") %>% corpus() -> asdf; summary(asdf)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "Volume(?=\\d{2})") %>% corpus() -> asdf; summary(asdf)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "\\") %>% corpus() -> asdf; summary(asdf)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "/") %>% corpus() -> asdf; summary(asdf)
adsf %>%
select(docvar7)
asdf %>%
select(docvar7)
class(asdf)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "/")
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "/") %>% select(docvar7)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "/")
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "/") %>% select(doc_id, text, docvar7)
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "/") %>% select(doc_id, text, docvar7) %>% corpus()
readtext(file = "/Users/ekb5/Corpora/Saints/txt/*/*.txt", docvarsfrom = "filepaths", dvsep = "/") %>% select(doc_id, text, docvar7) %>% corpus() %>% summary
setwd("/Users/ekb5/Corpora/Saints/txt")
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/") %>% select(doc_id, text, docvar7) %>% corpus()
readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/")
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/", docvarnames = c("Volume", "filename")) %>% select(doc_id, text, docvar7) %>% corpus()
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/", docvarnames = c("Volume", "filename")) %>%  corpus()
saints
summary(saints)
setwd("/Users/ekb5/Corpora/Saints/txt")
for (i in 1:3) {
to_str <- str_glue("saints0{i} <- readtext('/Volume0{i}') %>% corpus(); docvars(saints0{i}, 'Volume') <- '0{i}'")
eval(parse(text = to_str))
}
for (i in 1:3) {
to_str <- str_glue("saints0{i} <- readtext('/Volume0{i}/*.txt') %>% corpus(); docvars(saints0{i}, 'Volume') <- '0{i}'")
eval(parse(text = to_str))
}
for (i in 1:3) {
to_str <- str_glue("saints0{i} <- readtext('Volume0{i}/*.txt') %>% corpus(); docvars(saints0{i}, 'Volume') <- '0{i}'")
eval(parse(text = to_str))
}
saints01
saints <- saints01 + saints02 + saints03
saints
saints
corpus_subset(saints, Volume == "Volume01")
summary(saints)
corpus_subset(saints, Volume == "01")
corpus_subset(saints, Volume == "01") %>% corpus
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/", docvarnames = c("Volume", "filename")) %>%  corpus()
library(quanteda)
library(readtext)
library(tidyverse)
setwd("/Users/ekb5/Corpora/Saints/txt")
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/", docvarnames = c("Volume", "filename")) %>%  corpus()
saints
saints %>%
tokens()
library(quanteda.textstats)
saints %>%
tokens() %>%
textstat_collocations()
?textstat_collocations
saints %>%
tokens() %>%
textstat_collocations()  %>%
arrange(desc(lambda))
saints %>%
tokens() %>%
textstat_collocations()  %>%
arrange(desc(lambda)) %>%
head(10)
saints %>%
tokens() %>%
textstat_collocations()  %>%
arrange(desc(lambda)) %>%
ggplot(aes(lambda, z)) +
geom_point()+
geom_smooth(method=lm)
saints %>%
tokens() %>%
textstat_collocations()  %>%
arrange(desc(z))
saints %>%
tokens() %>%
tokens_remove(stopwords("en")) %>%
textstat_collocations() %>%
arrange(desc(lambda))
?textstat_collocations()
?textstat_collocations
saints %>%
tokens() %>%
textstat_collocations()  %>%
arrange(desc(z))
dictionary(list(temple = c("temple", "sealing", "spire", "open house"),
missionary = c("missionary", "mission", "labor")))
dict <- dictionary(list(temple = c("temple", "sealing", "spire", "open house"),
missionary = c("missionary", "mission", "labor")))
kwic(saints %>% tokens(), pattern = dict)
kwic(saints %>% tokens(), pattern = dict, valuetype = "regex")
library("quanteda.textstats")
dfmat_inaug_post1980 <- corpus_subset(data_corpus_inaugural, Year > 1980) |>
tokens(remove_punct = TRUE) |>
tokens_wordstem(language = "en") |>
tokens_remove(stopwords("en")) |>
dfm()
tstat_obama <- textstat_simil(dfmat_inaug_post1980,
dfmat_inaug_post1980[c("2009-Obama", "2013-Obama"), ],
margin = "documents", method = "cosine")
as.list(tstat_obama)
dotchart(as.list(tstat_obama)$"2013-Obama", xlab = "Cosine similarity", pch = 19)
data_corpus_sotu <- readRDS(url("https://quanteda.org/data/data_corpus_sotu.rds"))
dfmat_sotu <- corpus_subset(data_corpus_sotu, Date > as.Date("1980-01-01")) |>
tokens(remove_punct = TRUE) |>
tokens_wordstem(language = "en") |>
tokens_remove(stopwords("en")) |>
dfm()
dfmat_sotu <- dfm_trim(dfmat_sotu, min_termfreq = 5, min_docfreq = 3)
tstat_dist <- dfmat_sotu |>
dfm_weight(scheme = "prop") |>
textstat_dist()
# hiarchical clustering the distance object
pres_cluster <- hclust(as.dist(tstat_dist))
# label with document names
pres_cluster$labels <- docnames(dfmat_sotu)
# plot as a dendrogram
plot(pres_cluster, xlab = "", sub = "",
main = "Euclidean Distance on Normalized Token Frequency")
period <- ifelse(docvars(data_corpus_inaugural, "Year") < 1945, "pre-war", "post-war")
dfmat1 <- dfm(data_corpus_inaugural, groups = period)
head(dfmat1) # make sure 'post-war' is in the first row
dfmat1 <- dfm(tokens(data_corpus_inaugural), groups = period)
dfmat1 <- dfm(tokens(data_corpus_inaugural), dfm_group = period)
dfmat1 <- dfm(tokens(data_corpus_inaugural), dfm_group(period))
dfm_group()
?dfm_group()
dfmat1 <- dfm(tokens(data_corpus_inaugural) %>% dfm_group(groups = period)
)
dfm(tokens(data_corpus_inaugural)
dfm(tokens(data_corpus_inaugural))
data_corpus_inaugural
tokens(data_corpus_inaugural)
dfm(tokens(data_corpus_inaugural))
dfmat1 <- dfm(tokens(data_corpus_inaugural)) %>% dfm_group(groups = period))
dfmat1 <- dfm(tokens(data_corpus_inaugural)) %>% dfm_group(groups = period)
head(dfmat1) # make sure 'post-war' is in the first row
data_corpus_inaugural
docvars(data_corpus_inaugural)
period <- ifelse(docvars(data_corpus_inaugural, "Year") < 1945, "pre-war", "post-war")
period
dfm(tokens(data_corpus_inaugural))
dfmat1 <- dfm(tokens(data_corpus_inaugural)) %>% dfm_group(groups = period)
head(dfmat1) # make sure 'post-war' is in the first row
?textstat_keyness
library(rves)
library(rvest)
page <- read_html("https://eur-lex.europa.eu/legal-content/ES/TXT/HTML/?uri=CELEX:12016M/TXT")
page %>%
html_text2()
page %>%
html_text2() %>%
write_lines(file = "/Users/ekb5/Downloads/eu_leyes.txt")
2709/3
sessionInfo()
10^-15.005441665649414
install.packages("nycflights13")
library("nycflights13")
flights
flights %>% view
library(tidyverse)
flights %>% view
flights
1301/60
60*24
library(nycflights13)
flights
flights %>% view
library(tidyverse)
flights %>% view
library(nycflights13)
names(flights)
library(tidyverse)
?select
library("nycflights13")
flights
flights |>
summarize(mean(dep_delay))
library("tidyverse")
flights |>
summarize(mean(dep_delay))
clear()
flights |>
filter(!is.na(dep_delay)) |>
group_by(carrier) |>
summarize(mean(dep_delay))
library(nycflights13)
flights |> names
flights |> names()
library("readxl")
library("tidyverse")
setwd("/Users/ekb5/Documents/data_analysis/datasets/")
sibilants <- read_excel("data_FRC_spch_rate.xlsx", sheet = "data")
sibilants
sibilants |> pull(spch_rate)
?left_join
?separate
?unite
x <- "Bobby: I'm hungry!"
str_extract(x, "^.*?:")
str_extract(x, "^.*?(?!:)")
str_extract(x, ".*?(?!:)")
str_extract(x, ".*?")
str_extract(x, "^.*?")
str_extract(x, "^.*")
str_extract(x, "^[^:]+")
str_extract(x, "[^:]+4")
str_extract(x, "[^:]+$")
?unite
0.6436^4
0.3564 + 0.3564 - (0.3564 * 0.3564)
library("readxl")
library("tidyverse")
setwd("/Users/ekb5/Downloads/")
sibilants <- read_excel("data_L2s.xlsx", sheet = "data_hls_2017-10-25")
sibilants
?pivot_longer
library("tidyverse")
?pivot_longer
?tribble
