---
title: "Creating frequency lists"
---

## Objective

Students will create frequency lists from files on their harddrive.

## Frequency in language

Frequency is an important construct in many areas of language, and more generally, in human cognition (see our amazing pattern recognition abilities). Frequency affects which words and phrases are learned first, in L1s and L2s. More frequent grammatical constructions are learned before (and better for L2 speakers) than less frequent ones (e.g., active voice vs. passive voice in English). More frequency words experience phonetically-driven sound changes (e.g., lenition) first. More frequent word resist analogical leveling (*keep* -\> *\*keeped*, but *leap* -\> *leapt*, *leaped*).

In summary, frequency is super important, and being able to get frequencies of language features, especially words, is an important skill for a language-oriented data analyst.

## Getting frequencies of words in files

The logic to calculate frequencies of files on your (2s impersonal) harddrive (yeah, I know, I just spelled that word as one word on purpose because it's just one lexical unit) in R is simple:

1.  Parse the files so that all words in all files are in a single vector with N elements (N being the total number of words across all files);

2.  Ask R to count up the number of word tokens per word type in the vector.

    1.  There are two ways (Dr. Brown will show) to count up word tokens per word type:

        1.  With the base R `table()` function;
        2.  Convert the vector into a one-column data frame and then use `count()` in `tidyverse`.

### The `table()` function

Let's take a toy example:

```{r}
#| output: false
suppressPackageStartupMessages(library("tidyverse"))

# create a sentence
sentence <- "I like linguistics, and I like my students, but I love my wife and chidren. Sorry students. Maybe next time."

# uppercase (or lowercase) the string, so that uppercase and lowercase words (e.g., "The" and "the" and "THE") are treated as the same word
sentence <- str_to_upper(sentence)

# tokenize the string into words
words <- str_extract_all(sentence, "[-'â€™A-Z]+")

# unlist the list so that we're left with a vector
words <- unlist(words)

# throw the vector at table() and watch the magic happen!
freqs <- table(words)
print(freqs)
```

The output of the `table()` function is a named one-dimensional array (aka. vector), of class `table`. The values are the integers (i.e., the frequencies), and each integer has a name (i.e., a word). In order to extract only the names, you can use the `names()` function:

```{r}
#| output: false

print(names(freqs))
```

We probably want to sort the frequencies in descending order:

```{r}
#| output: false

freqs <- sort(freqs, decreasing = TRUE)
print(freqs)
```

The frequencies and their names (i.e., the words) can be put into a data frame and then exported out as a CSV file.

### Activity

Do just that, that is, export to a CSV file the words and frequencies using the result of the above toy example. One tip: You'll need to coerce the data type of the numbers to integer with `as.integer(freqs)` when assigning the array to a column in the data frame.

After giving it a good-faith effort, if you need help, take a look at Dr. Brown's code below:

```{r}
#| code-fold: true
#| output: false

df <- tibble(wd = names(freqs), freq = as.integer(freqs))
write_csv(df, file = "freqs.csv")
```

Let's ramp it up:

Create a frequency list of words in many text files of your choice (e.g., from Project Gutenberg or the *Saints* files in the LMS). After a good-faith effort, if you need help, take a look at Dr. Brown's start to the code below:

```{r}
#| code-fold: true
#| output: true

# get filenames
filenames <- dir(path = "/Users/ekb5/Corpora/Saints/txt/", pattern = "\\.txt$", full.names = TRUE, recursive = TRUE)

# create a collector string to collect the text of all files
all_str <- ""

# loop over the filenames
for (filename in filenames) {
  
  # get the text from the current file
  txt <- read_file(filename)
  
  # add the text of the current file to the collector string
  all_str <- str_c(all_str, txt, sep = " ")
}
```

After the `for` loop, the variable `all_str` is one big string with all text from all files. You should now be able to modify the code in the toy example above to get the frequencies of the words in this single big string. Go for it! You got this! Let's go! You're a super star! Etc.!

### The `count()` function

A second way (among other ways) is to take the vector with words (not the big single string, but the vector with each word as a separate element) and create a one-column data frame, and then use `count()` (within `tidyverse`). Let's go!

Using the `words` vector from the toy example above:

```{r}
#| output: false
df <- tibble(wd = words)
freqs <- df %>% count(wd)
print(freqs)
```

**Quick little aside**: The pipe operator `%>%` passes the value on the left-hand side of the operator into the function on the right-hand side, as the first argument to that function.

### Activity

That's right, it's time to step up and use the `count()` function to calculate frequencies and then write them out to a CSV file. Ready... set... go!

After a good-faith effort, if you need some help, take a look at Dr. Brown's code below:

```{r}
#| code-fold: true
# using the toy example above:
# (enjoy all the pipe operators!)
tibble(wd = words) %>% 
  count(wd) %>% 
  write_csv(file = "freqs.csv")
```
