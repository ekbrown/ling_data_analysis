---
title: "quanteda"
---

## Objective

-   Students will analyze textual data (aka. texts) with the `quanteda` R package.

## Using R for textual analysis

The `quanteda` R package is a super useful package for analyzing texts in R. Some of the techniques are corpus linguistic techniques through and through: tokenizing into words or sentences, keyword-in-context, removing stopwords. Other techniques might be better considered computational linguistic techniques: sentiment analysis, document feature matrix. Regardless, we need to get to know this package, as I can't call myself a good professor of Linguistics Data Analysis with R without looking at this package.

## Install the package and its friends

Step #1: Install the `quanteda` package and the other related package that the `quanteda` creators recommend:

```{r}
#| eval: false
install.packages(c("quanteda", "quanteda.textmodels", "quanteda.textstats", "quanteda.textplots", "readtext", "spacyr", "remotes"), repos = "http://cran.rstudio.com")
```

Then, we need to install two packages on a Github:

```{r}
#| eval: false
remotes::install_github("quanteda/quanteda.corpora")
remotes::install_github("kbenoit/quanteda.dictionaries")
```

## Create a corpus

Let's create a corpus in `quanteda`! First, let's get their example code [here](https://quanteda.io/articles/quickstart.html#creating-a-corpus) working on our computers.

Let's read in the three currently published volumes of [*Saints*](https://history.churchofjesuschrist.org/saints?lang=eng&cid=rdb_v_saints_eng). First, download the zipped file `Saints.zip` from the LMS. Then, unzip (aka. decompress or extract) the zipped file so that you end up with a directory with several subdirectories organized by volume number.

Let's read in the corpus such that we add a column with metadata about which volume each file is from. There are three ways (that Dr. Brown can think of) to do that.

### Option 1: Each volume individually (most verbose)

```{r}
#| eval: false
library("tidyverse")
library("quanteda")
library("readtext")
setwd("/Users/ekb5/Corpora/Saints/txt/")

# load each volume as a separate corpus
saints01 <- readtext("Volume01/*.txt") %>% corpus()  # same as: saints01 <- corpus(readtext("Volume01/*.txt"))
docvars(saints01, "Volume") <- "01"

saints02 <- readtext("Volume02/*.txt") %>% corpus()
docvars(saints02, "Volume") <- "02"

saints03 <- readtext("Volume03/*.txt") %>% corpus()
docvars(saints03, "Volume") <- "03"

# combine the three corpora into one, with the docvar column identifying which volume each file comes from
saints <- saints01 + saints02 + saints03

print(summary(saints))
```

### Option 2: Automate the previous approach (less verbose)

We can use the [`eval()`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/eval) and [`parse()`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/parse) functions in base R to automate the previous approach. Shall we?

```{r}
#| eval: false
library("tidyverse")
library("quanteda")
library("readtext")
setwd("/Users/ekb5/Corpora/Saints/txt/")

# our ol' friend the for loop 
for (i in 1:3) {
  to_str <- str_glue("saints0{i} <- readtext('Volume0{i}/*.txt') %>% corpus(); docvars(saints0{i}, 'Volume') <- '0{i}'")
  eval(parse(text = to_str))
}

# combine the three corpora into a new fourth one
saints <- saints01 + saints02 + saints03

print(summary(saints))
```

### Option 3: Use the `docvarfrom`, `dvsep`, and `docvarnames` arguments (least verbose)

The `readtext()` function has a handful of arguments. The `docvarfrom`, `dvsep`, and the `docvarnames` arguments can create a new `docvar` (i.e., metadata about the corpus) from the filenames and/or the filepaths. Take a look at the documentation [here](https://www.rdocumentation.org/packages/readtext/versions/0.90/topics/readtext).

```{r}
#| eval: false
library("tidyverse")
library("quanteda")
library("readtext")
setwd("/Users/ekb5/Corpora/Saints/txt/")

# one pipeline will do the trick!
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/", docvarnames = c("Volume", "filename")) %>% corpus()

print(summary(saints))
```

### Activity

You guessed it! It's your turn. Create a corpus of your choice with files of your choice (e.g., perhaps from Project Gutenberg). Add a `docvar` of your choice, which may mean you need to preprocess the files a bit by specifying filenames or directory structure in a certain way.

## Creating subcorpora

It is super simple to create a subcorpus using a `docvar` (i.e., metadata about the files in the corpus). For example, let's say we have our one Saints corpus with a `docvar` of Volume with values of "01", "02", and "03". In order to create a subcorpus of just the files in Volume 1, we could run the following code. See the docs [here](https://quanteda.io/articles/quickstart.html#subsetting-corpus-objects).

```{r}
#| eval: false
vol01 <- corpus_subset(saints, Volume == "01") 
```

### Activity

Give it a try! Create a subcorpus from a larger corpus of your choice using the `docvar` of your choice.

## Keyword-in-context (KWIC)

Let's perform into a corpus linguistic technique: the keyword-in-context display or concordance lines or concordances. It is super easy to do so with the [`tokens()`](https://quanteda.io/reference/tokens.html) and [`kwic()`](https://quanteda.io/reference/kwic.html) functions. See example [here](https://quanteda.io/articles/quickstart.html#exploring-corpus-texts).

Let's start out slowly with a simple word search using our `Saints` corpus:

```{r kwic}
#| eval: false
# Assuming packages are loaded and corpus has been created

# tokenize and then get concordances
toks <- tokens(saints)
concordances <- kwic(toks, pattern = "tree")
print(concordances)
```

Let's ramp it up with some regular expression pizzazz. **Question**: What does the regular expression find?

```{r}
#| eval: false
toks <- tokens(saints)
concordances <- kwic(toks, pattern = "\\w+(\\w{2,})\\W+\\w+\\1\\b", valuetype = "regex")
print(concordances)
```

The main tokenizer function `tokens()` has a handful of arguments that are worth inspecting. Take a gander [here](https://quanteda.io/reference/tokens.html). Also, there many other tokenizer functions that can be useful. Take a look [here](https://quanteda.io/reference/index.html#tokens-functions).

### Activity

Get to know the tokenizing functions and the kwic() function (example [here](https://quanteda.io/articles/quickstart.html#exploring-corpus-texts) and docs [here](https://quanteda.io/reference/kwic.html)) with the corpus of your choice.

## Searching with a dictionary

```{r dictionary}
#| eval: false

library("tidyverse")
library("quanteda")
library("readtext")
setwd("/Users/ekb5/Corpora/Saints/txt/")

# one pipeline will do the trick!
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/", docvarnames = c("Volume", "filename")) %>% corpus()

# create a dictionary with named vectors
dict <- dictionary(list(temple = c("temple", "sealing", "spire", "open house"),
                missionary = c("missionary", "mission", "labor\\b")))

# search with the dictionary
kwic(tokens(saints), pattern = dict, valuetype = "regex") %>% print()
```

## Collocations

You can retrieve collocations with the `textstat_collocations()` function [here](https://quanteda.io/reference/textstat_collocations.html). At the time of making this lesson plan, it only calculates the lambda metric proposed by Blaheta and Johnson (2001):

Blaheta, D. & Johnson, M. (2001). [Unsupervised learning of multi-word verbs](http://web.science.mq.edu.au/~mjohnson/papers/2001/dpb-colloc01.pdf). Presented at the ACLEACL Workshop on the Computational Extraction, Analysis and Exploitation of Collocations.

However, the doc page says that there are plans to add more measures. Hopefully log-dice is in the works, as it's a great word association metric.

Let's retrieve the collocations in the first three volumes of *Saints*:

```{r collocations}
#| eval: false

library(quanteda)
library(readtext)
library(tidyverse)
library(quanteda.textstats)

# change directories into the Saints directories
setwd("/Users/ekb5/Corpora/Saints/txt")

# load the Saints corpus, creating a docvar with the Volume that each file belongs to
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/", docvarnames = c("Volume", "filename")) %>%  corpus() 
# retrieve collocations
saints %>% 
  tokens() %>% 
  textstat_collocations()  %>% 
  arrange(desc(lambda)) %>%  # arrange them in descending order by lambda
  head(50)  # show only first 50 collocations
```

That's okay, but there are lots of proper nouns and low-frequency collocations. We can remove those with the `min_count` argument: `textstat_collocations(min_count = 25)`.

If we want, we can also remove function words, which have little semantic value, with a stopword list:

```{r}
#| eval: false

library("stopwords")
saints %>% 
  tokens() %>% 
  tokens_remove(stopwords("en")) %>% 
  textstat_collocations(min_count = 25) %>% 
  arrange(desc(lambda)) %>% 
  head(50)
```

### Activity

Go for it! That is, retrieve collocations in a corpus of your choice. Look at and possibly use some of the other arguments in the textstat_collocations() function. Reminder: In order to pull up the doc page of a function within the RStudio IDE, you can type a question mark and then the name of the function (no space between them) in the console, e.g., `?textstat_collocations`. And [here](https://quanteda.io/reference/textstat_collocations.html)'s that same doc page on the internet.

## Document-feature matrix

Within `quanteda`, a document-feature matrix is very similar to a document-term matrix (if you know what that is). It's a tabular dataset (hence the name "matrix") and has the name of the files (aka. documents) as rows, and the features (i.e., words and punctuation) are the columns. [Here](https://quanteda.io/articles/quickstart.html#constructing-a-document-feature-matrix)'s an example. Let's take a look with the *Saints* corpus:

```{r document-feature matrix}
#| eval: false

library("tidyverse")
library("quanteda")
library("readtext")
setwd("/Users/ekb5/Corpora/Saints/txt/")

# one pipeline will do the trick!
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/", docvarnames = c("Volume", "filename")) %>% corpus()

saints %>% 
  tokens() %>% 
  dfm()
```

We can use a dictionary of patterns to limit the number of features (i.e., columns) that are returned in the matrix:

```{r}
#| eval: false
dict <- dictionary(list(
  temple = c("temple", "sealing", "spire", "open house"),
  missionary = c("missionary", "mission", "labor"))
  )

saints %>% 
  tokens() %>% 
  tokens_lookup(dictionary = dict) %>% 
  dfm() %>% 
  as_tibble() %>%  # cast to tibble in order to use arrange below
  arrange(desc(temple))
```

## Similarities between texts

You can plot similarities between texts in a corpus. Let's run the example code in the Quick Guide [here](https://quanteda.io/articles/quickstart.html#similarities-between-texts).

Now, let's make this work with our *Saints* corpus.

## Keyness analysis

We can use quanteda to perform keyness analysis, that is, to identify the keywords that typify a section of the corpus from the rest of the corpus.

Let's get the creators' example working in the doc page (in console `?textstat_keyness`) and [here](https://quanteda.io/articles/pkgdown/examples/plotting.html?q=keyness#plot-keyness-in-a-target-and-reference-group).
