---
title: "quanteda"
---

## Objective

-   Students will analyze textual data (aka. texts) with the `quanteda` R package.

## Using R for textual analysis

The `quanteda` R package is a super useful package for analyzing texts in R. Some of the techniques are corpus linguistic techniques through and through: tokenizing into words or sentences, keyword-in-context, removing stopwords. Other techniques might be better considered computational linguistic techniques: sentiment analysis, document feature matrix. Regardless, we need to get to know this package, as I can't call myself a good professor of Linguistics Data Analysis with R without looking at this package.

## Install the package and its friends

Step #1: Install the `quanteda` package and the other related package that the `quanteda` creators recommend:

```{r}
#| eval: false
install.packages(c("quanteda", "quanteda.textmodels", "quanteda.textstats", "quanteda.textplots", "readtext", "spacyr", "remotes"), repos = "http://cran.rstudio.com")
```

Then, we need to install two packages on a Github:

```{r}
#| eval: false
remotes::install_github("quanteda/quanteda.corpora")
remotes::install_github("kbenoit/quanteda.dictionaries")
```

## Create a corpus

Let's create a corpus in `quanteda`! First, let's get their example code [here](https://quanteda.io/articles/quickstart.html#creating-a-corpus) working on our computers.

Let's read in the three currently published volumes of [*Saints*](https://history.churchofjesuschrist.org/saints?lang=eng&cid=rdb_v_saints_eng). First, download the zipped file `Saints.zip` from the LMS. Then, unzip (aka. decompress or extract) the zipped file so that you end up with a directory with several subdirectories organized by volume number.

Let's read in the corpus such that we add a column with metadata about which volume each file is from. There are three ways (that Dr. Brown can think of) to do that.

### Option 1: Each volume individually (most verbose)

```{r}
#| eval: false
library("tidyverse")
library("quanteda")
library("readtext")
setwd("/Users/ekb5/Corpora/Saints/txt/")

# load each volume as a separate corpus
saints01 <- readtext("Volume01/*.txt") %>% corpus()  # same as: saints01 <- corpus(readtext("Volume01/*.txt"))
docvars(saints01, "Volume") <- "01"

saints02 <- readtext("Volume02/*.txt") %>% corpus()
docvars(saints02, "Volume") <- "02"

saints03 <- readtext("Volume03/*.txt") %>% corpus()
docvars(saints03, "Volume") <- "03"

# combine the three corpora into one, with the docvar column identifying which volume each file comes from
saints <- saints01 + saints02 + saints03

print(summary(saints))
```

### Option 2:  Automate the previous approach (less verbose)

We can use the [`eval()`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/eval) and [`parse()`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/parse) functions in base R to automate the previous approach. Shall we?

```{r}
#| eval: false
library("tidyverse")
library("quanteda")
library("readtext")
setwd("/Users/ekb5/Corpora/Saints/txt/")

# our ol' friend the for loop 
for (i in 1:3) {
  to_str <- str_glue("saints0{i} <- readtext('Volume0{i}/*.txt') %>% corpus(); docvars(saints0{i}, 'Volume') <- '0{i}'")
  eval(parse(text = to_str))
}

# combine the three corpora into a new fourth one
saints <- saints01 + saints02 + saints03

print(summary(saints))
```

### Option 3: Use the `docvarfrom`, `dvsep`, and `docvarnames` arguments (least verbose)

The `readtext()` function has a handful of arguments. The `docvarfrom`, `dvsep`, and the `docvarnames` arguments can create a new `docvar` (i.e., metadata about the corpus) from the filenames and/or the filepaths. Take a look at the documentation [here](https://www.rdocumentation.org/packages/readtext/versions/0.90/topics/readtext).

```{r}
#| eval: false
library("tidyverse")
library("quanteda")
library("readtext")
setwd("/Users/ekb5/Corpora/Saints/txt/")

# one pipeline will do the trick!
saints <- readtext(file = "*/*.txt", docvarsfrom = "filepaths", dvsep = "/", docvarnames = c("Volume", "filename")) %>% corpus()

print(summary(saints))
```

### Activity

You guessed it! It's your turn. Create a corpus of your choice with files of your choice (e.g., perhaps from Project Gutenberg). Add a `docvar` of your choice, which may mean you need to preprocess the files a bit by specifying filenames or directory structure in a certain way.

## Creating subcorpora

It is super simple to create a subcorpus using a `docvar` (i.e., metadata about the files in the corpus). For example, let's say we have our one Saints corpus with a `docvar` of Volume with values of "01", "02", and "03". In order to create a subcorpus of just the files in Volume 1, we could run the following code. See the docs [here](https://quanteda.io/articles/quickstart.html#subsetting-corpus-objects).

```{r}
#| eval: false
vol01 <- corpus_subset(saints, Volume == "01") 
```

### Activity

Give it a try! Create a subcorpus from a larger corpus of your choice using the `docvar` of your choice.

## Keyword-in-context (KWIC)

Let's perform into a corpus linguistic technique: the keyword-in-context display or concordance lines or concordances. It is super easy to do so with the [`tokens()`](https://quanteda.io/reference/tokens.html) and [`kwic()`](https://quanteda.io/reference/kwic.html) functions. See example [here](https://quanteda.io/articles/quickstart.html#exploring-corpus-texts).

Let's start out slowly with a simple word search using our `Saints` corpus:

```{r}
#| eval: false
# Assuming packages are loaded and corpus has been created

# tokenize and then get concordances
toks <- tokens(saints)
concordances <- kwic(toks, pattern = "tree")
print(concordances)
```

Let's ramp it up with some regular expression pizzazz. **Question**: What does the regular expression find?

```{r}
#| eval: false
toks <- tokens(saints)
concordances <- kwic(toks, pattern = "\\w+(\\w{2,})\\W+\\w+\\1\\b", valuetype = "regex")
print(concordances)
```

The main tokenizer function `tokens()` has a handful of arguments that are worth inspecting. Take a gander [here](https://quanteda.io/reference/tokens.html). Also, there many other tokenizer functions that can be useful. Take a look [here](https://quanteda.io/reference/index.html#tokens-functions).

### Activity

Get to know the tokenizing functions and the kwic() function (example [here](https://quanteda.io/articles/quickstart.html#exploring-corpus-texts) and docs [here](https://quanteda.io/reference/kwic.html)) with the corpus of your choice.

## MORE TO COME SOON ABOUT QUANTEDA
