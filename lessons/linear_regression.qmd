---
title: "Linear regression"
---

## Objective

Students will build a linear regression model.

## Ordinary Least Squares Regression

A very common form of regression is Ordinary Least Squares (OLS) linear regression. If no modifiers are put before the term "linear regression" then Ordinary Least Squares linear regression is being referred to.

The basic idea is to try to fit a straight line (aka. regression line) through data points such that you end up with the lowest possible sum of squared residuals.

## Residuals

A residual in regression is the distance between an observed data point (aka. observation) and the regression line. While there is a mathematical formula to find where exactly that line should go, that is, to find its slope and the value of $y$ when the line crosses $x=0$, for pedagogical purposes, let's do a bit of trial-n-error.

## Toy example

Let's create a simple data frame:

```{r}
suppressPackageStartupMessages(library("tidyverse"))
morenos <- tibble(
  person = c("Sam", "Eloisa", "Lola", "Heidi", "Ellen"),
  height = c(72, 70, 69, 63, 62),
  age = c(18, 17, 14, 13, 11)
)
print(morenos)
```

Let's plot height by age of the kids in the above data frame:

```{r}
p1 <- morenos %>% 
  ggplot(aes(x = age, y = height))+
  geom_point(size = 3)+
  theme_minimal()
plot(p1)
```

It looks like there is a relationship between height and age of these kids, such that as the kids get older, their heights increase (unsurprisingly).

## Trial-n-error

Let’s plot some lines and calculate the sum of the squares of the residuals for each line. In the plots below, the black points are observed values while the red points are predicted values. The vertical lines connecting the observed values (black points) and the predicted values (red points) are the residuals (aka. errors).

```{r}
# define a helper function
plot_dots_and_line <- function(df, input_slope, input_intercept) {
  temp <- df %>% 
    mutate(hypothetical = age * input_slope + input_intercept)
  p1 <- temp %>%
    ggplot(aes(x = age, y = height))+
    
    # draw a straight line with the slope and y-intercept
    geom_abline(slope = input_slope, intercept = input_intercept, color = "blue", linetype=3)+
    
    geom_point(aes(y = hypothetical), color = "red", size = 3)+
    geom_segment(aes(xend = age, yend = hypothetical), alpha = 0.5)+
    geom_point(size = 3)+
    ggtitle(str_interp("Slope = ${input_slope}; y-intercept = ${input_intercept}; Sum of squares of residuals = ${sum((temp$height - temp$hypothetical)^2)}"))+
    theme_minimal()
  return(p1)
}

# Make a list of guesses of slopes and y-intercepts
guesses <- list(
  list(g_slope = 0, g_intercept = 65),
  list(g_slope = 0.5, g_intercept = 65),
  list(g_slope = 0.5, g_intercept = 60),
  list(g_slope = 1, g_intercept = 50),
  list(g_slope = 1, g_intercept = 52)
)

# Loop over the guesses
for (guess in guesses) {
  p1 <- plot_dots_and_line(morenos, guess$g_slope, guess$g_intercept)
  plot(p1)
}
```

## The formula

Okay, enough horsin' around. Let's use the formula (actually formulas, or formulae if you must) to get the best-fit regression line, that is, the line with the lowest sum of squares of residuals.

Here's the formula to get the predicted y-value for a given x-value.

$$
\hat{y}=slope*x+intercept
$$

The $\hat{y}$ in the formula is referred to as "y-hat" and is the predicted (aka. fitted) y-value for a given x value. And how do you get the slope and intercept, you ask? Math (don’t freak out, it’s not that bad):

$$
slope = \frac{N * \sum{(x * y)} - \sum{x} * \sum{y}}{N * \sum{(x^{2})} - (\sum{x})^{2}}
$$

Where:\
$N$ = number of observations in dataset\
$*$ = Good ol’ fashioned multiplication\
$\sum$ = The sum of the numbers to the right\
$x$ = The explanatory (aka. independent or predictor) variable (here *age*)\
$y$ = The response (aka. dependent) variable (here *height*)

Let's look at each piece of this formula with color:\
$$
slope = \frac{N * \color{red} \sum{(x * y)} \color{black} - \color{orange} \sum{x} \color{black} * \color{green} \sum{y}}{N * \color{blue} \sum{(x^{2})} \color{black} - \color{purple} (\sum{x})^{2}}
$$

So:\
$N=5$ $\color{red} \sum{(x*y)} \color{black} =(18*72+17*70+14*69+13*63+11*62)=(1296+1190+966+819+682) = 4953$\
$\color{orange} \sum{x} \color{black} =(18+17+14+13+11) = 73$\
$\color{green} \sum{y} \color{black} =(72+70+69+63+62) = 336$\
$\color{blue} \sum{(x^{2})} \color{black} =(18^{2}+17^{2}+14^{2}+13^{2}+11^{2})=(324+289+196+169+121) = 1099$\
$\color{purple} (\sum{x})^{2} \color{black} =(18+17+14+13+11)^{2} = 73^{2} = 5329$\
Now, let’s put the numbers into the formula and finally get that rascally slope:\
\
$$slope=\frac{5* \color{red} 4953 \color{black} - \color{orange} 73 \color{black} * \color{green} 336}{5* \color{blue} 1099 \color{black} - \color{purple} 5329}=\frac{24765-24528}{5495-5329}=\frac{237}{166}= 1.427711
$$\
Lovely, the slope of the least squares line (aka. line of best fit) is 1.427711.

Now, here’s the formula for the y-intercept (which uses the slope that we just calculated):\
\
$$intercept = \frac{\color{green} \sum{y} \color{black} - slope * \color{orange} \sum{x}}{N}
$$\
We already calculated above all the numbers in this formula, so let’s get crackin’!:

$$intercept = \frac{\color{green} 336 \color{black} - 1.427711 * \color{orange} 73}{5}=\frac{336-104.2229}{5}=\frac{231.7771}{5}=46.35542
$$

Terrific, the y-intercept of the least squares line is 46.35542.

Now, let’s use the slope and the y-intercept we just calculated to draw a line through our data points and calculate the sum of the squares of residuals.

```{r}
plot_dots_and_line(morenos, 1.427711, 46.35542)
```

## Fit a linear regression

As I’m sure you have likely guessed, there’s a function to do all the math we just did by hand: [`lm()`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm). Because the dependent variable (aka. response variable) is continuous, we will fit a linear regression (rather than a logistic regression). When we have only one explanatory variable, the term "simple linear regression" is often used. Differently, when we have two or more explanatory variables, the term "multiple linear regression" is used. We see multiple linear regression below.

```{r}
fitted_model <- morenos %>% 
  lm(height ~ age, data = .)
print(fitted_model)
```

The `summary()` function gives you the above info as well as more info:

```{r}
summary(fitted_model)
```

The `Call` section and the `Residuals` sections are transparently named. With small datasets, the residuals are given here in the summary, but when there are many data points (and therefore an equal number of many residuals), only summary information is given about the residuals, specifically the min and max, and the first and third quartiles, and the median. Ideally, the residuals should be normally distributed and centered on zero. If the summary info given isn't enough to determine those assumptions, you can retrieve all the residuals with the `residuals()` function:

```{r}
residuals(fitted_model)
```

The `Coefficients` section gives a lot of numbers. The `Estimate` column gives the intercept, which again, is the predicted or fitted value of $y$ when $x$ is zero (i.e., the predicted value of the response variable when the explanatory variable has a value of zero). The slope of age is given in the `age` row, under the `Estimate` column. What this means, is that for each unit increase of the the explanatory (i.e., age), there is on average a 1.4277 unit increase in the response variable. So, in this toy example, this means that for each year older, a kid is 1.4277 inches taller. And, this general trend is statistically significant because the *p*-value (i.e., 0.02355) in the far right column (i.e., `Pr(>|t|)`) is smaller than the alpha level of 0.05 (is we consider that alpha level to be our threshold below this statistical significance occurs).

## Predicted values

We can get the predicted (aka. fitted) values (i.e., $\hat{y}$) from the model and add them to the original dataframe.

```{r}
morenos <- morenos %>% 
  mutate(predicted = predict(fitted_model))
print(morenos)
```

## Residuals

We can get the residuals from the model and add them to the original dataframe. The residuals are the distances between the observed values and their corresponding predicted values.

```{r}
morenos <- morenos %>% 
  mutate(residual = residuals(fitted_model)) 
print(morenos)
```

Let's now (more easily) plot the observed data points $y$, the predicted values (i.e., $\hat{y}$), and the residuals.

```{r}
morenos %>% 
  ggplot(aes(x = age, y = height))+
  geom_smooth(method = lm, se = FALSE, linetype = 3)+
  geom_point(aes(y = predicted), color = "red")+
  geom_point()+
  theme_minimal()+
  geom_segment(aes(xend = age, yend = predicted), alpha = 0.5) 
```

## Assumptions of linear regression

Linear regression (like most inferential statistical tests) has a number of assumptions that should be met in order to obtain reliable results. I'm borrowing liberally from Levshina (2015, p. 155) here.

Assumptions:

1.  The observations are independent from one another. If the observations are related to each other (e.g., some/many tokens come from the same person or from the same word), then mixed-effect regression should be used with person and/or word specified as what's called a random effect.

2.  The response variable is continuous. If ordinal, then ordinal regression should be used instead. If categorical with two levels in the response variable, then logistic regression should be used. If categorical with three or more levels in the response variable, then multinomial regression should be used.

3.  The relationship between the response and explanatory variable is linear. If not, then a transformation of the response and/or explanatory variables should be performed (e.g., taking the logarithm of word frequency).

4.  The residuals of the model vary constantly. This is called homoscedasticity of variance. In other words, the variability of the residuals does not increase or decrease with the response variable nor with the explanatory variable(s). Fitting a scatterplot of the residuals by the predicted (aka. fitted) values of the linear model is a good way to detect if there is heteroscedasticity of variance. If there is heteroscedasticity a Box-Cox transformation of the response variable can help. If still problematic, a boostrapping procedure is the next step.

5.  There is no multicollinearity of the explanatory variables, that is, they are not (overly) correlated with each other. Value Inflation Factors (VIF) scores below 5 or 10 indicate not much multicollinearity going on. The `car` package has a `vif()` function (and there are other packages with a similar functions.)

6.  The residuals are not autocorrelated. A p-value below 0.05 in the [`durbinWatsonTest()`](https://www.rdocumentation.org/packages/car/versions/3.1-2/topics/durbinWatsonTest) function in the `car` package indicates autocorrelation. This is rarely a problem.

7.  The residuals should be normally distributed, with a mean of zero. This assumption becomes less important as sample size increases.

Detailed explanations of these assumptions and how to test for are offered by Levshina (2015, pp. 155 - 162).

## Activity

From Chapter 4 "[Linear Regression 1](https://github.com/msonderegger/rmld-v1.1/blob/main/rmld_v1.1.pdf)" in *Regression Modeling for Linguistic Data* v1.1 by Sonderegger et al. (2022).

1.  Using the `languageR::english` dataset, create a linear regression model with `RTlexdec` as the response variable and `WrittenFrequency` as the only explanatory variable.

2.  Then, figure out what the predicted `RTlexdec` for each of the following written frequencies. First, calculate the predicted `RTlexdec` values manually using the y-intercept and the slope given by the linear regression model (and probably using R as a basic calculator, e.g., $y\text{-}intercept + slope * x$). Second, calculate the predicted `RTlexdec` values by using the [`predict.lm()`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/predict.lm) function.

    1.  What is the predicted `RTlexdec` value when `WrittenFrequency` is 5?

    2.  How about when `WrittenFrequency` is 10?

After a good-faith effort, if you need help take a look at Dr. Brown's code below.

```{r}
#| eval: true
#| code-fold: true

# fit linear regression model
languageR::english %>% 
  lm(RTlexdec ~ WrittenFrequency, data = .) -> m1
summary(m1)

# save the intercept and slope to variables
intercept <- m1$coefficients["(Intercept)"]
slope <- m1$coefficients["WrittenFrequency"]

# semi-manual math
print(intercept + slope * 5)
print(intercept + slope * 10)

# use the predict.lm() function
predict.lm(m1, newdata = tibble(WrittenFrequency = c(5, 10)))
```

## More than one explanatory variable

The real usefulness of regression analysis is its ability to measure the influence of several explanatory variables at once in order to determine which variables significantly predict or explain the response variable.

Let's look at a multiple linear regression with three explanatory variables. Within the `Rling` package there is a dataset called ELP, which contains a subset of the English Lexicon Project. Let's download the package from the companion website of the textbook [*How to Do Linguistics with R*](https://benjamins.com/sites/z.195/content/package.html) by Levshina. Then change the pathway in the following code and run it:

```{r}
#| eval: false
install.packages("/pathway/to/Rling_1.0.tar.gz", repos = NULL, type = "source")
```

And then load up the dataset and inspect it a bit:

```{r}
library("tidyverse")
library("Rling")
data(ELP)
glimpse(ELP)
summary(ELP)
```

The `Word` column is transparently named. The `Length` column is the length of the word in number of letters (as an integer). The `SUBTLWF` column gives the frequency (i.e., float or double or numeric) of the word normalized to per million words, as attested in a corpus of movie subtitles. The `POS` is transparently named (for a linguist, at least) and has three levels: `JJ` for adjective, `NN` for noun, and `VB` for verb. The `Mean_RT` column gives the mean reaction time in milliseconds (i.e., a float), and will the response variable in our regression.

Let's go with the linear regression!

The response variable (i.e., `Mean_RT` here) goes to the left of the formula operator (i.e., a tilde `~`), and the explanatory variables to the right of that operator, separated by a plus sign `+`. The data frame with the dataset goes with the `data` argument. As is (very) common practice, we'll take the log of frequency to put it on a more linear scale (from it's original zipfian scale; see Vsauce's [video about lexical frequency](https://youtu.be/fCn8zs912OE?si=HrO8L9uNa5l1KKBm)).

Behold, a linear regression and its results:

```{r}
m1 <- lm(Mean_RT ~ Length + log(SUBTLWF) + POS, data = ELP)
summary(m1)
```

### Interpretation

Let's review the interpretation of the output. The `Call` section simply shows the `lm()` call. The `Residuals` section gives summary info about the residuals (but we could see the individual residuals by using the `residuals()` function). The `Coefficients` section gives the y-intercept and the slopes of the three explanatory variables.

First, the `Length` row shows below the `Estimate` column that, on average, with each unit increases of the `Length` variable (i.e., each additional letter in word), there is a 19.555 milliseconds increase in `Mean_RT` (i.e., mean reaction time), and the p-value below the `Pr(>|t|)` column is below 0.0000000000000002 (R's default cutoff with p-values in this summary display). In simple terms, as the lengths of words increase, it takes humans significantly longer to react.

The `log(SUBTLWF)` row shows below the `Estimate` column that, on average, with each unit increase of frequency (on a log scale, given our use of `log()`), mean reaction times decrease by 29.288 milliseconds (note the negative sign before the number), and this happens at a statistically significant rate given the *p*-value.

The `POS` variable has two rows: `POSNN` and `POSVB`. But wait! What happened to `JJ`?! Because `JJ` comes first in the alphabet in comparison to `NN` and `VB`, it was chosen by R as the **reference level** against which the other levels in that categorical variable are compared. So, we compare each of the listed levels (i.e., `NN` and `VB`) to `JJ`, which is not listed. First things first, `POSNN` does not have a *p*-value below 0.05, so we conclude that there is no significant difference between `NN` and `JJ` in this dataset, and then we move on. Because `POSVB`'s *p*-value is below our alpha level of 0.05, we conclude that there is a significant difference between `VB` and `JJ`, such that, on average, reaction times to `VB` (i.e., verbs) is 29.184 milliseconds shorter (i.e., -29.184) than reaction times to `JJ` (i.e., adjectives).

By default, R chooses the level that comes first in alphabetical order to serve as the reference level. If we would like a different level to be the reference level, we can relevel the categorical variable so that another level is first with the [`fct_relevel()`](https://forcats.tidyverse.org/reference/fct_relevel.html) function. We saw this function previously in the context of boxplots, when we changed the order of the boxes.

## Sum contrasts

The comparison of a reference level to the other level(s) of a categorical variable is called "treatment contrasts", and this is the default behavior of `lm()` in R. Another option is called "sum contrasts", which is based on the mean of means of the response variable by each level in the categorical variable. The coefficients given indicate how much above or below that mean of means each level in the categorical is. For more info about sum contrasts, see [this website](https://marissabarlaz.github.io/portfolio/contrastcoding/#sum-coding) or see p. 146 of Levshina (2015) or p. 201 of *Regression Modeling for Linguistic Data* v1.1 (among other good resources).

## References

Levshina, Natalia. 2015. *How to do Linguistics with R: Data Exploration and Statistical Analysis*. Amsterdam / Philadelphia: John Benjamins.

Sonderegger, Morgan. 2022. *Regression Modeling for Linguistic Data.* [Available online](https://github.com/msonderegger/rmld-v1.1).

Also, I thank the creators of these websites, which I relied on heavily: <https://drsimonj.svbtle.com/visualising-residuals>\
<https://www.mathsisfun.com/data/least-squares-regression.html>
