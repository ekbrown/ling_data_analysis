---
title: "Data Wrangling"
---

## Objective

Students will manipulate or wrangle a data frame.

## The `dplyr` package

The `dplyr` R package is a core package of `tidyverse`, and has a handful of super useful functions for manipulating or wrangling a data frame. Most of the functions are verbs (e.g., `filter()`, `select()`, etc.) and so the documentation for the package often refers to the main functions of dplyr as verbs.

## Rows

There are three (I guess four) main functions (aka. verbs) for working with rows in `dplyr`: `filter()`, `arrange()` (and it's helper function `desc()`), and `distinct()`.

Pop (formative) quiz!

Instructions: Match each function with its corresponding purpose.

|              |                                                                                             |
|--------------|---------------------------------------------------------------------------------------------|
| `filter()`   | A. orders the rows based on values in one or more columns                                   |
| `arrange()`  | B. inverts the order so that the rows are in big-to-small order                             |
| `desc()`     | C. keeps only unique rows                                                                   |
| `distinct()` | D. keeps rows that evaluate to TRUE in a conditional statement based on one or more columns |

### Activity

Let's follow the examples given in Chapter 3 "Data transformation" of the book *R for Data Science* (2e). First, let's download the `nycflights13` R package to our harddrive:

```{r load nycflights13}
#| eval: false
install.packages("nycflights13", repos = "https://cran.rstudio.com")
```

Now, let's load the nycflights13 data frame into our R session or script, and our ol' friend `tidyverse`:

```{r}
#| eval: false
library("nycflights13")
library("tidyverse")
```

Finally, and most importantly, let's try to solve the exercises in section 3.2.5 [here](https://r4ds.hadley.nz/data-transform.html#exercises). After some good-faith work, if you need some help, take a look at Dr. Brown's code below:

"Had an arrival delay of two or more hours"

```{r}
#| eval: false
#| code-fold: true
flights |> 
  filter(arr_delay >= 120)
```

"Flew to Houston (IAH or HOU)"

```{r}
#| eval: false
#| code-fold: true
flights |>  
  filter(dest %in% c("IAH", "HOU"))
```

"Were operated by United, American, or Delta"

```{r}
#| eval: false
#| code-fold: true
flights |>  
  filter(carrier %in% c("UA", "AA", "DL"))
```

"Departed in summer (July, August, and September)"

```{r}
#| eval: false
#| code-fold: true
flights |>  
  filter(month %in% c(7, 8, 9))  # note: the month column has a data type of integer, and therefore you don't need quotes around the numbers
```

Here's another way to filter for only summer flights:

```{r}
#| eval: false
#| code-fold: true
flights |>  
  filter(month >= 7 & month <= 9)
```

"Arrived more than two hours late, but didn’t leave late"

```{r}
#| eval: false
#| code-fold: true
flights |>  
  filter(arr_delay > 120 & dep_delay <= 0)
```

"Were delayed by at least an hour, but made up over 30 minutes in flight"

```{r}
#| eval: false
#| code-fold: true
flights |>  
  filter(dep_delay >= 60 & arr_delay < 30)
```

"Sort flights to find the flights with longest departure delays."

```{r}
#| eval: false
#| code-fold: true
flights |>  
  arrange(desc(dep_delay))
```

"Find the flights that left earliest in the morning."

```{r}
#| eval: false
#| code-fold: true
flights |>  
  arrange(dep_time)
```

"Sort flights to find the fastest flights. (Hint: Try including a math calculation inside of your function.)"

```{r}
#| eval: false
#| code-fold: true
flights |>  
  arrange(desc(distance / air_time))
```

Here's another way, and a preview of the `mutate` function that we'll see below:

```{r}
#| eval: false
#| code-fold: true
flights |>  
  mutate(speed = distance / air_time) %>% 
  arrange(desc(speed))
```

"Was there a flight on every day of 2013?"

```{r}
#| eval: false
#| code-fold: true
flights |>  
  distinct(year, month, day)
```

"Which flights traveled the farthest distance?"

```{r}
#| eval: false
#| code-fold: true
flights |>  
  arrange(desc(distance))
```

"Which traveled the least distance?"

```{r}
#| eval: false
#| code-fold: true
flights |>  
  arrange(distance)
```

"Does it matter what order you used `filter()` and `arrange()` if you’re using both? Why/why not? Think about the results and how much work the functions would have to do."

Earl speculates that it's probably best to `filter` first so that `arrange` has fewer rows to sort.

## Columns

There are four main functions (i.e., verbs) for working with columns: `mutate()`, `select()` (and its handful of helper functions [here](https://dplyr.tidyverse.org/reference/select.html)), `rename()`, and `relocate()`.

Pop quiz time!

Instructions: Match each function (i.e., verb) with its corresponding purpose.

|              |                                                                      |
|--------------|----------------------------------------------------------------------|
| `mutate()`   | A. change the name of the specified column                           |
| `select()`   | B. creates new columns based on one or more already existing columns |
| `rename()`   | C. moves the position of the specified columns                       |
| `relocate()` | D. keeps only the specified columns                                  |

Let's try out the exercises provided in 3.3.5 of Chapter 3 "Data transformation" [here](https://r4ds.hadley.nz/data-transform.html#exercises-1). After a good-faith effort, if you need take a look at Dr. Brown's code below each exercise.

"Compare `dep_time`, `sched_dep_time`, and `dep_delay`. How would you expect those three numbers to be related?"

"Brainstorm as many ways as possible to select `dep_time`, `dep_delay`, `arr_time`, and `arr_delay` from `flights`"

```{r}
#| eval: false
#| code-fold: true
flights |> 
  select("dep_time", "dep_delay", "arr_time", "arr_delay")
```

```{r}
#| eval: false
#| code-fold: true
flights |> 
  select(matches("^(dep_|arr_)"))
```

"Does the result of running the following code surprise you? How do the select helpers deal with upper and lower case by default? How can you change that default?"

```{r}
#| eval: false
#| code-fold: true
flights |> 
  select(contains("TIME"))
```

"Rename `air_time` to `air_time_min` to indicate units of measurement and move it to the beginning of the data frame."

```{r}
#| eval: false
#| code-fold: true
flights |> 
  rename(air_time_min = air_time) |> 
  relocate(air_time_min, .before = 1)
```

"Why doesn’t the following work, and what does the error mean?"

```{r}
#| eval: false
flights |> 
  select(tailnum) |> 
  arrange(arr_delay)
```

### Activity

Now for some linguistic data. Download the "data_FRC_spch_rate.xlsx" Excel file from the Datasets module in the LMS ([here](https://byu.instructure.com/courses/25080/files/7945989?module_item_id=1966275)). This dataset was used to write the article in the journal *Corpus Linguistics and Linguistic Theory* [here](https://doi.org/10.1515/cllt-2020-0016).

Open it up in Excel (or Google Sheets) and inspect the columns in the `data` sheet and read what type of data each column holds in the `legend` sheet.

Now, read in the `data` sheet into R as a data frame (probably a tibble). Take some time to practice using the functions (i.e., verbs) that work with rows and the functions that work with columns.

```{r}
#| eval: false
library("readxl")
library("tidyverse")
setwd("/pathway/to/dir")
sibilants <- read_excel("data_FRC_spch_rate.xlsx", sheet = "data")
```

Then, try the following exercises. After a good-faith effort to complete these exercises on your own, if you need help take a look at Dr. Brown's code.

1.  Keep only the rows with tokens of /s/ that were maintained as a sibilant.

    ```{r}
    #| eval: false
    #| code-fold: true
    sibilants |> 
      filter(s == "maintained")
    ```

2.  Keep only the rows of tokens of /s/ that were deleted (no sibilance) in word final position.

    ```{r}
    #| eval: false
    #| code-fold: true
    sibilants |> 
      filter(s == "deleted", wd_pos == "wd_final")
    ```

3.  Create an alphabetized list of unique words with /s/ at the beginning of the word and following by high vowels.

    ```{r}
    #| eval: false
    #| code-fold: true
    sibilants |> 
      filter(wd_pos == "wd_initial", sound_post == "HiV") |> 
      distinct(word) |> 
      arrange(word)
    ```

4.  Sort the data frame in descending order by lexical frequency, and then in ascending order by word.

    ```{r}
    #| eval: false
    #| code-fold: true
    sibilants |> 
      arrange(desc(lex_freq), word)
    ```

5.  Keep only rows with tokens with a speech rate of between 8 and 12 segments per second. Hint: The data type of the column with speech rate (i.e., `spch_rate`) may not have been read in as a numeric (aka. float) value. So, first you may need to coerce that column to a numeric data type with the base R function `as.numeric()`.

    ```{r}
    #| eval: false
    #| code-fold: true
    sibilants |> 
      mutate(spch_rate = as.numeric(spch_rate)) |> 
      filter(spch_rate >= 8 & spch_rate <= 12)
    ```

6.  Create an alphabetized list of unique words with /s/ in which /s/ occurred in a tonic syllable.

    ```{r}
    #| eval: false
    #| code-fold: true
    sibilants |> 
      filter(stress == "tonic") |> 
      distinct(word) |> 
      arrange(word)
    ```

## Group by

Another super useful function in `dplyr` is `group_up()`. It doesn't change the data frame itself, but it performs subsequent functions based on the levels of a column or columns. It is often used in conjugation with the `summarize()` function to get group-level information.

As an example, let's first calculate mean departure delay of all flights in the `nycflights13` dataset, then next, let's calculate the mean departure delay by airline (i.e., `carrier` column). However, as a preprocessing step, we need to remove rows what don't have a number in `dep_delay`, but rather an `NA`, because the `mean()` function errs out with missing values. There are two ways to do this: (1) use `filter()` to keep only rows that don't have an `NA` in the `dep_delay` column, or (2) tell the `mean()` function to remove the `NA`s with the `na.rm` argument.

Here's how to use `filter()` for that purpose:

```{r}
#| eval: false
library("nycflights13")
library("tidyverse")

flights |> 
  filter(!is.na(dep_delay)) |>  # mind the exclamation point
  summarize(average_departure_delay = mean(dep_delay))
```

And here we use the `na.rm` argument within the `mean()` function:

```{r}
#| eval: false
flights |> 
  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE))
```

So, the mean departure delay across all airlines is 12.6 minutes. The natural follow-up question is whether some airlines have longer delays than others. Enter our new friend `group_by()`. Now, let's perform this mean operation based on the levels (aka. values) of the `carrier` column in order to get the mean departure delay time by airline:

```{r}
#| eval: false
flights |> 
  group_by(carrier) |> # we group before getting the mean
  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE))
```

Micro-activity: Modify the previous code block to sort the resulting data frame so that the airline with the shortest average delay is at the top of the resulting data frame and the airline with the longest average delay is at the bottom. After a good-faith effort, if you need some help, take a look at Dr. Brown's code.

```{r}
#| eval: false
#| code-fold: true
flights |> 
  group_by(carrier) |> 
  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE)) |> 
  arrange(average_departure_delay)
```

If you need to group by several columns, just add the additional column names to the `group_by()` call, for example:

```{r}
#| eval: false
flights |> 
  group_by(carrier, origin) |> 
  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE))
```

You don't have to use `group_by()` only with `summarize()`, but it's a common use case. And, importantly, after `summarize()` has finished its work, it ungroups the data frame. If you were to use `group_by()` with other functions, they don't automatically ungroup the data frame, and you would have explicitly do so with the `ungroup()` function. Here's an example:

```{r}
#| eval: false
flights |> 
  group_by(carrier) |> 
  mutate(aver_dep_delay_airline = mean(dep_delay, na.rm = TRUE)) |> 
  select(carrier, dep_delay, aver_dep_delay_airline)
```

Notice that the output says `Groups: carrier [16]`. This means that the data frame is still grouped by the 16 unique levels (aka. values) in the `carrier` column. In order to ungroup it, we can call `ungroup()` after we don't need the groups anymore:

```{r}
#| eval: false
flights |> 
  group_by(carrier) |> 
  mutate(aver_dep_delay_airline = mean(dep_delay, na.rm = TRUE)) |> 
  select(carrier, dep_delay, aver_dep_delay_airline) |> 
  ungroup()
```

### Activity

Use the `data` sheet within the `data_FRC_spch_rate.xlsx` Excel workbook to get familiar with the `group_by()` and `summarize()`.

```{r}
#| eval: false
library("readxl")
library("tidyverse")
setwd("/pathway/to/dir/")
sibilants <- read_excel("data_FRC_spch_rate.xlsx", sheet = "data")
```

Perhaps you might try getting the mean and/or median speech rate (i.e., column name `spch_rate`) based on whether the /s/ was maintained as a sibilant or deleted (using the column `s`). Note: You'll need to remove rows in the data frame that have `NaN` in `spch_rate` and then you need to convert `spch_rate` to a numeric data type. You can do both of these preprocessing steps right within the pipeline before getting the central tendency measures, if you want. If you get stuck, take a look at Dr. Brown's code below.

```{r}
#| eval: false
#| code-fold: true
sibilants |> 
  filter(spch_rate != "NaN") |>  # more rows with "NaN"
  mutate(spch_rate = as.numeric(spch_rate)) |>  # convert data type from character to numeric
  group_by(s) |> 
  summarize(
    mean_spch_rate = mean(spch_rate),
    median_spch_rate = median(spch_rate))
```

Activity: Try some other analyses with `group_by()` and `summarize()` and be prepared to share with a neighbor and/or the class.

## Joining data frames

Joining or merging two tables into one is often a necessary step in data analysis, including in linguistic data analysis. For example, let's say we have one data frame (perhaps coming from an Excel file) that has tokens of sounds in words, and we have another data frame (perhaps from a CSV file) with frequencies of words from a large corpus. It would be slick to get the frequencies of word from the frequency data frame and put the appropriate frequency next to the words in the sound data frame. Enter the family of join functions in `dplyr` [here](https://dplyr.tidyverse.org/reference/mutate-joins.html).

Let's start with a super simple example in order to understand the concept, then we'll apply this to bigger datasets. Let's create a data frame (i.e., `tibble`) with two columns: a person, and the fruit that they said during a conversation:

```{r}
#| eval: false
library(tidyverse)
main_df <- tibble(
  speaker = c("Bob", "Bob", "Billy", "Billy", "Britta", "Britta", "Bonnie"), 
  word = c("apple", "banana", "orange", "mango", "apple", "manzana", "kiwi")
)
print(main_df)
```

Now, let's create a data frame with some of the speakers' ages:

```{r}
#| eval: false
ages <- tibble(
  person = c("Bonnie", "Billy", "Bob"), 
  age = c(47, 6, 95)
)
print(ages)
```

You may have noticed that while Britta said a couple words in our `main_df` data frame, she isn't listed in the `ages` data frame. We'll see what happens below when a value in a column in the main data frame doesn't have a corresponding value in the to-be-joined data frame.

In order to create a new column with the age of each of the speakers in `main_df`, we can use the `left_join()` function to join `main_df` and `ages`. This function returns every row in the left-hand or first data frame in the function call (which, if we're using a pipe operator, is the data frame to the left of the pipe), and only the rows in the right-hand or second data frame that have a corresponding value based on the columns specified with the `by` argument. By default, `left_join` joins by common column names, but we can explicitly specify which column in the left-hand data frame corresponds to which column in the right-hand data frame with the `by` argument. Let's take a look:

```{r}
#| eval: false
main_df |> 
  left_join(ages, by = join_by(speaker == person))
```

Question: Why do we have `speaker` to the left of the double equal sign and `person` to the right?

You probably noticed that Britta's age is `NA` because she isn't listed in the `ages` data frame. If that's a deal-breaker, then we can remove rows with `NA`s with `filter()` as a final step in the pipeline (mind the exclamation point before `is.na()`):

```{r}
#| eval: false
main_df |> 
  left_join(ages, by = join_by(speaker == person)) |> 
  filter(!is.na(age))
```

### Toy activity

Let's create a toy data frame with frequencies of the words said by the speakers in `main_df`:

```{r}
#| eval: false
freqs <- tibble(
  wd = c("apple", "banana", "kiwi", "mango"), 
  freq = c(123, 234, 345, 456)
)
print(freqs)
```

Now, join the `freqs` data frame to the `main_df` so that the frequency of each word is in a new column to the right of the column holding the age of the speakers, so that you end up with a data frame with four columns: `speaker`, `word`, `age`, `freq`. Give it a try, but if you get stuck, take a look at Dr. Brown's code below. Hint: Make sure you are super aware of the column names in the various data frames that you need to join, so that you can give the correct names to the `by` argument.

```{r}
#| eval: false
#| code-fold: true
main_df |> 
  left_join(ages, by = join_by(speaker == person)) |> 
  left_join(freqs, by = join_by(word == wd))
```

### Activity (with a review)

Let's ramp it up to a real activity. Try the following:

1.  Review: Create a data frame with at least one column named `word` (i.e., that has one word per row) using text files of your choice (e.g., *Saints* or texts from Project Gutenberg). You might like to use a regular expression to find different words (e.g., `"\\w+ed\\b"` or `"\\bre\\w+"`), as the next steps will be boring if all rows have the same word.

2.  Review: Create a data frame of frequencies of words from text files of your choice or webscrape a frequency list from the internet (e.g., Wikitionary).

3.  Note: If after a good-faith effort to complete the previous two steps, you can't remember how to do these tasks, download and use two files in the LMS labeled `Dataset_for_over_dan.csv` and `freqs_dan.csv`. The first file has a keyword-in-context display of words in Danish that start with *for* or *over* while the second file has frequencies of words in Danish.

4.  Here's the main exercise of the activity: Join the data frame with words to the data frame with frequencies, so that you have a new column (called `freq`) with the frequency of the word in the `word` (or `node`) column. Again, I can't stress enough the importance of taking the time to double check that you know what the names are of the columns in the several data frame, so that you can correctly pass thme to the `by` argument.

## Separate and unite columns

Other useful functions, this time from the `tidyr` package within `tidyverse`, are the 3-member family of `separate_*` functions (i.e., `separate_wider_delim()`, `separate_wider_position()`, and `separate_wider_regex()`) and the `unite()` function.

The `separate_*` functions are transparently named and split up one column into two or more columns.

-   The `separate_wider_delim()` function splits on a delimiter given to the `delim` argument;

-   The `separate_wider_position()`function splits at fixed character widths given to the `widths` argument;

-   The `separate_wider_regex()` function splits on a regular expression given to the `patterns` argument.

Let's create a data frame with one column with two pieces of information in each row: the speaker who said something, following by a semi-colon, followed by what the speaker said:

```{r}
#| eval: false
library(tidyverse)
hungry <- tibble(
  conversation = c("Bobby: I'm hungry!", "Cathy: Let's stop at In-n-Out. What do you say to that?!", "Bobby: Sounds good to me. What do you say João?", "João: Eu digo que sim!", "Roberto: Yo también.")
)
print(hungry)
```

Let's start with `separate_wider_delim()`. Pay attention to the arguments in the function.

```{r}
#| eval: false
hungry |> 
  separate_wider_delim(cols = conversation, delim = ": ", names = c("person", "utterance"))
```

The function `separate_wider_regex()` takes a different approach. Rather than splitting on the given fixed-width delimiter given to the `delim` argument in the previous function, this function takes regular expressions that are used to extract matches out of the string in the row. Specifically, you give it a named character vector in which the names become column names and the elements of the vector are regular expressions to be matched. If no name is given, then that match doesn't make it into the resulting data frame. Let's use this function on the same `hungry` data frame from above. Notice that the regex `":"` doesn't have a name, and therefore isn't returned in the resulting data frame.

```{r}
#| eval: false
hungry |> 
  separate_wider_regex(cols = conversation, patterns = c(person = "^[^:]+", ":", utterance = "[^:]+$"))
```

For good measure, we'd probably want to trim off that extra space at the beginning of each utterance:

```{r}
#| eval: false
hungry |> 
  separate_wider_regex(cols = conversation, patterns = c(person = "^[^:]+", ":", utterance = "[^:]+$")) |> 
  mutate(utterance = str_trim(utterance))
```

Dr. Brown hasn't needed `separate_wider_position()` yet in his life, so let's move on! If you think you might need it, take a look at the docs [here](https://tidyr.tidyverse.org/reference/separate_wider_delim.html).

The `unite()` function is the inverse of the `separate_*` functions, that is, it merges into one column two or more columns. It takes was input the data frame (probably piped in with the pipe operator), the `col` argument which specifies the name of the new column, then the names of the two or more columns to be merged, and the `sep` argument which specifies the character(s) used to join the values in the new column. There are other arguments; take a look at the docs [here](https://tidyr.tidyverse.org/reference/unite.html?q=unite#null).

Let's continue with our `hungry` data frame from above. Let's put the `person` and `utterance` columns back together, but this time with a dash separating the speakers and their corresponding utterances.

```{r}
#| eval: false
df_separated <- hungry |> 
  separate_wider_delim(cols = conversation, delim = ": ", names = c("person", "utterance"))
print(df_separated)

df_separated |> 
  unite(col = "combined", person:utterance, sep = " - ")
```

### Activity

Use some of the `separate_*()` functions and `unite()` on the data frame of your choice.

## Pivoting data frames

Another useful, and often necessary, step in (linguistic) data analysis is changing the shape of a data frame before running an analysis. Some functions in R and in other languages (e.g., Python and Julia) and in other software (e.g., Excel) require the data frame to be in a specific shape in order to do certain analyses. Being able to transform the data to that shape is an important preprocessing step.

The two main functions for changing the shape of a data frame within `tidyverse` are [`pivot_longer()`](https://tidyr.tidyverse.org/reference/pivot_longer.html) and [`pivot_wider()`](https://tidyr.tidyverse.org/reference/pivot_wider.html). The function `pivot_longer()` increases the number of rows and decreases the number of columns, while `pivot_wider()` decreases the number of rows and increases the number of columns.

Let's take a super simple example. Let's create a toy data frame with one row and five columns with the ages of people in a sample:

```{r}
#| eval: false
library("tidyverse")
ages <- tibble(Sammy = 20, Edna = 18, Luisa = 16, Heidi = 14, Evelyn = 12)
print(ages)
```

As we see, each column represents one person, with the age in the row. Let's reshape the data frame so that we end up with five rows and two columns, with each row giving info about one person, and the first column giving the person's name and the second column giving the person's age:

```{r}
#| eval: false
ages |> 
  pivot_longer(cols = Sammy:Evelyn, names_to = "person", values_to = "age")
```

The resulting data frame is a "tidy" data frame because each column has only one variable (e.g., person or age) and the rows represent individual observations. The pre-transformed data frame above was **not** tidy because each column had more than one variable (e.g., person and age). Read [Hadley Wickham's paper](https://doi.org/10.18637/jss.v059.i10) for a full explanation of tidy data.

The `pivot_wider()` does the opposite, that is, it make a long(er) data frame wide(r). Let's create another toy data frame, this time in long format and make it wide:

```{r}
#| eval: false
heights <- tibble(
  person = c("Sammy", "Edna", "Luisa", "Heidi", "Evelyn"),
  height_in = c(72, 70, 69, 63, 64))
print(heights)
```

Let's pivot this data frame to a wide format so that there are five columns, with each person's name was the column name, and the rows are the corresponding heights in inches:

```{r}
#| eval: false
heights |> 
  pivot_wider(names_from = person, values_from = height_in)
```

### Toy activity and review

Create one data frame with the ages and heights of the five people in the above toy examples. Hint: In addition to `pivot_longer()` and/or `pivot_wider()`, you'll also need our ol' friend `left_join()` that we saw above.

After a good-faith effort, if you need help take a look at Dr. Brown's code below:

```{r}
#| eval: false
#| code-fold: true
ages |> 
  pivot_longer(cols = Sammy:Evelyn, names_to = "person", values_to = "age") |> 
  left_join(heights, by = join_by(person == person))
```

### Vowels

Let's take a look at a linguistic example from Dr. Stanley. Download the `sample_vowel_data.csv` file from the LMS \> Datasets. Currently, each row represents one time point for each vowel. There are 27 rows containing F1, F2, and F3 measurements from three tokens of three vowels each, at three timepoints.

Let's load the dataset into our R session as a data frame:

```{r}
#| eval: false
library("tidyverse")
vowels <- read_csv("/pathway/to/sample_vowel_data.csv")
print(vowels)
```

Let's change it into a wider format so that the unit of observation (i.e., row) is one vowel. There are 9 rows. Also, save it to a variable named `widest` so we can work with it later.

```{r}
#| eval: false
widest <- vowels |> 
  pivot_wider(names_from = percent, values_from = c(F1, F2, F3))
print(widest)
```

Let's change it to a longer format. Now the unit of measurement is one format measurement, per time point, per vowel token. There are 81 rows. Note that I'll need to specify which columns should be affected with the `cols` argument.

```{r}
#| eval: false
longest <- vowels |> 
    pivot_longer(cols = c(F1, F2, F3), 
                 names_to = "formant", 
                 values_to = "hz") |> 
    arrange(formant)
print(longest)
```

Let's use our `widest` data frame and pivot it longer. Also, as a shorthand for specifying the columns I want to pivot, I can use `matches()` and a regex:

```{r}
#| eval: false
widest |> 
    pivot_longer(cols = matches("F\\d_\\d\\d"), 
                 names_to = "formant_percent", 
                 values_to = "hz")
```

Here, because each column name contained two pieces of information (`F1_25`), the resulting data set after pivoting contains a single column with two pieces of information: `formant` and `percent`.

Enter our ol' friend `separate()`:

```{r}
#| eval: false
widest |> 
    pivot_longer(cols = matches("F\\d_\\d\\d"), 
                 names_to = "formant_percent", 
                 values_to = "hz") |> 
    separate(formant_percent, 
             into = c("formant", "percent"), 
             sep = "_")
```

But the separation above can happen within pivot functions directly. Below, we provide a vector of names as the value of the `names_to` argument. This then requires the use of the `names_sep` argument, which is where you specify the character that separates the two names.

```{r}
#| eval: false
widest |> 
    pivot_longer(cols = matches("F\\d_\\d\\d"), 
                 names_to = c("formant", "percent"), 
                 names_sep = "_", 
                 values_to = "hz")
```

### Ramping it up

Let's say we start off with what we see as the widest version of the data in the `widest` data frame. This is not a hypothetical: this is how a lot of sociophonetics software returns the data to the user! We want to pivot it so that it looks like our original version in the `vowels` data frame. One way is to make it longer and then wider again.

```{r}
#| eval: false

widest |> 
    pivot_longer(cols = matches("F\\d_\\d\\d"), 
                 names_to = c("formant", "percent"),
                 names_sep = "_",
                 values_to = "hz") |> 
    pivot_wider(names_from = formant,
                values_from = hz)
```

This produces the correct output, but it's a little clunky to do two pivots.

Enter the **black magic** that is `.value`. So instead, we replace the `formant` column name (in the `names_to` argument) with `.value` (mind the period). This then, somehow, takes what would have been the `formant` column and pivots it wider. Note that when we do this, we don't need the `values_from` argument anymore.

```{r}
#| eval: false
widest |> 
    pivot_longer(cols = matches("F\\d_\\d\\d"), 
                 names_to = c(".value", "percent"),
                 names_sep = "_")
```

### Big badboy activity

Download the "RPM_2019_for_Reliability.xlsx" Excel file from LMS \> Datasets module and transform (aka. transpose) the dataset into the shape asked for in the email below that Dr. Brown received from his brother [Alan](https://mcl.as.uky.edu/users/avbrow2):

"I’m sending along that data set that I had originally sent that had those problems. Stayc cleaned it all up and we took some students out who spoke other languages at home. The sheet of interest is the first one labeled “For Earl”. As we had talked about, we just need the data oriented horizontally by item number so across the top the columns would run from 1 to 36 left to right. Each row, then, would represent a different student and ‘1’s or ‘0’s in the columns would indicate correct or incorrect for each item/column. The initial three columns to the left would be the demographic information like gender, grade, immersion/non-immersion. Let me know if that doesn’t make sense."

The first ten rows (i.e., students, of the 104 students) of the transposed dataset should look like the following:

![Transposed dataset](/images/transposed_dataset.png){fig-alt="transposed dataset"}

A couple words to the wise:

-   As is often the case in the wild, this dataset is squirrelly. Take a look at the bottom of the dataset (in Excel) and you'll see that there is "104" below the last row of the first column. You'll need to use the `range` argument in the `read_excel()` function (within the `readxl` package) to read in only the cells with actual student data, that is, rows with something in all five columns.

-   The "104" indicates that there are 104 students in the dataset. The answers to the 36 questions that each student responded to are in sets of 36 rows, with the question number indicated by in the `Page/Item` column. You'll see that the numbers in that column go from 1 to 36, and then start over at 1 with the next student.

-   There is no unique identifier (e.g., ID number) for each student. Before transposing the data frame, you'll need to create a new column with a unique identifier. You can use the `rep()` function, with its `each` argument, to do this. See [Stack Overflow thread](https://stackoverflow.com/a/6432093/2884875) for an example.

-   After transposing the data frame, you should move the column with the unique identifier to the far left.

After a good-faith effort to complete this activity, if you need help take a look at Dr. Brown's code below:

```{r}
#| eval: false
#| code-fold: true
library("tidyverse")
library("readxl")
"/pathway/to/RPM_2019_for_Reliability.xlsx" |> 
  read_excel(range = "For Earl!A1:E3745") |> 
  mutate(id = rep(1:104, each = 36)) |> 
  pivot_wider(names_from = "Page/Item", values_from = "Correct/incorrect") |> 
  relocate(id, .before = 1)
```

## Working with categorical variables

There are a few handy functions in the `forcats` package within `tidyverse` for working with categorical variables (aka. factors).

The [`fct_recode()`](https://forcats.tidyverse.org/reference/fct_recode.html) function allows the user to manually change or combine levels (aka. values) of a categorical variable. Let's create a toy data frame with words and vowels:

```{r}
#| eval: false
library("tidyverse")
words <- tibble(
  word = c("wasp", "fleece", "beat", "bit", "pat", "bus", "hot", "cool", "firm", "father"),
  vowel = c("[ɑ]", "[i]", "[i]", "[ɪ]", "[æ]", "[ʌ]", "[ɔ]", "[u]", "[ɚ]", "[ɑ]")
)
print(words)
```

Now, let's create a new column in which we describe (some of) the vowels:

```{r}
#| eval: false
words |> 
  mutate(description = fct_recode(vowel, "high front closed" = "[i]", "r colored schwa" = "[ɚ]", "stressed schwa (wedge)" = "[ʌ]"))
```

The [`fct_collapse()`](https://forcats.tidyverse.org/reference/fct_collapse.html) function allows the user to put levels into groups:

```{r}
#| eval: false

words |> 
  mutate(height = fct_collapse(vowel, 
                               hi_V = c("[i]", "[ɪ]", "[u]"), 
                               lo_V = c("[ɑ]", "[æ]"),
                               other_level = "mid_V"))

```

Question: What does the `other_level` argument do in the function call above?

The [`fct_relevel()`](https://forcats.tidyverse.org/reference/fct_relevel.html) function allows the user to reorder the levels of a categorical variable. This is often useful when plotting data and the user wants an order of levels different from alphabetical order (the default).

Let's create a toy data frame of voiceless plosives and their voice onset times (VOT):

```{r}
#| eval: false
library(tidyverse)
voiceless_plosives <- tibble(
  plosive = c("t", "k", "p", "p", "k", "t", "p", "k", "t"),
  vot = c(23, 34, 45, 56, 67, 78, 89, 91, 21)
)
print(voiceless_plosives)
```

If we create a boxplot (preview of what's coming soon!), we see that "k" is on the left, followed by "p" and "t", because the default way to sort levels is by alphabetical order:

```{r}
#| eval: false
voiceless_plosives |> 
  ggplot(aes(x = plosive, y = vot)) +
  geom_boxplot()
```

We probably want "p" followed by "t" and then "k", as that's how they are presented in linguistic literature, based on the place of articulation. Let's use `fct_relevel()` to change the internal order of the levels, and then replot their VOTs:

```{r}
#| eval: false
voiceless_plosives |>
  mutate(plosive = fct_relevel(plosive, "p", "t", "k")) |> 
  ggplot(aes(x = plosive, y = vot)) +
  geom_boxplot()
```

### Activity

Let's create a toy data frame with Spanish words with a dental or alveolar word-final sibilant:

```{r}
#| eval: false

library("tidyverse")
sibilants <- tibble(
  person = c("Raúl", "Raúl", "José", "José", "María"),
  target_wd = c("árboles", "mesas", "lápiz", "es", "pues"),
  next_wd = c("de", "en", "y", "que", "."),
  next_segment = c("d", "e", "i", "k", "#")
)
print(sibilants)
```

Your task is to create a new column that groups the vowels into one level, the consonants into their own group, and then all other following segments should be placed into an `other` level.

After a good-faith effort, if you need help, take a look at Dr. Brown's code below:

```{r}
#| eval: false
#| code-fold: true

sibilants |>  
  mutate(next_sound_type = fct_collapse(next_segment, vowel = c("i","e"), consonant = c("d", "k"), other_level = "other"))

```
