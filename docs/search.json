[
  {
    "objectID": "lessons/File_IO.html",
    "href": "lessons/File_IO.html",
    "title": "File I/O in R",
    "section": "",
    "text": "Students will read data from files and writing data to files."
  },
  {
    "objectID": "lessons/File_IO.html#objective",
    "href": "lessons/File_IO.html#objective",
    "title": "File I/O in R",
    "section": "",
    "text": "Students will read data from files and writing data to files."
  },
  {
    "objectID": "lessons/File_IO.html#directory-operations",
    "href": "lessons/File_IO.html#directory-operations",
    "title": "File I/O in R",
    "section": "Directory operations",
    "text": "Directory operations\n\nComputers are structured in a hierarchal filesystem (e.g., “/Users/Fernando/Documents/my_novel.docx”).\nOften, R scripts need to change directories (aka. folders) in order to access specific files.\nR has several functions to move around a filesystem. Here are a few of them:\n\ngetwd() (here) prints to the console the current working directory (i.e., the directory at the script currently has access to).\nsetwd() (here) sets a directory so that the script has access to it.\ndir() (here, an alias for list.files()) lists the contents of a directory, either only in the one directory or also in all sub-directories (with the recursive argument)."
  },
  {
    "objectID": "lessons/File_IO.html#text-files-.txt",
    "href": "lessons/File_IO.html#text-files-.txt",
    "title": "File I/O in R",
    "section": "Text files (.txt)",
    "text": "Text files (.txt)\n\nReading in data (aka. input) from text files can be accomplished in two ways:\n\nWay 1: Slurp all text in the file at once and hold it in the working memory of the computer:\n\nscan() (here) returns a vector (the default) or list.\nreadLines() (here) in base R returns a vector with each line (i.e., hard return) in the input file as an element. This is a bare-bones version of scan().\nread_lines() (here) in the readr package (part of tidyverse) is a quicker version of readLines().\nread_file() (here) in the readr package slurps all text into a single string.\n\nWay 2: Read data line-by-line:\n\nThis is useful when the text file is massive and would be difficult to hold in memory at once. This approach holds only one line at a time in memory.\nSteps:\n\nCreate a connection to the file with the file() function (here).\nUse the readLines(n = 1) function in a while loop.\nSee an example here.\n\n\n\nWriting out data (aka. output) to a text files can be accomplished with several functions:\n\ncat() (here) in base R can write out to a text file when the file argument gives a pathway to a file.\nwriteLines() (here) is the output version of readLines() mentioned above.\nwrite_lines() (here) function is the output version of read_lines() mentioned above.\nwrite_file() (here) output-equivalent of read_file() above."
  },
  {
    "objectID": "lessons/File_IO.html#csv-files-.csv",
    "href": "lessons/File_IO.html#csv-files-.csv",
    "title": "File I/O in R",
    "section": "CSV files (.csv)",
    "text": "CSV files (.csv)\n\nReading in tabular datasets from CSV files can be accomplished with several functions:\n\nread.table() (here) is a versatile function with many arguments, that returns a data frame.\nread_csv() (here) in the readr package (part of tidyverse) reads CSV files that have a comma as the separator between columns, and returns a tibble.\nread_tsv() (here) in the readr package reads TSV files that have a tab as the separator between columns, and returns a tibble.\n\nNote: Some files with the extension .csv or .txt are actually .tsv files, that is, the column separator is a tab.\n\n\nWriting out a data frame to a CSV files is a cinch:\n\nwrite.table() (here) in base R is the output equivalent of read.table() mentioned above.\nwrite_csv() (here) in the readr package is the output equivalent of the read_csv() mentioned above.\n\n\n\nActivity\n\nInstall the tidyverse suite of packages with the command install.packages(\"tidyverse\") in the console.\nDownload some TXT files of your choice (perhaps from Project Gutenberg or Saints.zip from the Canvas Module “Datasets”).\nCreate a script that reads in all TXT files in a directory (and perhaps any subdirectories) and simply print the text to the console.\n\nInclude library(\"tidyverse\") at the top of your script (i.e., .r file.)\n\nRamp it up by breaking up the text into words and printing those to the console.\nNow for some fun, as a class let’s count the number of words in each text file, and print the name of the file and the number of words to the console.\nAs a final step, let’s write out a CSV file with two columns: column A = the name of the file, column B = the number of words in that file."
  },
  {
    "objectID": "lessons/File_IO.html#excel-files",
    "href": "lessons/File_IO.html#excel-files",
    "title": "File I/O in R",
    "section": "Excel files",
    "text": "Excel files\n\nReading in (input) an Excel is easy with readxl::read_excel().\n\nYou can specify which worksheet to read (with the sheet argument), or even a specific set of cells within a specific worksheet (with the range argument).\n\nWriting out (output)\n\nxlsx::write.xlsx() function does the trick.\n\n\n\nActivity\n\nDownload an Excel (.xlsx) file of your choice (perhaps from the Module “Datasets” in Canvas) or use one that’s already on your harddrive.\nOpen the Excel file (in Excel) and inspect the worksheet(s) to figure out where the data is (i.e., sheet name, cell range).\nRead in the appropriate worksheet and display it within RStudio with the view() function."
  },
  {
    "objectID": "lessons/File_IO.html#spss-.sav-stata-.dta-and-sas-.sas-files",
    "href": "lessons/File_IO.html#spss-.sav-stata-.dta-and-sas-.sas-files",
    "title": "File I/O in R",
    "section": "SPSS (.sav), Stata (.dta), and SAS (.sas) files",
    "text": "SPSS (.sav), Stata (.dta), and SAS (.sas) files\n\nReading in (input)\n\nThe haven R package does the trick.\n\nWriting out (output)\n\nWho cares? You should output it as something more cross-platform-friendly like CSV.\n\n\n\nActivity\n\nDownload the SPSS (.sav) file in the CMS.\nRead in the SPSS file and display it with view()."
  },
  {
    "objectID": "lessons/File_IO.html#feather-.feather-files",
    "href": "lessons/File_IO.html#feather-.feather-files",
    "title": "File I/O in R",
    "section": "Feather (.feather) files",
    "text": "Feather (.feather) files\n\nThis file format is quickly read and written, which are good for big data files.\nReading in (input)\n\nThe feather::read_feather() function does it.\n\nWriting out (output)\n\nThe feather::write_feather() function does it."
  },
  {
    "objectID": "lessons/data_structures.html",
    "href": "lessons/data_structures.html",
    "title": "Data structures in R",
    "section": "",
    "text": "Students will become familiar with common data structures in R."
  },
  {
    "objectID": "lessons/data_structures.html#objective",
    "href": "lessons/data_structures.html#objective",
    "title": "Data structures in R",
    "section": "",
    "text": "Students will become familiar with common data structures in R."
  },
  {
    "objectID": "lessons/data_structures.html#common-data-structures-in-r",
    "href": "lessons/data_structures.html#common-data-structures-in-r",
    "title": "Data structures in R",
    "section": "Common data structures in R",
    "text": "Common data structures in R\n\nvector: A single dimension collection of values of the same data type (e.g., all numeric or all character). Kinda like a list in Python or an array in Julia.\n\nNote: Values of different data types are coerced to the more complex data type, for example, if a numeric (aka. float) and an integer are put into the same vector, both values will have a numeric (aka. float) data type.\nA vector (and list, see below) can be created with the c() function (doc here).\n\ndata.frame: A tabular data structure with columns and rows, much like a table in a spreadsheet like Excel or Google Sheets. See doc here.\ntibble: A slightly modified, and better, data.frame in the tibble (doc here) package within the tidyverse metapackage or ecosystem (doc here). See doc here.\ndata.table: Another tabular data structure, written in C under the hood (so it’s fast) from the data.table package here. See a tutorial here.\nmatrix: Another tabular data structure in base R. Less common than data.frame and tibble and data.table. See doc here.\nlist: A flexible data structure that can hold whatever, including other data structures. See doc here and tutorial here.\n\nActivity\n\nStudents create a few vectors of the same length (i.e., the same number of elements).\nStudents create a single data frame with the several vectors created above. The doc here may be helpful.\nStudents create several data frames and put them in a list.\nStudent iterate over the list with a for loop and print each data frame to the console.\nStudents iterate over the list and then iterate over the row of each data frame, and print each row."
  },
  {
    "objectID": "lessons/create_freq.html",
    "href": "lessons/create_freq.html",
    "title": "Creating frequency lists",
    "section": "",
    "text": "Students will create frequency lists from files on their harddrive."
  },
  {
    "objectID": "lessons/create_freq.html#objective",
    "href": "lessons/create_freq.html#objective",
    "title": "Creating frequency lists",
    "section": "",
    "text": "Students will create frequency lists from files on their harddrive."
  },
  {
    "objectID": "lessons/create_freq.html#frequency-in-language",
    "href": "lessons/create_freq.html#frequency-in-language",
    "title": "Creating frequency lists",
    "section": "Frequency in language",
    "text": "Frequency in language\nFrequency is an important construct in many areas of language, and more generally, in human cognition (see our amazing pattern recognition abilities). Frequency affects which words and phrases are learned first, in both L1s and L2s. More frequent grammatical constructions are learned before (and better for L2 speakers) than less frequent ones (e.g., active voice vs. passive voice in English). More frequency words experience phonetically-driven sound changes (e.g., lenition) first. More frequent words resist analogical leveling (keep -&gt; *keeped, but leap -&gt; leapt -&gt; leaped).\nIn summary, frequency is super important, and being able to calculate frequencies of language features, especially words, is an important skill for a language-oriented data analyst."
  },
  {
    "objectID": "lessons/create_freq.html#getting-frequencies-of-words-in-files",
    "href": "lessons/create_freq.html#getting-frequencies-of-words-in-files",
    "title": "Creating frequency lists",
    "section": "Getting frequencies of words in files",
    "text": "Getting frequencies of words in files\nThe logic to calculate frequencies of files on a harddrive in R is simple (when holding all words in RAM; see an alternative below):\n\nParse the files so that all words in all files are in a single vector with N elements (N being the total number of words across all files);\nAsk R to count up the number of word tokens per word type in the vector.\n\nThere are two ways (Dr. Brown will show) to count up word tokens per word type:\n\nWith the base R table() function;\nConvert the vector into a one-column data frame and then use count() in tidyverse."
  },
  {
    "objectID": "lessons/create_freq.html#the-table-function",
    "href": "lessons/create_freq.html#the-table-function",
    "title": "Creating frequency lists",
    "section": "The table() function",
    "text": "The table() function\nLet’s create a toy example:\n\nlibrary(\"tidyverse\")\n\n# create a sentence\nsentence &lt;- \"I like linguistics, and I like my students, but I love my wife and chidren. Sorry students. Maybe next time.\"\n\n# uppercase (or lowercase) the string, so that uppercase and lowercase words (e.g., \"The\" and \"the\" and \"THE\") are treated as the same word\nsentence &lt;- str_to_upper(sentence)\n\n# tokenize the string into words\nwords &lt;- str_extract_all(sentence, \"[-'’A-Z]+\")\n\n# unlist the list so that we're left with a vector\nwords &lt;- unlist(words)\n\n# throw the vector at table() and watch the magic happen!\nfreqs &lt;- table(words)\nprint(freqs)\n\nThe output of the table() function is a named one-dimensional array (like a vector), of class table. The values are the integers (i.e., the frequencies), and each integer has a name (i.e., a word). In order to extract only the names, you can use the names() function:\n\nprint(names(freqs))\n\nWe probably want to sort the frequencies in descending order:\n\nfreqs &lt;- sort(freqs, decreasing = TRUE)\nprint(freqs)\n\nThe frequencies and their names (i.e., the words) can be put into a data frame and then exported out as a CSV file.\n\nActivity\nDo just that, that is, export to a CSV file the words and frequencies using the result of the above toy example. One tip: You’ll need to coerce the data type of the numbers to integer with as.integer(freqs) when assigning the array to a column in the data frame.\nAfter giving it a good-faith effort, if you need help, take a look at Dr. Brown’s code below:\n\n\nCode\ndf &lt;- tibble(wd = names(freqs), freq = as.integer(freqs))\nwrite_csv(df, file = \"freqs.csv\")\n\n\nLet’s ramp it up:\nCreate a frequency list of words in many text files of your choice (e.g., from Project Gutenberg or the Saints files in the LMS).\nAfter a good-faith effort, if you need help, take a look at Dr. Brown’s start to the code below:\n\n\nCode\n# get filenames\nfilenames &lt;- dir(path = \"/pathway/to/Saints/txt/\", pattern = \"\\\\.txt$\", full.names = TRUE, recursive = TRUE)\n\n# create a collector string to collect the text of all files\nall_str &lt;- \"\"\n\n# loop over the filenames\nfor (filename in filenames) {\n  \n  # get the text from the current file\n  txt &lt;- read_file(filename)\n  \n  # add the text of the current file to the collector string\n  all_str &lt;- str_c(all_str, txt, sep = \" \")\n  \n}  # next filename\n\n\nAfter the for loop, the variable all_str is one big string with all text from all files. You should now be able to modify the code in the toy example above to get the frequencies of the words in this single big string. Go for it! You got this! Let’s go! You’re a super star! Etc.!"
  },
  {
    "objectID": "lessons/create_freq.html#the-count-function",
    "href": "lessons/create_freq.html#the-count-function",
    "title": "Creating frequency lists",
    "section": "The count() function",
    "text": "The count() function\nA second way (among other ways) is to take the vector with words (not the big single string, but the vector with each word as a separate element) and create a one-column data frame, and then use count() (within tidyverse). Let’s go!\nUsing the words vector from the toy example above:\n\ndf &lt;- tibble(wd = words)\nfreqs &lt;- df %&gt;% count(wd)\nprint(freqs)\n\nQuick little aside: The pipe operator %&gt;% passes the value on the left-hand side of the operator into the function on the right-hand side, as the first argument to that function.\n\nActivity\nThat’s right, it’s time to step up and use the count() function to calculate frequencies and then write them out to a CSV file. Ready… set… go!\nAfter a good-faith effort, if you need some help, take a look at Dr. Brown’s code below:\n\n\nCode\n# using the toy example above:\n# (enjoy all the pipe operators!)\ntibble(wd = words) %&gt;% \n  count(wd) %&gt;% \n  write_csv(file = \"freqs.csv\")"
  },
  {
    "objectID": "lessons/create_freq.html#populate-a-list",
    "href": "lessons/create_freq.html#populate-a-list",
    "title": "Creating frequency lists",
    "section": "Populate a list",
    "text": "Populate a list\nA third way to calculate frequencies of words in files is to create an R list and populate the list with words, one file at a time. This is a good way if the corpus that you want frequencies from is too big to fit in the RAM of your computer, or too big to fit comfortably because it makes your computer work slowly. You can have your script bring into the RAM only one file at a time and populate an R list. This approach to getting frequencies is slower than the previous two, so unless you have a good reason for using this approach, it’s probably best to one of the previous two approaches (fun fact: Dr. Brown likes the count() approach best).\nLet’s take a look at an example using the Saints corpus:\n\n# get filenames\nfilenames &lt;- dir(path = \"/pathway/to/Saints/txt/\", pattern = \"\\\\.txt$\", full.names = TRUE, recursive = TRUE)\n\n# create a collector string to collect the text of all files\nfreqs &lt;- list()\n\n# loop over the filenames\nfor (filename in filenames) {\n  \n  # get the text from the current file and uppercase it\n  txt &lt;- read_file(filename) %&gt;% str_to_upper()\n  \n  # get words in current file and unlist the list into a vector\n  wds &lt;- str_extract_all(txt, \"[-'’A-Z]+\") %&gt;% unlist()\n  \n  # loop over the vector of words\n  for (wd in wds) {\n    \n    # test whether the current word is already in the list\n    if (wd %in% names(freqs)) {\n      \n      # if so, increment the counter for that word by one\n      freqs[[wd]] &lt;- freqs[[wd]] + 1\n      \n    } else {\n      # if not, add an entry for the word and give it a value of one\n      freqs[[wd]] &lt;- 1\n      \n    }  # end if\n  }  # next word  \n}  # next filename\n\n# print out the words and their frequencies to the console\nfor (wd in names(freqs)) {\n  cat(wd, \": \", freqs[[wd]], \"\\n\", sep=\"\")\n}\n\nThe next step would be put the words and their frequencies into a data frame in order to (more easily) sort them and to (more easily) write them out to a CSV file."
  },
  {
    "objectID": "lessons/visualization.html",
    "href": "lessons/visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "Students visualize data with easy-to-interpret plots."
  },
  {
    "objectID": "lessons/visualization.html#objective",
    "href": "lessons/visualization.html#objective",
    "title": "Visualization",
    "section": "",
    "text": "Students visualize data with easy-to-interpret plots."
  },
  {
    "objectID": "lessons/visualization.html#background",
    "href": "lessons/visualization.html#background",
    "title": "Visualization",
    "section": "Background",
    "text": "Background\nR kicks trash with visualizing data. And the go-to package for creating professional-looking plots is ggplot2, one of the core packages of the tidyverse ecosystem. Base R also creates plots, but anybody who’s anybody is using ggplot2. (Okay, maybe I’m overstating this a bit, but just you wait and see if I’m lying.)"
  },
  {
    "objectID": "lessons/visualization.html#basics",
    "href": "lessons/visualization.html#basics",
    "title": "Visualization",
    "section": "Basics",
    "text": "Basics\nFirst, we need some data in a data frame (e.g., a tibble). Let’s use the Data_ptk.xlsx file in the LMS:\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"readxl\")\nlibrary(\"ggmosaic\")\n\nptk &lt;- read_excel(\"../../../data_analysis/datasets/Data_ptk.xlsx\", sheet = \"data\")\nglimpse(ptk)\n\nRows: 1,578\nColumns: 39\n$ ALL_CASE    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ FILE        &lt;chr&gt; \"KS_E_01\", \"KS_E_01\", \"KS_E_01\", \"KS_E_01\", \"KS_E_01\", \"KS…\n$ CASE        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18,…\n$ PRE         &lt;chr&gt; NA, \"Television is a\", \"for\", \"for transmitting and receiv…\n$ HIT         &lt;chr&gt; \"T\", \"t\", \"t\", \"c\", \"c\", \"T\", \"t\", \"t\", \"t\", \"p\", \"t\", \"t\"…\n$ FOL         &lt;chr&gt; \"elevision is a telecommunication medium\", \"elecommunicati…\n$ TIME        &lt;chr&gt; \"00:00:12.980\", \"00:00:12.980\", \"00:00:15.960\", \"00:00:15.…\n$ SECS        &lt;dbl&gt; 12, 12, 15, 15, 15, 22, 22, 22, 25, 25, 25, 25, 28, 28, 28…\n$ START       &lt;dbl&gt; 13.59949, 14.43660, 16.15277, 18.38873, 19.44065, 22.56935…\n$ CP          &lt;dbl&gt; 1, 17, 5, 51, 72, 2, 42, 47, 2, 13, 25, 36, 1, 48, 70, 96,…\n$ WORD        &lt;chr&gt; \"Television\", \"telecommunication\", \"transmitting\", \"can\", …\n$ PHON        &lt;chr&gt; \"t\", \"t\", \"t\", \"k\", \"k\", \"t\", \"t\", \"t\", \"t\", \"p\", \"t\", \"t\"…\n$ prePHON     &lt;chr&gt; \"#\", \"a\", \"r\", \"t\", \"r\", \"\\\"\", \"i\", \"a\", NA, \"n\", \"r\", \"n\"…\n$ folPHON     &lt;chr&gt; \"e\", \"e\", \"r\", \"a\", \"o\", \"e\", \"o\", \"e\", \"e\", \"r\", \"e\", \"r\"…\n$ prePHON2    &lt;chr&gt; \"#\", \"@\", \"r\", \"t\", \"r\", \"#\", \"i\", \"@\", \"#\", \"n\", \"r\", \"n\"…\n$ folPHON2    &lt;chr&gt; \"E\", \"E\", \"r\", \"{\", \"V\", \"E\", \"@\", \"E\", \"E\", \"r\", \"E\", \"r\"…\n$ wdPOS       &lt;chr&gt; \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"i…\n$ LANG        &lt;chr&gt; \"Eng\", \"Eng\", \"Eng\", \"Eng\", \"Eng\", \"Eng\", \"Eng\", \"Eng\", \"E…\n$ GENRE       &lt;chr&gt; \"read\", \"read\", \"read\", \"read\", \"read\", \"read\", \"read\", \"r…\n$ VOT         &lt;dbl&gt; 0.05232002, 0.04551849, 0.07766637, 0.03610355, 0.05661694…\n$ COG         &lt;dbl&gt; 1566.0335, 2733.0783, 3184.4271, 883.2452, 1432.5341, 2435…\n$ wdDUR       &lt;dbl&gt; 0.53725609, 0.97269003, 0.57039458, 0.14330079, 0.35394835…\n$ wdPHON      &lt;chr&gt; \"tEl@BIZIn\", \"tEl@k@mjunIkeISIn\", \"tr{nsmItIN\", \"k{n\", \"kV…\n$ numWdPhon   &lt;dbl&gt; 9, 17, 10, 3, 6, 9, 2, 9, 9, 8, 9, 10, 9, 7, 9, 10, 9, 6, …\n$ secPerSeg   &lt;dbl&gt; 0.06061701, 0.05794822, 0.05474758, 0.05359862, 0.05946628…\n$ NOTES       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ RATIO       &lt;dbl&gt; 0.8631243, 0.7855028, 1.4186266, 0.6735911, 0.9520847, 0.6…\n$ prePhonBin  &lt;chr&gt; \"_pause\", \"_nonHiV\", \"_son\", \"_obs\", \"_son\", \"_pause\", \"_h…\n$ folPhonBin  &lt;chr&gt; \"_nonHiV\", \"_nonHiV\", \"_son\", \"_nonHiV\", \"_nonHiV\", \"_nonH…\n$ wdLOWER     &lt;chr&gt; \"television\", \"telecommunication\", \"transmitting\", \"can\", …\n$ prevMention &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"…\n$ freqPerMil  &lt;dbl&gt; 9.885247e+01, 5.613228e-01, 2.294102e+00, 2.056359e+03, 2.…\n$ cogStatus   &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"Y\", \"Y\", \"N\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\"…\n$ secChar     &lt;chr&gt; \"e\", \"e\", \"r\", \"a\", \"o\", \"e\", \"o\", \"e\", \"e\", \"r\", \"e\", \"r\"…\n$ secCharBin  &lt;chr&gt; \"vowel\", \"vowel\", \"cons\", \"vowel\", \"vowel\", \"vowel\", \"vowe…\n$ lexClass    &lt;chr&gt; \"N\", \"N\", \"V\", \"V\", \"A\", \"N\", \"P\", \"N\", \"N\", \"N\", \"N\", \"N\"…\n$ wdClass     &lt;chr&gt; \"C\", \"C\", \"C\", \"C\", \"C\", \"C\", \"F\", \"C\", \"C\", \"C\", \"C\", \"C\"…\n$ primary_key &lt;chr&gt; \"KS_E_01_1\", \"KS_E_01_2\", \"KS_E_01_3\", \"KS_E_01_4\", \"KS_E_…\n$ COG2        &lt;dbl&gt; 2570.525, 4937.739, 3617.235, 1780.305, 2531.263, 3324.923…\n\nhead(ptk)\n\n# A tibble: 6 × 39\n  ALL_CASE FILE     CASE PRE     HIT   FOL   TIME   SECS START    CP WORD  PHON \n     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1        1 KS_E_01     1  &lt;NA&gt;   T     \"ele… 00:0…    12  13.6     1 Tele… t    \n2        2 KS_E_01     2 \"Telev… t     \"ele… 00:0…    12  14.4    17 tele… t    \n3        3 KS_E_01     3 \"for\"   t     \"ran… 00:0…    15  16.2     5 tran… t    \n4        4 KS_E_01     4 \"for t… c     \"an … 00:0…    15  18.4    51 can   k    \n5        5 KS_E_01     5 \"for t… c     \"olo… 00:0…    15  19.4    72 colo… k    \n6        6 KS_E_01     6 \"\\\"\"    T     \"ele… 00:0…    22  22.6     2 Tele… t    \n# ℹ 27 more variables: prePHON &lt;chr&gt;, folPHON &lt;chr&gt;, prePHON2 &lt;chr&gt;,\n#   folPHON2 &lt;chr&gt;, wdPOS &lt;chr&gt;, LANG &lt;chr&gt;, GENRE &lt;chr&gt;, VOT &lt;dbl&gt;, COG &lt;dbl&gt;,\n#   wdDUR &lt;dbl&gt;, wdPHON &lt;chr&gt;, numWdPhon &lt;dbl&gt;, secPerSeg &lt;dbl&gt;, NOTES &lt;chr&gt;,\n#   RATIO &lt;dbl&gt;, prePhonBin &lt;chr&gt;, folPhonBin &lt;chr&gt;, wdLOWER &lt;chr&gt;,\n#   prevMention &lt;chr&gt;, freqPerMil &lt;dbl&gt;, cogStatus &lt;chr&gt;, secChar &lt;chr&gt;,\n#   secCharBin &lt;chr&gt;, lexClass &lt;chr&gt;, wdClass &lt;chr&gt;, primary_key &lt;chr&gt;,\n#   COG2 &lt;dbl&gt;\n\n\nThe function ggplot() (without a 2, like in the package name) creates a plotting window or palette:\n\nggplot()\n\nThe first argument to ggplot() is a data frame, which can be piped in:\n\nggplot(ptk)\n\n# same as above, but with a pipe\nptk |&gt; ggplot()\n\nThe second argument to ggplot() is a call to the aes() function (short for “aesthetics”) which maps columns in the data frame to elements on the plot. The first argument to aes() is what will be plotted on the x-axis and the second argument, if needed, is what will be plotted on the y-axis. For example, let’s put the levels of LANG on the x-axis:\n\nptk |&gt; \n  ggplot(aes(x = LANG))\n\nOr we can put the prePhonBin on the y-axis:\n\nptk |&gt; \n  ggplot(aes(y = prePhonBin))\n\nYou’ll notice that the above plots don’t actually plot anything yet; they simply set up the plotting window with the right axis labels. We now need to add elements to the plot."
  },
  {
    "objectID": "lessons/visualization.html#one-categorical-variable",
    "href": "lessons/visualization.html#one-categorical-variable",
    "title": "Visualization",
    "section": "One categorical variable",
    "text": "One categorical variable\nPerhaps the most basic plot is of one categorical variable. In order to see the distribution of the levels (aka. values or groups) of a categorical variable, let’s create a popular option: the good ol’ fashioned barplot. Note: The several layers of the plot are separated by a plus sign + rather than the pipe operator, and the layers are often specified with a geom_...() function (short for geometric object). See the section about layers in the reference manual of ggplot2.\n\nptk |&gt; \n  ggplot(aes(x = LANG))+\n  geom_bar()\n\nIt might be good to also get the exact number of tokens in each level with our ol’ friend:\n\nptk |&gt; \n  count(LANG)\n\n\nActivity\n\nTake a look at the variety of barplots that are available in ggplot.\nThen, using the Data_ptk.xlsx dataset, plot the distribution of several categorical variables, for example: LANG, GENRE, prePhonBin, folPhonBin, prevMention, cogStatus, lexClass, wdClass."
  },
  {
    "objectID": "lessons/visualization.html#two-categorical-variables",
    "href": "lessons/visualization.html#two-categorical-variables",
    "title": "Visualization",
    "section": "Two categorical variables",
    "text": "Two categorical variables\nThe following are popular plots to visualize the distribution of data points across two categorical variables.\n\nStacked barplot\nTo create a stacked (aka. filled) barplot, one categorical variable is given as x in aes() and the other is given as the fill argument in aes(), for example:\n\nptk |&gt; \n  ggplot(aes(x = LANG, fill = prePhonBin))+\n  geom_bar()\n\nSee more examples of barplot in the docs.\nFollow-up question: How can we get the exact numbers of tokens in each of the subgroups displayed in the plot? After a good-faith effort, if you need help, take a look at Dr. Brown’s code below:\n\n\nCode\nptk |&gt; \n  count(LANG, prePhonBin)\n\n\n\n\nPercent barplot\nTo create a percent barplot, that is, a stacked barplot that sums to 1.0, let’s modify the previous barplot by adding the argument position = \"fill\" to geom_bar(), but outside of aes() (which isn’t used at all).\n\nptk |&gt; \n  ggplot(aes(x = LANG, fill=prePhonBin))+\n  geom_bar(position = \"fill\")\n\nWe notice that the y-axis is labeled “count”, but those aren’t counts, they’re proportions. Let’s change the axis labels with the labs() element:\n\nptk |&gt; \n  ggplot(aes(x = LANG, fill=prePhonBin))+\n  geom_bar(position = \"fill\")+\n  labs(x = \"Language\", y = \"Proportion\")\n\n\n\nGrouped barplot\nTo create a grouped barplot, modify the previous barplot by changing the argument to position = \"dodge\". Simple enough.\n\nptk |&gt; \n  ggplot(aes(x = LANG, fill = prePhonBin))+\n  geom_bar(position = \"dodge\")\n\n\n\nMosaic plot\nAnother useful plot for two categorical variables is a mosaic plot, which displays proportions as relative sizes of the various pieces of the mosaic. Let’s download two R packages that can be used to create mosaic plots: ggmosaic is (given its name) meant to extend ggplot2, and vcd has a transparently named package name: Visualizing Categorical Data.\n\ninstall.packages(c(\"ggmosaic\", \"vcd\"), repos = \"https://cran.rstudio.com\")\n\nLet’s plot prePhonBin on the x-axis and LANG on the y-axis:\n\nlibrary(\"ggmosaic\")\nptk |&gt; \n  ggplot()+\n  geom_mosaic(aes(x = prePhonBin, fill = LANG))\n\n\nlibrary(\"vcd\")\n\ntemp &lt;- ptk |&gt; \n  count(LANG, prePhonBin) |&gt; \n  pivot_wider(names_from = prePhonBin, values_from = n)\nrow_names &lt;- temp |&gt; pull(1)\ntemp &lt;- temp |&gt; data.matrix()\ntemp &lt;- temp[,-1]\nrownames(temp) &lt;- row_names\nvcd::mosaic(temp, shade = TRUE, varnames = FALSE)\n\n\n\nActivity\n\nGive it a whirl! Using the Data_ptk.xlsx dataset, plot the distribution of sets of two categorical variables, e.g., LANG and GENRE, prePhonBin and folPhonBin, prevMention and cogStatus, lexClass and wdClass, and any combination of these variables."
  },
  {
    "objectID": "lessons/visualization.html#one-continuous-variable",
    "href": "lessons/visualization.html#one-continuous-variable",
    "title": "Visualization",
    "section": "One continuous variable",
    "text": "One continuous variable\nIt is useful to visualize the distribution of continuous variables in order to see whether the variable is distributed normally or has some skew or is bimodal.\nThe following plots are useful for this purpose. You only need to specify the x argument in aes() within ggplot() with the one variable.\n\nHistogram\nA histogram slices or bins of a continuous variable into N bins and then plots the count within each bin on the y-axis. It gives a barplot (i.e., it looks like a barplot), but instead of levels within a categorical variable, the bars are equidistant intervals across a continuous variable. Here’s an example with the continuous variable VOT in the ptk dataset:\n\nptk |&gt; \n  ggplot(aes(x = VOT))+\n  geom_histogram()\n\nYou can specify a specific number of bins with the bins argument, for example:\n\nptk |&gt; \n  ggplot(aes(x = VOT))+\n  geom_histogram(bins = 15)\n\n\n\nDensity plot\nDensity plots are like histograms, but they use a curved line rather than bins:\n\nptk |&gt; \n  ggplot(aes(VOT))+\n  geom_density()\n\nThe cool thing about density plots is that we can specify that the area below the curve sum to 1. That fact will be important when we look at p-values.\nLet’s plot VOT as a density plot:\n\nptk |&gt; \n  ggplot(aes(scale(VOT)))+\n  geom_density(fill = \"blue\")\n\n\n\nActivity\nGive it a go! Plot the distribution of several different continuous variables in the ptk dataset, and be ready to share with a neighbor and/or the class.\nMORE TO COME REAL SOON!"
  },
  {
    "objectID": "lessons/regexes.html",
    "href": "lessons/regexes.html",
    "title": "Regular expressions",
    "section": "",
    "text": "Students will become proficient with writing regular expressions, including with capture groups and lookaround."
  },
  {
    "objectID": "lessons/regexes.html#objective",
    "href": "lessons/regexes.html#objective",
    "title": "Regular expressions",
    "section": "",
    "text": "Students will become proficient with writing regular expressions, including with capture groups and lookaround."
  },
  {
    "objectID": "lessons/regexes.html#regular-expressions-aka.-regexes",
    "href": "lessons/regexes.html#regular-expressions-aka.-regexes",
    "title": "Regular expressions",
    "section": "Regular expressions (aka. regexes)",
    "text": "Regular expressions (aka. regexes)\n\nRegular expressions are used to match strings. \n\nNote: In R, you must use double backslashes, e.g., \\\\w+\n\nOnline regex checker are useful, such as here (for Python), here, and here.\nLetters represent themselves: \"ed\" returns ed anywhere in the string, for example, Ed studied in the education building."
  },
  {
    "objectID": "lessons/regexes.html#character-classes",
    "href": "lessons/regexes.html#character-classes",
    "title": "Regular expressions",
    "section": "Character classes",
    "text": "Character classes\n\n\\\\w = alphanumeric character; \\\\W = non-alphanumeric character\n\\\\s = whitespace (i.e., spaces, tab breaks, newlines); \\\\S = non-whitespace\n\\\\d = Arabic numeral (i.e., 0-9); \\\\D = non-Arabic numeral\n[] = character class finds one of the characters between the square brackets: \n\n[aeiou] finds one of the five orthographic vowels\n[Aa] find either uppercase or lowercase a \n[a-z] finds one lowercase English character\n[a-zA-Z] returns one lowercase English character or one uppercase English character \nExample: \"latin[aox]\" returns latina, latino, latinx."
  },
  {
    "objectID": "lessons/regexes.html#the-pipe-which-is-just-above-the-return-key-on-my-keyboard-is-an-or-operator",
    "href": "lessons/regexes.html#the-pipe-which-is-just-above-the-return-key-on-my-keyboard-is-an-or-operator",
    "title": "Regular expressions",
    "section": "| (the “pipe” which is just above the return key on my keyboard) is an “or” operator:  ",
    "text": "| (the “pipe” which is just above the return key on my keyboard) is an “or” operator:  \n\nExample: \"\\\\bth(is|at|ese|ose) \\\\w+\" returns an English demonstrative determiner followed by a space, followed by a contiguous span or one or more of alphanumeric character, for example, this bag, that cat, these plants, those buildings.\nQuantifiers\n\n{min, max} = returns between min and max number of the previous character: \"\\\\w{2,5}\" returns between two and five alphanumeric characters. Note that \"\\\\w{,5}\" returns up to five alphanumeric characters, and \"\\\\w{2,}\" finds two or more alphanumeric characters.\n{integer} = returns the exact number of the previous character: \"\\\\d{4}\" returns exactly four Arabic numerals (for example, to find four-digit years in a text or corpus)\nShortcut quantifiers:\n\n? means the same as {0, 1}, meaning it returns zero or one of the previous pattern, that is, the previous character is optional\n* is the same as {0,} and returns zero or more of the previous pattern: yes\\\\!* returns yes, followed by any number of exclamation points, including none at all: yes, yes!, yes!!!, etc.. \n+ means {1,} and returns one or more of the previous pattern, for example, \"go+l\" returns gol, goool, gooooooool\n\n\n\n\nActivity\n\nWhat do the following regexes match? See example in Section 2.1 here.\n\n\"\\\\b[Tt]he\\\\b\\\\s+\\\\b[Ii]nternet\\\\b\"\n\"\\\\w+ed\\\\b\"\n\"\\\\bcent(er|re)\\\\b\"\n\"\\\\bwalk(s|ed|ing)?\\\\b\"\n\"\\\\b[^aeiou\\\\W]{2,}\\\\w+\"\n\"\\\\b[^aeiou\\\\W][aeiou]\\\\w+\""
  },
  {
    "objectID": "lessons/regexes.html#capture-groups",
    "href": "lessons/regexes.html#capture-groups",
    "title": "Regular expressions",
    "section": "Capture groups",
    "text": "Capture groups\n\nWarning: This gets wild. \nYou can have a regular expression remember what it captured in order to search for that same sequence of characters.\nYou can encapsulate a pattern in parentheses to capture, and then refer to that same sequence of characters with \\\\1 for the first capture group, or \\\\2 for the second capture group (if you have more than one capture group in the same regex), etc.\nExample: \"\\\\w+(\\\\w) \\\\1\\\\w+\" returns a bigram whose first word ends with the same letter that the second word begins with, e.g., walkeddown\n\n\nActivity\n\nWhat do the following regexes match?\n\n\"([aeiou])\\\\1\"\n\"\\\\w*([aeiou])\\\\1\\\\w*\"\n\"the (\\\\w+)er they were, the \\\\1er they will be\"\n\"[Tt]he (\\\\w+)er they (\\\\w+),? the \\\\1er we \\\\2\"\n\"\\\\w+(\\\\w{2,})\\\\W+\\\\w+\\\\1\\\\b\""
  },
  {
    "objectID": "lessons/regexes.html#lookaround",
    "href": "lessons/regexes.html#lookaround",
    "title": "Regular expressions",
    "section": "Lookaround",
    "text": "Lookaround\n\nLookaround allows you to use surrounding characters to find other characters, but to not consume those surrounding characters.\n\nSee lookahead examples in Section 2.1.7 here.\n\n\n\nActivity\n\nDownload at least several TXT files of your choice (perhaps from Project Gutenberg or Saints from the LMS).\nLoop over the files and search for a regex of your choice with a capture group, and print to screen the results. Use several different regex functions from the stringr package, for example, str_match_all(), str_extract_all(), str_locate_all().\nCreate a tabular dataset of your choice with a regex of your choice. As a first step, you might simple create a data frame with two columns: filename, and regex match.\nRamp it up by creating more columns, perhaps the number of characters in the match, or the number of (orthographic) vowels in the match, etc."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\nMy name is Earl Kjar Brown and I’m a Dane."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ling_data_analysis",
    "section": "",
    "text": "Website with Earl Kjar Brown’s lesson plans for Linguistic Data Analysis with R. Click on the lesson plans in the navigation menu."
  },
  {
    "objectID": "lessons/webscrape_freq.html",
    "href": "lessons/webscrape_freq.html",
    "title": "Webscrape frequencies",
    "section": "",
    "text": "Students will use R to webscrape an already-created frequency list on the internet."
  },
  {
    "objectID": "lessons/webscrape_freq.html#objective",
    "href": "lessons/webscrape_freq.html#objective",
    "title": "Webscrape frequencies",
    "section": "",
    "text": "Students will use R to webscrape an already-created frequency list on the internet."
  },
  {
    "objectID": "lessons/webscrape_freq.html#overview-of-the-internet",
    "href": "lessons/webscrape_freq.html#overview-of-the-internet",
    "title": "Webscrape frequencies",
    "section": "Overview of the internet",
    "text": "Overview of the internet\nHere’s a super simplified overview of how the internet works: Clients (e.g., computers, smart phones) that are connected to the internet make HTTP requests to web servers (e.g., byu.edu, npr.org, instagram.com, etc.), and those web servers send back HTTP responses. Here’s a diagram to illustrate this basic idea."
  },
  {
    "objectID": "lessons/webscrape_freq.html#r-as-web-browser",
    "href": "lessons/webscrape_freq.html#r-as-web-browser",
    "title": "Webscrape frequencies",
    "section": "R as web browser",
    "text": "R as web browser\nR can act like a web browser by making HTTP requests and receiving HTTP responses from web servers. The rvest package here makes it easy to have R interact with the internet. That package also contains useful functions to parse the HTML in the HTTP response in order to extract information using either CSS selectors or XPath expressions.\nIt’s a good idea, and actually necessary with some websites, to make R look like a normal web browser when making the request to the web server. We do this by setting the user-agent to a common one that many web browsers use:\n\n# set the user-agent to make R look like a normal web browser to the web server\nhttr::set_config(httr::user_agent(\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"))"
  },
  {
    "objectID": "lessons/webscrape_freq.html#frequency-lists-on-the-internet",
    "href": "lessons/webscrape_freq.html#frequency-lists-on-the-internet",
    "title": "Webscrape frequencies",
    "section": "Frequency lists on the internet",
    "text": "Frequency lists on the internet\nThere are many frequency lists on the internet, and the free online dictionary wikitionary makes available a plethora of them here.\nSometimes, the frequency lists are in HTML tables, for example:\n\nThe 1,000 most frequent words in TV and Movie scripts and transcripts in English here.\nThe 1,900 most frequent Hindi words here.\nThe 10,000 most frequency Serbian words based on TV and Movie subtitles here.\n\nOther times, the frequency lists are presented as HTML lists (whether ordered or unordered), for example:\n\nThe 5,000 most frequent words in Danish here.\n1,000 Japanese basic words here.\nThe 2,000 most frequent words in fiction in English here."
  },
  {
    "objectID": "lessons/webscrape_freq.html#html-tables",
    "href": "lessons/webscrape_freq.html#html-tables",
    "title": "Webscrape frequencies",
    "section": "HTML tables",
    "text": "HTML tables\nLet’s webscrape a frequency list that is sitting in an HTML table. Let’s use the list of the 1,000 most frequent words in TV and Movie scripts in English here.\n\n# if need, use install.packages() first to download the following packages to your harddrive\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\nurl &lt;- \"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/TV/2006/1-1000\"\n\n# request the page\npage &lt;- read_html(url)\n\n# get the HTML table holding the frequency list\nfreqs_table &lt;- html_element(page, \"table\") \n\n# convert the HTML table into a data frame (i.e., tibble)\nfreqs_df &lt;- html_table(freqs_table, header = TRUE)\n\n# print the frequency table to the console\nprint(freqs_df)\n\nYou may have noticed that the above code could be put into a single pipeline by using the pipe operator repeatedly:\n\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/TV/2006/1-1000\" %&gt;% \n  read_html() %&gt;% \n  html_element(\"table\") %&gt;% \n  html_table(header = TRUE) %&gt;% \n  print()\n\n\nActivity\nYour turn! Find a frequency list of your choice online that is in an HTML table and webscrape it into a data frame in R. If you’ve having trouble finding one that is in an HTML table, try the 1,900 most frequent Hindi words here or the 10,000 most frequent Serbian words here.\nAfter a good-faith effort, if you need help, see Dr. Brown’s code below:\nHindi\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Hindi_1900\" %&gt;%  # put URL in a string\n  read_html() %&gt;%  # request page\n  html_element(\"table\") %&gt;%  # find first \"table\" HTML element\n  html_table() %&gt;%  # convert HTML table to data frame (i.e., tibble)\n  print()  # print data frame to console\n\n\nSerbian\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n# extra exercise: write in appropriate comments below\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Serbian_wordlist\" %&gt;% \n  read_html() %&gt;% \n  html_element(\"table\") %&gt;% \n  html_table() %&gt;% \n  print()"
  },
  {
    "objectID": "lessons/webscrape_freq.html#html-lists",
    "href": "lessons/webscrape_freq.html#html-lists",
    "title": "Webscrape frequencies",
    "section": "HTML lists",
    "text": "HTML lists\nWebscraping HTML lists, whether ordered (i.e., with numbers) or unordered (i.e., with bullets) is more work because we can’t use the slick html_table() function in rvest to convert an HTML table into a data frame in R. Rather, we have to identify which part of the data is the word and which part is the frequency. Further, we have to parse each frequency list separately, as we can’t assume that all frequency lists are formatted the same way.\nLet’s inspect the HTML of the frequency list of the 5,000 most frequency words in Danish in TV and Movie subtitles here. (Seriously, go inspect the HTML of that list before moving on.)\nWe see that the frequency list is an ordered list (HTML tag &lt;ol&gt;) and that each word and its corresponding frequency is in a list item (HTML tag &lt;li&gt;) and that the word and its frequency are separated by a space. We can use this information to scrape out the frequency list, and then use the space to identify the word and the frequency in order to put them into separate columns in a data frame in R.\nPro-tip: SelectorGadget (here) is an absolutely super helpful Google Chrome add-on extension that helps to quick identify the CSS Selector of elements of a webpage.\nLet’s get rolling with the code:\n\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n# put the URL in a string\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Danish_wordlist\" %&gt;% \n  \n  # request the HTML page\n  read_html() %&gt;% \n  \n  # extract the HTML elements &lt;li&gt; that are children of HTML elements &lt;ol&gt;\n  html_elements(css = \"ol li\") %&gt;% \n  \n  # keep only the text, that is, remove the HTML tags\n  html_text() %&gt;% \n  \n  # create a data frame with one column named \"both\"\n  # note: the dot below pipes the result of the previous step into where the dot is (rather than at the beginning of the function, which is the default behavior of the pipe operator)\n  tibble(both = .) %&gt;% \n  \n  # separate the \"both\" column into two columns (i.e., \"wd\" and \"freq\") on the space between the word and the frequency\n  separate_wider_delim(cols = \"both\", delim = \" \", names = c(\"wd\", \"freq\")) %&gt;% \n  \n  # print the data frame to the console\n  print()\n\n\nActivity\nIt’s that time of the class period: your turn!\nFind a frequency (or rank) list of your choice on the internet (probably at wikitionary.org) that is not in an HTML table and parse it into a data frame in R. If you need help finding one, try the 2,000 most frequent words in fiction in English here or the 1,000 Japanese basic words here.\nAfter a good-faith effort, if you need some help, see Dr. Brown’s code below:\nEnglish\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Contemporary_fiction\" %&gt;% \n  read_html() %&gt;% \n  html_elements(css = \"ol li\") %&gt;% \n  html_text() %&gt;% \n  tibble(wd = .) %&gt;% \n  print()\n\n\nJapanese\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\"https://en.wiktionary.org/wiki/Appendix:1000_Japanese_basic_words\" %&gt;% \n  read_html() %&gt;% \n  html_elements(css = \".mw-parser-output &gt; ul li\") %&gt;% \n  html_text() %&gt;% \n  tibble(entry = .) %&gt;% \n  print()"
  },
  {
    "objectID": "lessons/webscrape_freq.html#question",
    "href": "lessons/webscrape_freq.html#question",
    "title": "Webscrape frequencies",
    "section": "Question",
    "text": "Question\nIn most of the code in this lesson, the frequency or rank lists were simply printed to the console. How could we modify the code so that instead of printing to the console, we write the lists out to CSV files, one list per CSV file?"
  },
  {
    "objectID": "lessons/wrangling.html",
    "href": "lessons/wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Students will manipulate or wrangle a data frame."
  },
  {
    "objectID": "lessons/wrangling.html#objective",
    "href": "lessons/wrangling.html#objective",
    "title": "Data Wrangling",
    "section": "",
    "text": "Students will manipulate or wrangle a data frame."
  },
  {
    "objectID": "lessons/wrangling.html#the-dplyr-package",
    "href": "lessons/wrangling.html#the-dplyr-package",
    "title": "Data Wrangling",
    "section": "The dplyr package",
    "text": "The dplyr package\nThe dplyr R package is a core package of tidyverse, and has a handful of super useful functions for manipulating or wrangling a data frame. Most of the functions are verbs (e.g., filter(), select(), etc.) and so the documentation for the package often refers to the main functions of dplyr as verbs."
  },
  {
    "objectID": "lessons/wrangling.html#rows",
    "href": "lessons/wrangling.html#rows",
    "title": "Data Wrangling",
    "section": "Rows",
    "text": "Rows\nThere are three (I guess four) main functions (aka. verbs) for working with rows in dplyr: filter(), arrange() (and it’s helper function desc()), and distinct().\nPop (formative) quiz!\nInstructions: Match each function with its corresponding purpose.\n\n\n\n\n\n\n\nfilter()\nA. orders the rows based on values in one or more columns\n\n\narrange()\nB. inverts the order so that the rows are in big-to-small order\n\n\ndesc()\nC. keeps only unique rows\n\n\ndistinct()\nD. keeps rows that evaluate to TRUE in a conditional statement based on one or more columns\n\n\n\n\nActivity\nLet’s follow the examples given in Chapter 3 “Data transformation” of the book R for Data Science (2e). First, let’s download the nycflights13 R package to our harddrive:\n\ninstall.packages(\"nycflights13\", repos = \"https://cran.rstudio.com\")\n\nNow, let’s load the nycflights13 data frame into our R session or script, and our ol’ friend tidyverse:\n\nlibrary(\"nycflights13\")\nlibrary(\"tidyverse\")\n\nFinally, and most importantly, let’s try to solve the exercises in section 3.2.5 here. After some good-faith work, if you need some help, take a look at Dr. Brown’s code below:\n“Had an arrival delay of two or more hours”\n\n\nCode\nflights |&gt; \n  filter(arr_delay &gt;= 120)\n\n\n“Flew to Houston (IAH or HOU)”\n\n\nCode\nflights |&gt;  \n  filter(dest %in% c(\"IAH\", \"HOU\"))\n\n\n“Were operated by United, American, or Delta”\n\n\nCode\nflights |&gt;  \n  filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n\n“Departed in summer (July, August, and September)”\n\n\nCode\nflights |&gt;  \n  filter(month %in% c(7, 8, 9))  # note: the month column has a data type of integer, and therefore you don't need quotes around the numbers\n\n\nHere’s another way to filter for only summer flights:\n\n\nCode\nflights |&gt;  \n  filter(month &gt;= 7 & month &lt;= 9)\n\n\n“Arrived more than two hours late, but didn’t leave late”\n\n\nCode\nflights |&gt;  \n  filter(arr_delay &gt; 120 & dep_delay &lt;= 0)\n\n\n“Were delayed by at least an hour, but made up over 30 minutes in flight”\n\n\nCode\nflights |&gt;  \n  filter(dep_delay &gt;= 60 & arr_delay &lt; 30)\n\n\n“Sort flights to find the flights with longest departure delays.”\n\n\nCode\nflights |&gt;  \n  arrange(desc(dep_delay))\n\n\n“Find the flights that left earliest in the morning.”\n\n\nCode\nflights |&gt;  \n  arrange(dep_time)\n\n\n“Sort flights to find the fastest flights. (Hint: Try including a math calculation inside of your function.)”\n\n\nCode\nflights |&gt;  \n  arrange(desc(distance / air_time))\n\n\nHere’s another way, and a preview of the mutate function that we’ll see below:\n\n\nCode\nflights |&gt;  \n  mutate(speed = distance / air_time) %&gt;% \n  arrange(desc(speed))\n\n\n“Was there a flight on every day of 2013?”\n\n\nCode\nflights |&gt;  \n  distinct(year, month, day)\n\n\n“Which flights traveled the farthest distance?”\n\n\nCode\nflights |&gt;  \n  arrange(desc(distance))\n\n\n“Which traveled the least distance?”\n\n\nCode\nflights |&gt;  \n  arrange(distance)\n\n\n“Does it matter what order you used filter() and arrange() if you’re using both? Why/why not? Think about the results and how much work the functions would have to do.”\nEarl speculates that it’s probably best to filter first so that arrange has fewer rows to sort."
  },
  {
    "objectID": "lessons/wrangling.html#columns",
    "href": "lessons/wrangling.html#columns",
    "title": "Data Wrangling",
    "section": "Columns",
    "text": "Columns\nThere are four main functions (i.e., verbs) for working with columns: mutate(), select() (and its handful of helper functions here), rename(), and relocate().\nPop quiz time!\nInstructions: Match each function (i.e., verb) with its corresponding purpose.\n\n\n\n\n\n\n\nmutate()\nA. change the name of the specified column\n\n\nselect()\nB. creates new columns based on one or more already existing columns\n\n\nrename()\nC. moves the position of the specified columns\n\n\nrelocate()\nD. keeps only the specified columns\n\n\n\nLet’s try out the exercises provided in 3.3.5 of Chapter 3 “Data transformation” here. After a good-faith effort, if you need take a look at Dr. Brown’s code below each exercise.\n“Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?”\n“Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights”\n\n\nCode\nflights |&gt; \n  select(\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\")\n\n\n\n\nCode\nflights |&gt; \n  select(matches(\"^(dep_|arr_)\"))\n\n\n“Does the result of running the following code surprise you? How do the select helpers deal with upper and lower case by default? How can you change that default?”\n\n\nCode\nflights |&gt; \n  select(contains(\"TIME\"))\n\n\n“Rename air_time to air_time_min to indicate units of measurement and move it to the beginning of the data frame.”\n\n\nCode\nflights |&gt; \n  rename(air_time_min = air_time) |&gt; \n  relocate(air_time_min, .before = 1)\n\n\n“Why doesn’t the following work, and what does the error mean?”\n\nflights |&gt; \n  select(tailnum) |&gt; \n  arrange(arr_delay)\n\n\nActivity\nNow for some linguistic data. Download the “data_FRC_spch_rate.xlsx” Excel file from the Datasets module in the LMS (here). This dataset was used to write the article in the journal Corpus Linguistics and Linguistic Theory here.\nOpen it up in Excel (or Google Sheets) and inspect the columns in the data sheet and read what type of data each column holds in the legend sheet.\nNow, read in the data sheet into R as a data frame (probably a tibble). Take some time to practice using the functions (i.e., verbs) that work with rows and the functions that work with columns.\n\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\nsetwd(\"/pathway/to/dir\")\nsibilants &lt;- read_excel(\"data_FRC_spch_rate.xlsx\", sheet = \"data\")\n\nThen, try the following exercises. After a good-faith effort to complete these exercises on your own, if you need help take a look at Dr. Brown’s code.\n\nKeep only the rows with tokens of /s/ that were maintained as a sibilant.\n\n\nCode\nsibilants |&gt; \n  filter(s == \"maintained\")\n\n\nKeep only the rows of tokens of /s/ that were deleted (no sibilance) in word final position.\n\n\nCode\nsibilants |&gt; \n  filter(s == \"deleted\", wd_pos == \"wd_final\")\n\n\nCreate an alphabetized list of unique words with /s/ at the beginning of the word and following by high vowels.\n\n\nCode\nsibilants |&gt; \n  filter(wd_pos == \"wd_initial\", sound_post == \"HiV\") |&gt; \n  distinct(word) |&gt; \n  arrange(word)\n\n\nSort the data frame in descending order by lexical frequency, and then in ascending order by word.\n\n\nCode\nsibilants |&gt; \n  arrange(desc(lex_freq), word)\n\n\nKeep only rows with tokens with a speech rate of between 8 and 12 segments per second. Hint: The data type of the column with speech rate (i.e., spch_rate) may not have been read in as a numeric (aka. float) value. So, first you may need to coerce that column to a numeric data type with the base R function as.numeric().\n\n\nCode\nsibilants |&gt; \n  mutate(spch_rate = as.numeric(spch_rate)) |&gt; \n  filter(spch_rate &gt;= 8 & spch_rate &lt;= 12)\n\n\nCreate an alphabetized list of unique words with /s/ in which /s/ occurred in a tonic syllable.\n\n\nCode\nsibilants |&gt; \n  filter(stress == \"tonic\") |&gt; \n  distinct(word) |&gt; \n  arrange(word)"
  },
  {
    "objectID": "lessons/wrangling.html#group-by",
    "href": "lessons/wrangling.html#group-by",
    "title": "Data Wrangling",
    "section": "Group by",
    "text": "Group by\nAnother super useful function in dplyr is group_up(). It doesn’t change the data frame itself, but it performs subsequent functions based on the levels of a column or columns. It is often used in conjugation with the summarize() function to get group-level information.\nAs an example, let’s first calculate mean departure delay of all flights in the nycflights13 dataset, then next, let’s calculate the mean departure delay by airline (i.e., carrier column). However, as a preprocessing step, we need to remove rows what don’t have a number in dep_delay, but rather an NA, because the mean() function errs out with missing values. There are two ways to do this: (1) use filter() to keep only rows that don’t have an NA in the dep_delay column, or (2) tell the mean() function to remove the NAs with the na.rm argument.\nHere’s how to use filter() for that purpose:\n\nlibrary(\"nycflights13\")\nlibrary(\"tidyverse\")\n\nflights |&gt; \n  filter(!is.na(dep_delay)) |&gt;  # mind the exclamation point\n  summarize(average_departure_delay = mean(dep_delay))\n\nAnd here we use the na.rm argument within the mean() function:\n\nflights |&gt; \n  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE))\n\nSo, the mean departure delay across all airlines is 12.6 minutes. The natural follow-up question is whether some airlines have longer delays than others. Enter our new friend group_by(). Now, let’s perform this mean operation based on the levels (aka. values) of the carrier column in order to get the mean departure delay time by airline:\n\nflights |&gt; \n  group_by(carrier) |&gt; # we group before getting the mean\n  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE))\n\nMicro-activity: Modify the previous code block to sort the resulting data frame so that the airline with the shortest average delay is at the top of the resulting data frame and the airline with the longest average delay is at the bottom. After a good-faith effort, if you need some help, take a look at Dr. Brown’s code.\n\n\nCode\nflights |&gt; \n  group_by(carrier) |&gt; \n  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE)) |&gt; \n  arrange(average_departure_delay)\n\n\nIf you need to group by several columns, just add the additional column names to the group_by() call, for example:\n\nflights |&gt; \n  group_by(carrier, origin) |&gt; \n  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE))\n\nYou don’t have to use group_by() only with summarize(), but it’s a common use case. And, importantly, after summarize() has finished its work, it ungroups the data frame. If you were to use group_by() with other functions, they don’t automatically ungroup the data frame, and you would have explicitly do so with the ungroup() function. Here’s an example:\n\nflights |&gt; \n  group_by(carrier) |&gt; \n  mutate(aver_dep_delay_airline = mean(dep_delay, na.rm = TRUE)) |&gt; \n  select(carrier, dep_delay, aver_dep_delay_airline)\n\nNotice that the output says Groups: carrier [16]. This means that the data frame is still grouped by the 16 unique levels (aka. values) in the carrier column. In order to ungroup it, we can call ungroup() after we don’t need the groups anymore:\n\nflights |&gt; \n  group_by(carrier) |&gt; \n  mutate(aver_dep_delay_airline = mean(dep_delay, na.rm = TRUE)) |&gt; \n  select(carrier, dep_delay, aver_dep_delay_airline) |&gt; \n  ungroup()\n\n\nActivity\nUse the data sheet within the data_FRC_spch_rate.xlsx Excel workbook to get familiar with the group_by() and summarize().\n\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\nsetwd(\"/pathway/to/dir/\")\nsibilants &lt;- read_excel(\"data_FRC_spch_rate.xlsx\", sheet = \"data\")\n\nPerhaps you might try getting the mean and/or median speech rate (i.e., column name spch_rate) based on whether the /s/ was maintained as a sibilant or deleted (using the column s). Note: You’ll need to remove rows in the data frame that have NaN in spch_rate and then you need to convert spch_rate to a numeric data type. You can do both of these preprocessing steps right within the pipeline before getting the central tendency measures, if you want. If you get stuck, take a look at Dr. Brown’s code below.\n\n\nCode\nsibilants |&gt; \n  filter(spch_rate != \"NaN\") |&gt;  # more rows with \"NaN\"\n  mutate(spch_rate = as.numeric(spch_rate)) |&gt;  # convert data type from character to numeric\n  group_by(s) |&gt; \n  summarize(\n    mean_spch_rate = mean(spch_rate),\n    median_spch_rate = median(spch_rate))\n\n\nActivity: Try some other analyses with group_by() and summarize() and be prepared to share with a neighbor and/or the class."
  },
  {
    "objectID": "lessons/wrangling.html#joining-data-frames",
    "href": "lessons/wrangling.html#joining-data-frames",
    "title": "Data Wrangling",
    "section": "Joining data frames",
    "text": "Joining data frames\nJoining or merging two tables into one is often a necessary step in data analysis, including in linguistic data analysis. For example, let’s say we have one data frame (perhaps coming from an Excel file) that has tokens of sounds in words, and we have another data frame (perhaps from a CSV file) with frequencies of words from a large corpus. It would be slick to get the frequencies of word from the frequency data frame and put the appropriate frequency next to the words in the sound data frame. Enter the family of join functions in dplyr here.\nLet’s start with a super simple example in order to understand the concept, then we’ll apply this to bigger datasets. Let’s create a data frame (i.e., tibble) with two columns: a person, and the fruit that they said during a conversation:\n\nlibrary(tidyverse)\nmain_df &lt;- tibble(\n  speaker = c(\"Bob\", \"Bob\", \"Billy\", \"Billy\", \"Britta\", \"Britta\", \"Bonnie\"), \n  word = c(\"apple\", \"banana\", \"orange\", \"mango\", \"apple\", \"manzana\", \"kiwi\")\n)\nprint(main_df)\n\nNow, let’s create a data frame with some of the speakers’ ages:\n\nages &lt;- tibble(\n  person = c(\"Bonnie\", \"Billy\", \"Bob\"), \n  age = c(47, 6, 95)\n)\nprint(ages)\n\nYou may have noticed that while Britta said a couple words in our main_df data frame, she isn’t listed in the ages data frame. We’ll see what happens below when a value in a column in the main data frame doesn’t have a corresponding value in the to-be-joined data frame.\nIn order to create a new column with the age of each of the speakers in main_df, we can use the left_join() function to join main_df and ages. This function returns every row in the left-hand or first data frame in the function call (which, if we’re using a pipe operator, is the data frame to the left of the pipe), and only the rows in the right-hand or second data frame that have a corresponding value based on the columns specified with the by argument. By default, left_join joins by common column names, but we can explicitly specify which column in the left-hand data frame corresponds to which column in the right-hand data frame with the by argument. Let’s take a look:\n\nmain_df |&gt; \n  left_join(ages, by = join_by(speaker == person))\n\nQuestion: Why do we have speaker to the left of the double equal sign and person to the right?\nYou probably noticed that Britta’s age is NA because she isn’t listed in the ages data frame. If that’s a deal-breaker, then we can remove rows with NAs with filter() as a final step in the pipeline (mind the exclamation point before is.na()):\n\nmain_df |&gt; \n  left_join(ages, by = join_by(speaker == person)) |&gt; \n  filter(!is.na(age))\n\n\nToy activity\nLet’s create a toy data frame with frequencies of the words said by the speakers in main_df:\n\nfreqs &lt;- tibble(\n  wd = c(\"apple\", \"banana\", \"kiwi\", \"mango\"), \n  freq = c(123, 234, 345, 456)\n)\nprint(freqs)\n\nNow, join the freqs data frame to the main_df so that the frequency of each word is in a new column to the right of the column holding the age of the speakers, so that you end up with a data frame with four columns: speaker, word, age, freq. Give it a try, but if you get stuck, take a look at Dr. Brown’s code below. Hint: Make sure you are super aware of the column names in the various data frames that you need to join, so that you can give the correct names to the by argument.\n\n\nCode\nmain_df |&gt; \n  left_join(ages, by = join_by(speaker == person)) |&gt; \n  left_join(freqs, by = join_by(word == wd))\n\n\n\n\nActivity (with a review)\nLet’s ramp it up to a real activity. Try the following:\n\nReview: Create a data frame with at least one column named word (i.e., that has one word per row) using text files of your choice (e.g., Saints or texts from Project Gutenberg). You might like to use a regular expression to find different words (e.g., \"\\\\w+ed\\\\b\" or \"\\\\bre\\\\w+\"), as the next steps will be boring if all rows have the same word.\nReview: Create a data frame of frequencies of words from text files of your choice or webscrape a frequency list from the internet (e.g., Wikitionary).\nNote: If after a good-faith effort to complete the previous two steps, you can’t remember how to do these tasks, download and use two files in the LMS labeled Dataset_for_over_dan.csv and freqs_dan.csv. The first file has a keyword-in-context display of words in Danish that start with for or over while the second file has frequencies of words in Danish.\nHere’s the main exercise of the activity: Join the data frame with words to the data frame with frequencies, so that you have a new column (called freq) with the frequency of the word in the word (or node) column. Again, I can’t stress enough the importance of taking the time to double check that you know what the names are of the columns in the several data frame, so that you can correctly pass thme to the by argument."
  },
  {
    "objectID": "lessons/wrangling.html#separate-and-unite-columns",
    "href": "lessons/wrangling.html#separate-and-unite-columns",
    "title": "Data Wrangling",
    "section": "Separate and unite columns",
    "text": "Separate and unite columns\nOther useful functions, this time from the tidyr package within tidyverse, are the 3-member family of separate_* functions (i.e., separate_wider_delim(), separate_wider_position(), and separate_wider_regex()) and the unite() function.\nThe separate_* functions are transparently named and split up one column into two or more columns.\n\nThe separate_wider_delim() function splits on a delimiter given to the delim argument;\nThe separate_wider_position()function splits at fixed character widths given to the widths argument;\nThe separate_wider_regex() function splits on a regular expression given to the patterns argument.\n\nLet’s create a data frame with one column with two pieces of information in each row: the speaker who said something, following by a semi-colon, followed by what the speaker said:\n\nlibrary(tidyverse)\nhungry &lt;- tibble(\n  conversation = c(\"Bobby: I'm hungry!\", \"Cathy: Let's stop at In-n-Out. What do you say to that?!\", \"Bobby: Sounds good to me. What do you say João?\", \"João: Eu digo que sim!\", \"Roberto: Yo también.\")\n)\nprint(hungry)\n\nLet’s start with separate_wider_delim(). Pay attention to the arguments in the function.\n\nhungry |&gt; \n  separate_wider_delim(cols = conversation, delim = \": \", names = c(\"person\", \"utterance\"))\n\nThe function separate_wider_regex() takes a different approach. Rather than splitting on the given fixed-width delimiter given to the delim argument in the previous function, this function takes regular expressions that are used to extract matches out of the string in the row. Specifically, you give it a named character vector in which the names become column names and the elements of the vector are regular expressions to be matched. If no name is given, then that match doesn’t make it into the resulting data frame. Let’s use this function on the same hungry data frame from above. Notice that the regex \":\" doesn’t have a name, and therefore isn’t returned in the resulting data frame.\n\nhungry |&gt; \n  separate_wider_regex(cols = conversation, patterns = c(person = \"^[^:]+\", \":\", utterance = \"[^:]+$\"))\n\nFor good measure, we’d probably want to trim off that extra space at the beginning of each utterance:\n\nhungry |&gt; \n  separate_wider_regex(cols = conversation, patterns = c(person = \"^[^:]+\", \":\", utterance = \"[^:]+$\")) |&gt; \n  mutate(utterance = str_trim(utterance))\n\nDr. Brown hasn’t needed separate_wider_position() yet in his life, so let’s move on! If you think you might need it, take a look at the docs here.\nThe unite() function is the inverse of the separate_* functions, that is, it merges into one column two or more columns. It takes was input the data frame (probably piped in with the pipe operator), the col argument which specifies the name of the new column, then the names of the two or more columns to be merged, and the sep argument which specifies the character(s) used to join the values in the new column. There are other arguments; take a look at the docs here.\nLet’s continue with our hungry data frame from above. Let’s put the person and utterance columns back together, but this time with a dash separating the speakers and their corresponding utterances.\n\ndf_separated &lt;- hungry |&gt; \n  separate_wider_delim(cols = conversation, delim = \": \", names = c(\"person\", \"utterance\"))\nprint(df_separated)\n\ndf_separated |&gt; \n  unite(col = \"combined\", person:utterance, sep = \" - \")\n\n\nActivity\nUse some of the separate_*() functions and unite() on the data frame of your choice."
  },
  {
    "objectID": "lessons/wrangling.html#pivoting-data-frames",
    "href": "lessons/wrangling.html#pivoting-data-frames",
    "title": "Data Wrangling",
    "section": "Pivoting data frames",
    "text": "Pivoting data frames\nAnother useful, and often necessary, step in (linguistic) data analysis is changing the shape of a data frame before running an analysis. Some functions in R and in other languages (e.g., Python and Julia) and in other software (e.g., Excel) require the data frame to be in a specific shape in order to do certain analyses. Being able to transform the data to that shape is an important preprocessing step.\nThe two main functions for changing the shape of a data frame within tidyverse are pivot_longer() and pivot_wider(). The function pivot_longer() increases the number of rows and decreases the number of columns, while pivot_wider() decreases the number of rows and increases the number of columns.\nLet’s take a super simple example. Let’s create a toy data frame with one row and five columns with the ages of people in a sample:\n\nlibrary(\"tidyverse\")\nages &lt;- tibble(Sammy = 20, Edna = 18, Luisa = 16, Heidi = 14, Evelyn = 12)\nprint(ages)\n\nAs we see, each column represents one person, with the age in the row. Let’s reshape the data frame so that we end up with five rows and two columns, with each row giving info about one person, and the first column giving the person’s name and the second column giving the person’s age:\n\nages |&gt; \n  pivot_longer(cols = Sammy:Evelyn, names_to = \"person\", values_to = \"age\")\n\nThe resulting data frame is a “tidy” data frame because each column has only one variable (e.g., person or age) and the rows represent individual observations. The pre-transformed data frame above was not tidy because each column had more than one variable (e.g., person and age). Read Hadley Wickham’s paper for a full explanation of tidy data.\nThe pivot_wider() does the opposite, that is, it make a long(er) data frame wide(r). Let’s create another toy data frame, this time in long format and make it wide:\n\nheights &lt;- tibble(\n  person = c(\"Sammy\", \"Edna\", \"Luisa\", \"Heidi\", \"Evelyn\"),\n  height_in = c(72, 70, 69, 63, 64))\nprint(heights)\n\nLet’s pivot this data frame to a wide format so that there are five columns, with each person’s name was the column name, and the rows are the corresponding heights in inches:\n\nheights |&gt; \n  pivot_wider(names_from = person, values_from = height_in)\n\n\nToy activity and review\nCreate one data frame with the ages and heights of the five people in the above toy examples. Hint: In addition to pivot_longer() and/or pivot_wider(), you’ll also need our ol’ friend left_join() that we saw above.\nAfter a good-faith effort, if you need help take a look at Dr. Brown’s code below:\n\n\nCode\nages |&gt; \n  pivot_longer(cols = Sammy:Evelyn, names_to = \"person\", values_to = \"age\") |&gt; \n  left_join(heights, by = join_by(person == person))\n\n\n\n\nVowels\nLet’s take a look at a linguistic example from Dr. Stanley. Download the sample_vowel_data.csv file from the LMS &gt; Datasets. Currently, each row represents one time point for each vowel. There are 27 rows containing F1, F2, and F3 measurements from three tokens of three vowels each, at three timepoints.\nLet’s load the dataset into our R session as a data frame:\n\nlibrary(\"tidyverse\")\nvowels &lt;- read_csv(\"/pathway/to/sample_vowel_data.csv\")\nprint(vowels)\n\nLet’s change it into a wider format so that the unit of observation (i.e., row) is one vowel. There are 9 rows. Also, save it to a variable named widest so we can work with it later.\n\nwidest &lt;- vowels |&gt; \n  pivot_wider(names_from = percent, values_from = c(F1, F2, F3))\nprint(widest)\n\nLet’s change it to a longer format. Now the unit of measurement is one format measurement, per time point, per vowel token. There are 81 rows. Note that I’ll need to specify which columns should be affected with the cols argument.\n\nlongest &lt;- vowels |&gt; \n    pivot_longer(cols = c(F1, F2, F3), \n                 names_to = \"formant\", \n                 values_to = \"hz\") |&gt; \n    arrange(formant)\nprint(longest)\n\nLet’s use our widest data frame and pivot it longer. Also, as a shorthand for specifying the columns I want to pivot, I can use matches() and a regex:\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = \"formant_percent\", \n                 values_to = \"hz\")\n\nHere, because each column name contained two pieces of information (F1_25), the resulting data set after pivoting contains a single column with two pieces of information: formant and percent.\nEnter our ol’ friend separate():\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = \"formant_percent\", \n                 values_to = \"hz\") |&gt; \n    separate(formant_percent, \n             into = c(\"formant\", \"percent\"), \n             sep = \"_\")\n\nBut the separation above can happen within pivot functions directly. Below, we provide a vector of names as the value of the names_to argument. This then requires the use of the names_sep argument, which is where you specify the character that separates the two names.\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = c(\"formant\", \"percent\"), \n                 names_sep = \"_\", \n                 values_to = \"hz\")\n\n\n\nRamping it up\nLet’s say we start off with what we see as the widest version of the data in the widest data frame. This is not a hypothetical: this is how a lot of sociophonetics software returns the data to the user! We want to pivot it so that it looks like our original version in the vowels data frame. One way is to make it longer and then wider again.\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = c(\"formant\", \"percent\"),\n                 names_sep = \"_\",\n                 values_to = \"hz\") |&gt; \n    pivot_wider(names_from = formant,\n                values_from = hz)\n\nThis produces the correct output, but it’s a little clunky to do two pivots.\nEnter the black magic that is .value. So instead, we replace the formant column name (in the names_to argument) with .value (mind the period). This then, somehow, takes what would have been the formant column and pivots it wider. Note that when we do this, we don’t need the values_from argument anymore.\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = c(\".value\", \"percent\"),\n                 names_sep = \"_\")\n\n\n\nBig badboy activity\nDownload the “RPM_2019_for_Reliability.xlsx” Excel file from LMS &gt; Datasets module and transform (aka. transpose) the dataset into the shape asked for in the email below that Dr. Brown received from his brother Alan:\n“I’m sending along that data set that I had originally sent that had those problems. Stayc cleaned it all up and we took some students out who spoke other languages at home. The sheet of interest is the first one labeled “For Earl”. As we had talked about, we just need the data oriented horizontally by item number so across the top the columns would run from 1 to 36 left to right. Each row, then, would represent a different student and ’1’s or ’0’s in the columns would indicate correct or incorrect for each item/column. The initial three columns to the left would be the demographic information like gender, grade, immersion/non-immersion. Let me know if that doesn’t make sense.”\nThe first ten rows (i.e., students, of the 104 students) of the transposed dataset should look like the following:\n\n\n\nTransposed dataset\n\n\nA couple words to the wise:\n\nAs is often the case in the wild, this dataset is squirrelly. Take a look at the bottom of the dataset (in Excel) and you’ll see that there is “104” below the last row of the first column. You’ll need to use the range argument in the read_excel() function (within the readxl package) to read in only the cells with actual student data, that is, rows with something in all five columns.\nThe “104” indicates that there are 104 students in the dataset. The answers to the 36 questions that each student responded to are in sets of 36 rows, with the question number indicated by in the Page/Item column. You’ll see that the numbers in that column go from 1 to 36, and then start over at 1 with the next student.\nThere is no unique identifier (e.g., ID number) for each student. Before transposing the data frame, you’ll need to create a new column with a unique identifier. You can use the rep() function, with its each argument, to do this. See Stack Overflow thread for an example.\nAfter transposing the data frame, you should move the column with the unique identifier to the far left.\n\nAfter a good-faith effort to complete this activity, if you need help take a look at Dr. Brown’s code below:\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"readxl\")\n\"/pathway/to/RPM_2019_for_Reliability.xlsx\" |&gt; \n  read_excel(range = \"For Earl!A1:E3745\") |&gt; \n  mutate(id = rep(1:104, each = 36)) |&gt; \n  pivot_wider(names_from = \"Page/Item\", values_from = \"Correct/incorrect\") |&gt; \n  relocate(id, .before = 1)"
  },
  {
    "objectID": "lessons/wrangling.html#working-with-categorical-variables",
    "href": "lessons/wrangling.html#working-with-categorical-variables",
    "title": "Data Wrangling",
    "section": "Working with categorical variables",
    "text": "Working with categorical variables\nThere are a few handy functions in the forcats package within tidyverse for working with categorical variables (aka. factors).\nThe fct_recode() function allows the user to manually change or combine levels (aka. values) of a categorical variable. Let’s create a toy data frame with words and vowels:\n\nlibrary(\"tidyverse\")\nwords &lt;- tibble(\n  word = c(\"wasp\", \"fleece\", \"beat\", \"bit\", \"pat\", \"bus\", \"hot\", \"cool\", \"firm\", \"father\"),\n  vowel = c(\"[ɑ]\", \"[i]\", \"[i]\", \"[ɪ]\", \"[æ]\", \"[ʌ]\", \"[ɔ]\", \"[u]\", \"[ɚ]\", \"[ɑ]\")\n)\nprint(words)\n\nNow, let’s create a new column in which we describe (some of) the vowels:\n\nwords |&gt; \n  mutate(description = fct_recode(vowel, \"high front closed\" = \"[i]\", \"r colored schwa\" = \"[ɚ]\", \"stressed schwa (wedge)\" = \"[ʌ]\"))\n\nThe fct_collapse() function allows the user to put levels into groups:\n\nwords |&gt; \n  mutate(height = fct_collapse(vowel, \n                               hi_V = c(\"[i]\", \"[ɪ]\", \"[u]\"), \n                               lo_V = c(\"[ɑ]\", \"[æ]\"),\n                               other_level = \"mid_V\"))\n\nQuestion: What does the other_level argument do in the function call above?\nThe fct_relevel() function allows the user to reorder the levels of a categorical variable. This is often useful when plotting data and the user wants an order of levels different from alphabetical order (the default).\nLet’s create a toy data frame of voiceless plosives and their voice onset times (VOT):\n\nlibrary(tidyverse)\nvoiceless_plosives &lt;- tibble(\n  plosive = c(\"t\", \"k\", \"p\", \"p\", \"k\", \"t\", \"p\", \"k\", \"t\"),\n  vot = c(23, 34, 45, 56, 67, 78, 89, 91, 21)\n)\nprint(voiceless_plosives)\n\nIf we create a boxplot (preview of what’s coming soon!), we see that “k” is on the left, followed by “p” and “t”, because the default way to sort levels is by alphabetical order:\n\nvoiceless_plosives |&gt; \n  ggplot(aes(x = plosive, y = vot)) +\n  geom_boxplot()\n\nWe probably want “p” followed by “t” and then “k”, as that’s how they are presented in linguistic literature, based on the place of articulation. Let’s use fct_relevel() to change the internal order of the levels, and then replot their VOTs:\n\nvoiceless_plosives |&gt;\n  mutate(plosive = fct_relevel(plosive, \"p\", \"t\", \"k\")) |&gt; \n  ggplot(aes(x = plosive, y = vot)) +\n  geom_boxplot()\n\n\nActivity\nLet’s create a toy data frame with Spanish words with a dental or alveolar word-final sibilant:\n\nlibrary(\"tidyverse\")\nsibilants &lt;- tibble(\n  person = c(\"Raúl\", \"Raúl\", \"José\", \"José\", \"María\"),\n  target_wd = c(\"árboles\", \"mesas\", \"lápiz\", \"es\", \"pues\"),\n  next_wd = c(\"de\", \"en\", \"y\", \"que\", \".\"),\n  next_segment = c(\"d\", \"e\", \"i\", \"k\", \"#\")\n)\nprint(sibilants)\n\nYour task is to create a new column that groups the vowels into one level, the consonants into their own group, and then all other following segments should be placed into an other level.\nAfter a good-faith effort, if you need help, take a look at Dr. Brown’s code below:\n\n\nCode\nsibilants |&gt;  \n  mutate(next_sound_type = fct_collapse(next_segment, vowel = c(\"i\",\"e\"), consonant = c(\"d\", \"k\"), other_level = \"other\"))"
  },
  {
    "objectID": "lessons/r_rstudio.html",
    "href": "lessons/r_rstudio.html",
    "title": "R and RStudio",
    "section": "",
    "text": "Students will become familiar with the R programming language a bit and the RStudio Integrated Development Environment (IDE)"
  },
  {
    "objectID": "lessons/r_rstudio.html#objective",
    "href": "lessons/r_rstudio.html#objective",
    "title": "R and RStudio",
    "section": "",
    "text": "Students will become familiar with the R programming language a bit and the RStudio Integrated Development Environment (IDE)"
  },
  {
    "objectID": "lessons/r_rstudio.html#the-r-programming-language",
    "href": "lessons/r_rstudio.html#the-r-programming-language",
    "title": "R and RStudio",
    "section": "The R programming language",
    "text": "The R programming language\n\nR (here) is a programming language specifically designed for statistical analysis and visualization.\nAs an open-source language, there are many third-party add-on packages (here) that extend the use of R that are available on the Comprehensive R Archive Network (aka. CRAN here).\nSo called “Task Views” (here) collect and briefly describe packages related to specific fields, including one for Natural Language Processing (here)."
  },
  {
    "objectID": "lessons/r_rstudio.html#rstudio-ide",
    "href": "lessons/r_rstudio.html#rstudio-ide",
    "title": "R and RStudio",
    "section": "RStudio IDE",
    "text": "RStudio IDE\n\nIntegrated Development Environments (IDEs) facilitate writing computer code.\nRStudio (here) from the company Posit (here) is the go-to IDE for R.\n\nHas many useful keyboard shortcuts, cf. Help &gt; Keyboard Shortcuts Help\n\nEarl’s favorites are:\n\nAssignment operator: ALT + -\nPipe operator in tidyverse: CTRL/CMD + SHIFT + m\n\n\nHas many cheat sheets for various tools within R and RStudio, cf. Help &gt; Cheat Sheets"
  },
  {
    "objectID": "lessons/r_rstudio.html#r-projects",
    "href": "lessons/r_rstudio.html#r-projects",
    "title": "R and RStudio",
    "section": "R projects",
    "text": "R projects\n\nR projects help keep things organized:\n\nData files, like CSV (.csv) and/or Excel (.xlsx) files\nSource code files, like R scripts (.r or .R files)\n\nTo create an R project:\n\nFile &gt; New Project..."
  },
  {
    "objectID": "lessons/r_rstudio.html#activity",
    "href": "lessons/r_rstudio.html#activity",
    "title": "R and RStudio",
    "section": "Activity",
    "text": "Activity\n\nStudents explore R and RStudio IDE, perhaps using the RStudio cheat sheet (Help &gt; Cheat Sheets &gt; RStudio IDE Cheat Sheet)"
  },
  {
    "objectID": "lessons/programming_basics.html",
    "href": "lessons/programming_basics.html",
    "title": "Programming basics",
    "section": "",
    "text": "Students will learn to code the basic building blocks of programming in R."
  },
  {
    "objectID": "lessons/programming_basics.html#objective",
    "href": "lessons/programming_basics.html#objective",
    "title": "Programming basics",
    "section": "",
    "text": "Students will learn to code the basic building blocks of programming in R."
  },
  {
    "objectID": "lessons/programming_basics.html#primitive-data-types-in-r",
    "href": "lessons/programming_basics.html#primitive-data-types-in-r",
    "title": "Programming basics",
    "section": "Primitive data types in R",
    "text": "Primitive data types in R\n\ninteger\n\nThis a whole number, i.e., there is no decimal component, for example “5”.\nTo specify an integer in R, type an uppercase “L” immediately to the right of the number (i.e., no space between the number and the “L”).\n\nE.g., 5L\n\n\nnumeric\n\nThis is a number with a decimal component, for example, “3.14”. Note: Unless an “L” is placed to the right of a whole number, R treats it as a numeric.\nTo specific a numeric in R, just the type the good ol’ fashioned number.\n\nE.g., 3.14\n\nNote: Dr. Brown may refer to this data type as a “float” because of language transfer from Python and Julia.\n\ncharacter\n\nE.g., \"hello world\" and c(\"hello\", \"hola\", \"hej\")\n\nNote: Dr. Brown will likely refer to this data type as “string” because of language transfer from Python and Julia.\nYou can extract part of a string with the sub_str() function (doc here).\n\n\nlogical\n\nThis data type has one of two values, either TRUE or FALSE (or T or F for shorthand).\nDr. Brown may refer to this data type as a “Boolean” because of language transfer from Python and Julia.\n\nThe class() function returns the data type of a variable or value.\n\nEg. class(5L) returns integer, while class(\"hello world\") returns character.\n\n\nActivity\n\nStudents use the class() function to become familiar with the data type of the values that they type."
  },
  {
    "objectID": "lessons/programming_basics.html#operators-in-r",
    "href": "lessons/programming_basics.html#operators-in-r",
    "title": "Programming basics",
    "section": "Operators in R",
    "text": "Operators in R\n\nAssignment operator: There are two assignment operators in R. The most common is &lt;- but = also works. For example:\n\nfruit &lt;- \"apple\"\nfruits &lt;- c(\"apple\", \"banana\", \"orange\", \"mango\")\n\nKeyboard shortcut in RStudio: ALT/OPT + -\n\npets = c(\"dog\", \"cat\", \"fish\", \"Madagascar hissing cockroach\")\nage = 46 (note: this creates a numeric rather than an integer; if an integer is wanted: age = 46L)\n\nInclusion operator: %in% tests for inclusion of a value in a collection of values (e.g., a vector), for example:\n\n\"apple\" %in% c(\"banana\", \"apple\", \"mango\") returns TRUE\n\"kiwi\" %in% c(\"banana\", \"apple\", \"mango\") returns FALSE\n\nEqual operator: == (i.e., two equal signs together with no space between them) tests whether the left-hand value and the right-hand value are identical, for example:\n\n\"mango\" == \"mango\" returns TRUE\n\"apple\" == \"manzana\" returns FALSE\n\"Hannah\" == \"HANNAH\" returns FALSE\n\nSuper important note: Computers treat lowercase and uppercase letters differently.\n\nThe equal operator can be used with a string on the left-hand side an a vector of strings on the right-hand side, for example:\n\n\"apple\" == c(\"banana\", \"apple\", \"mango\") returns FALSE TRUE FALSE\nQuick discussion: Speculate with a neighbor about the reason the above expression returns FALSE TRUE FALSE.\n\n\n\nActivity\n\nStudents use these three operators to create variables and vectors, and test for inclusion of a string in a vector of strings."
  },
  {
    "objectID": "lessons/programming_basics.html#comments",
    "href": "lessons/programming_basics.html#comments",
    "title": "Programming basics",
    "section": "Comments",
    "text": "Comments\n\nComments within computer code helps the human readers, whether other humans or your later self, to quickly understand what the various parts of a computer script do.\nComments in R are specified with a hashtag, for example:\n\n\n# assign a value to a variable\ndog &lt;- \"fido\"\n\n# create a vector of multiple elements\nkids &lt;- c(\"Bobby\", \"Luisa\", \"José\")"
  },
  {
    "objectID": "lessons/programming_basics.html#if-else-in-r",
    "href": "lessons/programming_basics.html#if-else-in-r",
    "title": "Programming basics",
    "section": "if else in R",
    "text": "if else in R\n\nThe logic is simple: Ask a question, and if the answer is TRUE, then do this thing, but if the answer is FALSE, then do that thing.\n\n\n\nTwo approaches to if else in R:\n\nThe most common approach is to use a code block. See an example in the Stack Overflow answer here.\nA less common approach, but super useful for simple if else cases, is to use a function:\n\nbase R ifelse() function here;\ndpylr (part of the tidyverse ecosystem) if_else() function here.\n\n\n\nActivity\n\nStudents create a string with a single word, and then use if else (either a code block of a function) to print to the user whether the word begins with a vowel or a consonant.\n\nHint 1: The print() and cat() can be used to print to the console.\nHint 2: The sub_str() function can be used to extract a sub part of a string.\nHint 3: The %in% operator tests whether the left-hand value is within the right-hand collection."
  },
  {
    "objectID": "lessons/programming_basics.html#loops-in-r",
    "href": "lessons/programming_basics.html#loops-in-r",
    "title": "Programming basics",
    "section": "Loops in R",
    "text": "Loops in R\n\nThe mighty and super useful for loop iterates over all elements of a collection (e.g., a vector), for example see below (and see another example here):\n\n\n# create a vector\nfruits &lt;- c(\"apple\", \"mango\", \"banana\", \"orange\")\n\n# loop over the elements of the vector\nfor (fruit in fruits) {\n  print(fruit)  # print the current element to the console\n}\n\n\nThe less-common-but-still-useful while loop tests the conditional statement at the beginning of each iteration and runs the body of the loop if the statement evaluates to TRUE. See an example here.\nUseful keywords for both for loops and while loops:\n\nThe next keyword skips the rest of the current iteration and continues to the next iteration. This is very much like continue in Python and Julia.\nThe break keyword stops the loop completely, regardless of which iteration it was in, and no further iteration are executed.\n\n\nActivity\n\nStudents create a for loop to iterate from 1 to 10, skipping even numbers and printing out odd numbers.\n\nHint: The modulus operator %% will be helpful (see here)."
  },
  {
    "objectID": "lessons/programming_basics.html#defining-functions-in-r",
    "href": "lessons/programming_basics.html#defining-functions-in-r",
    "title": "Programming basics",
    "section": "Defining functions in R",
    "text": "Defining functions in R\n\nA very useful ability in R (and all programming languages) is for a user to define their own custom function.\nThe function() function does the trick.\n\nSee a tutorial here.\n\n\nActivity\n\nStudents define a function that takes as input a word and returns as output a logical value (aka. Boolean value) indicating whether the word begins with one of the five orthographic vowels (i.e., a, e, i, o, u).\n\nHint: The %in% keyword will be helpful here.\n\nNow for a little fun and to put these basic programming skills together: Students define a function (likely with smaller helper functions) that translates a sentence from English into Pig Latin. A little refresher on Pig Latin: If a word begins with a vowel, the word yay is added to the end of it; if a word begins with a consonant or consonant cluster (e.g., ch, gr), that consonant or consonant cluster is moved to the end of the word followed by ay.\n\nHint: the stringr package (part of the tidyverse) will be useful here, especially the str_c(), str_sub(), str_split() (and unlist()) functions, as will the letters and LETTERS built-in constants.\nHint: After a good-faith effort, if you need help, see the script written by Dr. Brown by clicking on “▶ Code” below.\n\n\n\n\nCode\nsuppressPackageStartupMessages(library(\"tidyverse\"))\n\n# helper function 1, for vowel words\ntrans_v &lt;-  function(wd, vowels) {\n  return(str_c(wd, \"yay\"))\n}\n\n# helper function 2, for consonant words\ntrans_c &lt;- function(wd, vowels, first_let) {\n  second_let &lt;- str_sub(wd, 2, 2)\n  if (!str_to_lower(second_let) %in% vowels) {\n    first_two &lt;- str_sub(wd, 1, 2)\n    rest_wd &lt;- str_sub(wd, 3, str_length(wd))\n    return(str_c(rest_wd, first_two, \"ay\"))\n  } else {\n    rest_wd &lt;- str_sub(wd, 2, str_length(wd))\n    return(str_c(rest_wd, first_let, \"ay\"))\n  }\n}\n\n# the main function\ntrans_pig &lt;- function(sentence, vowels) {\n  wds &lt;- unlist(str_split(sentence, \"\\\\s+\"))\n  trans_sent &lt;- \"\"\n  for (wd in wds) {\n    first_let &lt;- str_sub(wd, 1, 1)\n    if (str_to_lower(first_let) %in% vowels) {\n      # this is a vowel word\n      trans_sent &lt;- str_c(trans_sent, trans_v(wd, vowels), \" \")\n    } else {\n      # the current word is a consonant word\n      trans_sent &lt;- str_c(trans_sent, trans_c(wd, vowels, first_let), \" \")\n    }\n  }\n  return(str_trim(trans_sent))\n}\n\n### test the function\nsentence &lt;- \"I do not like green eggs and ham.\"\nvowels &lt;- c(\"a\", \"e\", \"i\", \"o\", \"u\")\nprint(trans_pig(sentence, vowels))"
  },
  {
    "objectID": "lessons/quanteda.html",
    "href": "lessons/quanteda.html",
    "title": "quanteda",
    "section": "",
    "text": "Students will analyze textual data (aka. texts) with the quanteda R package."
  },
  {
    "objectID": "lessons/quanteda.html#objective",
    "href": "lessons/quanteda.html#objective",
    "title": "quanteda",
    "section": "",
    "text": "Students will analyze textual data (aka. texts) with the quanteda R package."
  },
  {
    "objectID": "lessons/quanteda.html#using-r-for-textual-analysis",
    "href": "lessons/quanteda.html#using-r-for-textual-analysis",
    "title": "quanteda",
    "section": "Using R for textual analysis",
    "text": "Using R for textual analysis\nThe quanteda R package is a super useful package for analyzing texts in R. Some of the techniques are corpus linguistic techniques through and through: tokenizing into words or sentences, keyword-in-context, removing stopwords. Other techniques might be better considered computational linguistic techniques: sentiment analysis, document feature matrix. Regardless, we need to get to know this package, as I can’t call myself a good professor of Linguistics Data Analysis with R without looking at this package."
  },
  {
    "objectID": "lessons/quanteda.html#install-the-package-and-its-friends",
    "href": "lessons/quanteda.html#install-the-package-and-its-friends",
    "title": "quanteda",
    "section": "Install the package and its friends",
    "text": "Install the package and its friends\nStep #1: Install the quanteda package and the other related package that the quanteda creators recommend:\n\ninstall.packages(c(\"quanteda\", \"quanteda.textmodels\", \"quanteda.textstats\", \"quanteda.textplots\", \"readtext\", \"spacyr\", \"remotes\"), repos = \"http://cran.rstudio.com\")\n\nThen, we need to install two packages on a Github:\n\nremotes::install_github(\"quanteda/quanteda.corpora\")\nremotes::install_github(\"kbenoit/quanteda.dictionaries\")"
  },
  {
    "objectID": "lessons/quanteda.html#create-a-corpus",
    "href": "lessons/quanteda.html#create-a-corpus",
    "title": "quanteda",
    "section": "Create a corpus",
    "text": "Create a corpus\nLet’s create a corpus in quanteda! First, let’s get their example code here working on our computers.\nLet’s read in the three currently published volumes of Saints. First, download the zipped file Saints.zip from the LMS. Then, unzip (aka. decompress or extract) the zipped file so that you end up with a directory with several subdirectories organized by volume number.\nLet’s read in the corpus such that we add a column with metadata about which volume each file is from. There are three ways (that Dr. Brown can think of) to do that.\n\nOption 1: Each volume individually (most verbose)\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# load each volume as a separate corpus\nsaints01 &lt;- readtext(\"Volume01/*.txt\") %&gt;% corpus()  # same as: saints01 &lt;- corpus(readtext(\"Volume01/*.txt\"))\ndocvars(saints01, \"Volume\") &lt;- \"01\"\n\nsaints02 &lt;- readtext(\"Volume02/*.txt\") %&gt;% corpus()\ndocvars(saints02, \"Volume\") &lt;- \"02\"\n\nsaints03 &lt;- readtext(\"Volume03/*.txt\") %&gt;% corpus()\ndocvars(saints03, \"Volume\") &lt;- \"03\"\n\n# combine the three corpora into one, with the docvar column identifying which volume each file comes from\nsaints &lt;- saints01 + saints02 + saints03\n\nprint(summary(saints))\n\n\n\nOption 2: Automate the previous approach (less verbose)\nWe can use the eval() and parse() functions in base R to automate the previous approach. Shall we?\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# our ol' friend the for loop \nfor (i in 1:3) {\n  to_str &lt;- str_glue(\"saints0{i} &lt;- readtext('Volume0{i}/*.txt') %&gt;% corpus(); docvars(saints0{i}, 'Volume') &lt;- '0{i}'\")\n  eval(parse(text = to_str))\n}\n\n# combine the three corpora into a new fourth one\nsaints &lt;- saints01 + saints02 + saints03\n\nprint(summary(saints))\n\n\n\nOption 3: Use the docvarfrom, dvsep, and docvarnames arguments (least verbose)\nThe readtext() function has a handful of arguments. The docvarfrom, dvsep, and the docvarnames arguments can create a new docvar (i.e., metadata about the corpus) from the filenames and/or the filepaths. Take a look at the documentation here.\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# one pipeline will do the trick!\nsaints &lt;- readtext(file = \"*/*.txt\", docvarsfrom = \"filepaths\", dvsep = \"/\", docvarnames = c(\"Volume\", \"filename\")) %&gt;% corpus()\n\nprint(summary(saints))\n\n\n\nActivity\nYou guessed it! It’s your turn. Create a corpus of your choice with files of your choice (e.g., perhaps from Project Gutenberg). Add a docvar of your choice, which may mean you need to preprocess the files a bit by specifying filenames or directory structure in a certain way."
  },
  {
    "objectID": "lessons/quanteda.html#creating-subcorpora",
    "href": "lessons/quanteda.html#creating-subcorpora",
    "title": "quanteda",
    "section": "Creating subcorpora",
    "text": "Creating subcorpora\nIt is super simple to create a subcorpus using a docvar (i.e., metadata about the files in the corpus). For example, let’s say we have our one Saints corpus with a docvar of Volume with values of “01”, “02”, and “03”. In order to create a subcorpus of just the files in Volume 1, we could run the following code. See the docs here.\n\nvol01 &lt;- corpus_subset(saints, Volume == \"01\") \n\n\nActivity\nGive it a try! Create a subcorpus from a larger corpus of your choice using the docvar of your choice."
  },
  {
    "objectID": "lessons/quanteda.html#keyword-in-context-kwic",
    "href": "lessons/quanteda.html#keyword-in-context-kwic",
    "title": "quanteda",
    "section": "Keyword-in-context (KWIC)",
    "text": "Keyword-in-context (KWIC)\nLet’s perform into a corpus linguistic technique: the keyword-in-context display or concordance lines or concordances. It is super easy to do so with the tokens() and kwic() functions. See example here.\nLet’s start out slowly with a simple word search using our Saints corpus:\n\n# Assuming packages are loaded and corpus has been created\n\n# tokenize and then get concordances\ntoks &lt;- tokens(saints)\nconcordances &lt;- kwic(toks, pattern = \"tree\")\nprint(concordances)\n\nLet’s ramp it up with some regular expression pizzazz. Question: What does the regular expression find?\n\ntoks &lt;- tokens(saints)\nconcordances &lt;- kwic(toks, pattern = \"\\\\w+(\\\\w{2,})\\\\W+\\\\w+\\\\1\\\\b\", valuetype = \"regex\")\nprint(concordances)\n\nThe main tokenizer function tokens() has a handful of arguments that are worth inspecting. Take a gander here. Also, there many other tokenizer functions that can be useful. Take a look here.\n\nActivity\nGet to know the tokenizing functions and the kwic() function (example here and docs here) with the corpus of your choice."
  },
  {
    "objectID": "lessons/quanteda.html#searching-with-a-dictionary",
    "href": "lessons/quanteda.html#searching-with-a-dictionary",
    "title": "quanteda",
    "section": "Searching with a dictionary",
    "text": "Searching with a dictionary\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# one pipeline will do the trick!\nsaints &lt;- readtext(file = \"*/*.txt\", docvarsfrom = \"filepaths\", dvsep = \"/\", docvarnames = c(\"Volume\", \"filename\")) %&gt;% corpus()\n\n# create a dictionary with named vectors\ndict &lt;- dictionary(list(temple = c(\"temple\", \"sealing\", \"spire\", \"open house\"),\n                missionary = c(\"missionary\", \"mission\", \"labor\\\\b\")))\n\n# search with the dictionary\nkwic(tokens(saints), pattern = dict, valuetype = \"regex\") %&gt;% print()"
  },
  {
    "objectID": "lessons/quanteda.html#collocations",
    "href": "lessons/quanteda.html#collocations",
    "title": "quanteda",
    "section": "Collocations",
    "text": "Collocations\nYou can retrieve collocations with the textstat_collocations() function here. At the time of making this lesson plan, it only calculates the lambda metric proposed by Blaheta and Johnson (2001):\nBlaheta, D. & Johnson, M. (2001). Unsupervised learning of multi-word verbs. Presented at the ACLEACL Workshop on the Computational Extraction, Analysis and Exploitation of Collocations.\nHowever, the doc page says that there are plans to add more measures. Hopefully log-dice is in the works, as it’s a great word association metric.\nLet’s retrieve the collocations in the first three volumes of Saints:\n\nlibrary(quanteda)\nlibrary(readtext)\nlibrary(tidyverse)\nlibrary(quanteda.textstats)\n\n# change directories into the Saints directories\nsetwd(\"/pathway/to/Saints/txt\")\n\n# load the Saints corpus, creating a docvar with the Volume that each file belongs to\nsaints &lt;- readtext(file = \"*/*.txt\", docvarsfrom = \"filepaths\", dvsep = \"/\", docvarnames = c(\"Volume\", \"filename\")) %&gt;%  corpus() \n# retrieve collocations\nsaints %&gt;% \n  tokens() %&gt;% \n  textstat_collocations()  %&gt;% \n  arrange(desc(lambda)) %&gt;%  # arrange them in descending order by lambda\n  head(50)  # show only first 50 collocations\n\nThat’s okay, but there are lots of proper nouns and low-frequency collocations. We can remove those with the min_count argument: textstat_collocations(min_count = 25).\nIf we want, we can also remove function words, which have little semantic value, with a stopword list:\n\nlibrary(\"stopwords\")\nsaints %&gt;% \n  tokens() %&gt;% \n  tokens_remove(stopwords(\"en\")) %&gt;% \n  textstat_collocations(min_count = 25) %&gt;% \n  arrange(desc(lambda)) %&gt;% \n  head(50)\n\n\nActivity\nGo for it! That is, retrieve collocations in a corpus of your choice. Look at and possibly use some of the other arguments in the textstat_collocations() function. Reminder: In order to pull up the doc page of a function within the RStudio IDE, you can type a question mark and then the name of the function (no space between them) in the console, e.g., ?textstat_collocations. And here’s that same doc page on the internet."
  },
  {
    "objectID": "lessons/quanteda.html#document-feature-matrix",
    "href": "lessons/quanteda.html#document-feature-matrix",
    "title": "quanteda",
    "section": "Document-feature matrix",
    "text": "Document-feature matrix\nWithin quanteda, a document-feature matrix is very similar to a document-term matrix (if you know what that is). It’s a tabular dataset (hence the name “matrix”) and has the name of the files (aka. documents) as rows, and the features (i.e., words and punctuation) are the columns. Here’s an example. Let’s take a look with the Saints corpus:\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# one pipeline will do the trick!\nsaints &lt;- readtext(file = \"*/*.txt\", docvarsfrom = \"filepaths\", dvsep = \"/\", docvarnames = c(\"Volume\", \"filename\")) %&gt;% corpus()\n\nsaints %&gt;% \n  tokens() %&gt;% \n  dfm()\n\nWe can use a dictionary of patterns to limit the number of features (i.e., columns) that are returned in the matrix:\n\ndict &lt;- dictionary(list(\n  temple = c(\"temple\", \"sealing\", \"spire\", \"open house\"),\n  missionary = c(\"missionary\", \"mission\", \"labor\"))\n  )\n\nsaints %&gt;% \n  tokens() %&gt;% \n  tokens_lookup(dictionary = dict) %&gt;% \n  dfm() %&gt;% \n  as_tibble() %&gt;%  # cast to tibble in order to use arrange below\n  arrange(desc(temple))"
  },
  {
    "objectID": "lessons/quanteda.html#similarities-between-texts",
    "href": "lessons/quanteda.html#similarities-between-texts",
    "title": "quanteda",
    "section": "Similarities between texts",
    "text": "Similarities between texts\nYou can plot similarities between texts in a corpus. Let’s run the example code in the Quick Guide here.\nNow, let’s make this work with our Saints corpus."
  },
  {
    "objectID": "lessons/quanteda.html#keyness-analysis",
    "href": "lessons/quanteda.html#keyness-analysis",
    "title": "quanteda",
    "section": "Keyness analysis",
    "text": "Keyness analysis\nWe can use quanteda to perform keyness analysis, that is, to identify the keywords that typify a section of the corpus from the rest of the corpus.\nLet’s get the creators’ example working in the doc page (in console ?textstat_keyness) and here.\n\nActivity\nTake some time to look over the other analysis abilities of quanteda and try to get one or two working with a corpus of your choice."
  }
]