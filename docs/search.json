[
  {
    "objectID": "lessons/File_IO.html",
    "href": "lessons/File_IO.html",
    "title": "File I/O in R",
    "section": "",
    "text": "Students will read data from files and writing data to files."
  },
  {
    "objectID": "lessons/File_IO.html#objective",
    "href": "lessons/File_IO.html#objective",
    "title": "File I/O in R",
    "section": "",
    "text": "Students will read data from files and writing data to files."
  },
  {
    "objectID": "lessons/File_IO.html#directory-operations",
    "href": "lessons/File_IO.html#directory-operations",
    "title": "File I/O in R",
    "section": "Directory operations",
    "text": "Directory operations\n\nComputers are structured in a hierarchal filesystem (e.g., “/Users/Fernando/Documents/my_novel.docx”).\nOften, R scripts need to change directories (aka. folders) in order to access specific files.\nR has several functions to move around a filesystem. Here are a few of them:\n\ngetwd() (here) prints to the console the current working directory (i.e., the directory that the script currently has access to).\nsetwd() (here) sets a directory so that the script has access to it.\ndir() (here, an alias for list.files()) lists the contents of a directory, either only in the one directory or also in all sub-directories (with the recursive argument)."
  },
  {
    "objectID": "lessons/File_IO.html#text-files-.txt",
    "href": "lessons/File_IO.html#text-files-.txt",
    "title": "File I/O in R",
    "section": "Text files (.txt)",
    "text": "Text files (.txt)\n\nReading in data (aka. input) from text files can be accomplished in two ways:\n\nWay 1: Slurp all text in the file at once and hold it in the working memory of the computer:\n\nscan() (here) returns a vector (the default) or list.\nreadLines() (here) in base R returns a vector with each line (i.e., hard return) in the input file as an element. This is a bare-bones version of scan().\nread_lines() (here) in the readr package (part of tidyverse) is a quicker version of readLines().\nread_file() (here) in the readr package slurps all text into a single string.\n\nWay 2: Read data line-by-line:\n\nThis is useful when the text file is massive and would be difficult to hold in memory at once. This approach holds only one line at a time in memory.\nSteps:\n\nCreate a connection to the file with the file() function (here).\nUse the readLines(n = 1) function in a while loop.\nSee an example here.\n\n\n\nWriting out data (aka. output) to a text files can be accomplished with several functions:\n\ncat() (here) in base R can write out to a text file when the file argument gives a pathway to a file.\nwriteLines() (here) is the output version of readLines() mentioned above.\nwrite_lines() (here) function is the output version of read_lines() mentioned above.\nwrite_file() (here) output-equivalent of read_file() above."
  },
  {
    "objectID": "lessons/File_IO.html#csv-files-.csv",
    "href": "lessons/File_IO.html#csv-files-.csv",
    "title": "File I/O in R",
    "section": "CSV files (.csv)",
    "text": "CSV files (.csv)\n\nReading in tabular datasets from CSV files can be accomplished with several functions:\n\nread.table() (here) is a versatile function with many arguments, that returns a data frame.\nread_csv() (here) in the readr package (part of tidyverse) reads CSV files that have a comma as the separator between columns, and returns a tibble.\nread_tsv() (here) in the readr package reads TSV files that have a tab as the separator between columns, and returns a tibble.\n\nNote: Some files with the extension .csv or .txt are actually .tsv files, that is, the column separator is a tab.\n\n\nWriting out a data frame to a CSV files is a cinch:\n\nwrite.table() (here) in base R is the output equivalent of read.table() mentioned above.\nwrite_csv() (here) in the readr package is the output equivalent of the read_csv() mentioned above.\n\n\n\nActivity\n\nInstall the tidyverse suite of packages with the command install.packages(\"tidyverse\") in the console.\nDownload some TXT files of your choice (perhaps from Project Gutenberg or Saints.zip from the Canvas Module “Datasets”).\nCreate a script that reads in all TXT files in a directory (and perhaps any subdirectories) and simply print the text to the console.\n\nInclude library(\"tidyverse\") at the top of your script (i.e., .r file.)\n\nRamp it up by breaking up the text into words and printing those to the console.\nNow for some fun, as a class let’s count the number of words in each text file, and print the name of the file and the number of words to the console.\nAs a final step, let’s write out a CSV file with two columns: column A = the name of the file, column B = the number of words in that file."
  },
  {
    "objectID": "lessons/File_IO.html#excel-files",
    "href": "lessons/File_IO.html#excel-files",
    "title": "File I/O in R",
    "section": "Excel files",
    "text": "Excel files\n\nReading in (input) an Excel is easy with readxl::read_excel().\n\nYou can specify which worksheet to read (with the sheet argument), or even a specific set of cells within a specific worksheet (with the range argument).\n\nWriting out (output)\n\nxlsx::write.xlsx() function does the trick.\n\n\n\nActivity\n\nDownload an Excel (.xlsx) file of your choice (perhaps from the Module “Datasets” in Canvas) or use one that’s already on your harddrive.\nOpen the Excel file (in Excel) and inspect the worksheet(s) to figure out where the data is (i.e., sheet name, cell range).\nRead in the appropriate worksheet and display it within RStudio with the view() function."
  },
  {
    "objectID": "lessons/File_IO.html#spss-.sav-stata-.dta-and-sas-.sas-files",
    "href": "lessons/File_IO.html#spss-.sav-stata-.dta-and-sas-.sas-files",
    "title": "File I/O in R",
    "section": "SPSS (.sav), Stata (.dta), and SAS (.sas) files",
    "text": "SPSS (.sav), Stata (.dta), and SAS (.sas) files\n\nReading in (input)\n\nThe haven R package does the trick.\n\nWriting out (output)\n\nWho cares? You should output it as something more cross-platform-friendly like CSV or feather (aka. arrow) or parquet.\n\n\n\nActivity\n\nDownload the SPSS (.sav) file in the CMS.\nRead in the SPSS file and display it with view()."
  },
  {
    "objectID": "lessons/File_IO.html#feather-.feather-files",
    "href": "lessons/File_IO.html#feather-.feather-files",
    "title": "File I/O in R",
    "section": "Feather (.feather) files",
    "text": "Feather (.feather) files\n\nThis file format is quickly read and written, which are good for big data files.\nReading in (input)\n\nThe feather::read_feather() function does it.\n\nWriting out (output)\n\nThe feather::write_feather() function does it."
  },
  {
    "objectID": "lessons/data_structures.html",
    "href": "lessons/data_structures.html",
    "title": "Data structures in R",
    "section": "",
    "text": "Students will become familiar with common data structures in R."
  },
  {
    "objectID": "lessons/data_structures.html#objective",
    "href": "lessons/data_structures.html#objective",
    "title": "Data structures in R",
    "section": "",
    "text": "Students will become familiar with common data structures in R."
  },
  {
    "objectID": "lessons/data_structures.html#common-data-structures-in-r",
    "href": "lessons/data_structures.html#common-data-structures-in-r",
    "title": "Data structures in R",
    "section": "Common data structures in R",
    "text": "Common data structures in R\n\nvector: A single dimension collection of values of the same data type (e.g., all numeric or all character). Kinda like a list in Python or an array in Julia.\n\nNote: Values of different data types are coerced to the more complex data type, for example, if a numeric (aka. float) and an integer are put into the same vector, both values will have a numeric (aka. float) data type.\nA vector (and list, see below) can be created with the c() function (doc here).\n\ndata.frame: A tabular data structure with columns and rows, much like a table in a spreadsheet like Excel or Google Sheets. See doc here.\ntibble: A slightly modified, and better, data.frame in the tibble (doc here) package within the tidyverse metapackage or ecosystem (doc here). See doc here.\ndata.table: Another tabular data structure, written in C under the hood (so it’s fast) from the data.table package here. See a tutorial here.\nmatrix: Another tabular data structure in base R. Less common than data.frame and tibble and data.table. See doc here.\nlist: A flexible data structure that can hold whatever, including other data structures. See doc here and tutorial here.\n\nActivity\n\nStudents create a few vectors of the same length (i.e., the same number of elements).\nStudents create a single data frame with the several vectors created above. The doc here may be helpful.\nStudents create several data frames and put them in a list.\nStudent iterate over the list with a for loop and print each data frame to the console.\nStudents iterate over the list and then iterate over the row of each data frame, and print each row."
  },
  {
    "objectID": "lessons/PCA.html",
    "href": "lessons/PCA.html",
    "title": "PCA",
    "section": "",
    "text": "Students perform Principal Components Analysis (PCA)"
  },
  {
    "objectID": "lessons/PCA.html#objective",
    "href": "lessons/PCA.html#objective",
    "title": "PCA",
    "section": "",
    "text": "Students perform Principal Components Analysis (PCA)"
  },
  {
    "objectID": "lessons/PCA.html#acknowledgment",
    "href": "lessons/PCA.html#acknowledgment",
    "title": "PCA",
    "section": "Acknowledgment",
    "text": "Acknowledgment\nMost of the following material comes from Levshina (2015), chapter 18 “Multidimensional analysis of register variation: Principal Components Analysis and Factor Analysis”.\nLevshina, Natalia. 2015. How to do Linguistics with R: Data Exploration and Statistical Analysis. Amsterdam / Philadelphia: John Benjamins."
  },
  {
    "objectID": "lessons/PCA.html#many-dimensions",
    "href": "lessons/PCA.html#many-dimensions",
    "title": "PCA",
    "section": "Many dimensions",
    "text": "Many dimensions\nThe purpose of PCA is to reduce the number of dimensions (aka. variables) in a dataset. This is especially useful when you have many variables that are correlated to each other.\nLet’s install the packages we’ll need:\n\ninstall.packages(c(\"Rling\", \"psych\", \"FactoMineR\", \"factoextra\", \"corrplot\"), repos = \"http://cran.rstudio.com\")\n\nNow, let’s load in the reg_bnc dataset from the Rling package, which has relative frequencies of different word classes in 69 subregisters of the British National Corpus, organized by six metaregisters: Academic, Fiction, News, Non-academic prose, Spoken, and Miscellaneous.\n\nsuppressPackageStartupMessages(library(\"tidyverse\"))\nlibrary(\"Rling\")\ndata(reg_bnc)\n\nIn order to run PCA, there needs to be some multicollinearity, otherwise there’s no need to try to reduce the number of dimensions (aka. variables) in order to try to find underlying components.\nWe can look at the correlations (Pearson’s r, to be exact) between all pairwise comparisons of variables. A general rule of thumb mentioned by Levshina is that the Pearson’s r correlations should have an absolute value of at least 0.3. Let’s take a look:\n\nreg_bnc %&gt;% \n  select(-Reg) %&gt;% # keep only continuous data types\n  cor() %&gt;% \n  round(2)\n\n          Ncomm Nprop Vpres Vpast    P1    P2   Adj ConjCoord ConjSub Interject\nNcomm      1.00  0.23 -0.41 -0.21 -0.83 -0.75  0.86     -0.13   -0.52     -0.67\nNprop      0.23  1.00 -0.34  0.36 -0.37 -0.50  0.13     -0.45   -0.68     -0.39\nVpres     -0.41 -0.34  1.00 -0.46  0.42  0.50 -0.35      0.21    0.48      0.41\nVpast     -0.21  0.36 -0.46  1.00  0.03 -0.11 -0.16      0.07   -0.22      0.02\nP1        -0.83 -0.37  0.42  0.03  1.00  0.80 -0.79      0.23    0.57      0.70\nP2        -0.75 -0.50  0.50 -0.11  0.80  1.00 -0.70      0.31    0.57      0.79\nAdj        0.86  0.13 -0.35 -0.16 -0.79 -0.70  1.00      0.04   -0.39     -0.62\nConjCoord -0.13 -0.45  0.21  0.07  0.23  0.31  0.04      1.00    0.26      0.18\nConjSub   -0.52 -0.68  0.48 -0.22  0.57  0.57 -0.39      0.26    1.00      0.36\nInterject -0.67 -0.39  0.41  0.02  0.70  0.79 -0.62      0.18    0.36      1.00\nNum        0.21  0.28 -0.28 -0.13 -0.25 -0.16  0.03     -0.41   -0.28     -0.09\n            Num\nNcomm      0.21\nNprop      0.28\nVpres     -0.28\nVpast     -0.13\nP1        -0.25\nP2        -0.16\nAdj        0.03\nConjCoord -0.41\nConjSub   -0.28\nInterject -0.09\nNum        1.00\n\n\nWhile not every correlation coefficient is above 0.3, a good number of them are. We could remove variables that have correlation coefficients below 0.3, but let’s keep going with all variables.\nAnother way to test for multicollinearity is with a Barlett test, available in the psych R package. The null hypothesis is that the variables are not correlated, so a p-value below 0.05 would indicate that we can reject the null hypothesis of no correlation between the variables.\n\nreg_bnc %&gt;% \n  select(-Reg) %&gt;% # keep only continuous data types\n  psych::cortest.bartlett()\n\nR was not square, finding R from data\n\n\n$chisq\n[1] 536.3401\n\n$p.value\n[1] 4.109611e-80\n\n$df\n[1] 55\n\n\nBecause the p-value is below the alpha level of 0.05, we reject the null hypothesis that there is no correlation between the variables. Let’s keep going!\nNow, let’s run a Principal Components Analysis with the PCA function in the FactoMineR package. We give the PCA function the quali.sup = 1 argument to tell it that the first column (i.e., 1) is a qualitative (aka. categorical) variable with info about each observation.\n\nlibrary(\"FactoMineR\")\npca1 &lt;- PCA(reg_bnc, quali.sup = 1)\n\n\n\n\n\n\n\nBy default, the PCA function draws some plots for us. The “PCA graph of individuals” plots the 69 observations in a two-dimensional space with the first principal component (aka. dimension) on the x-axis labeled with “Dim 1”, and the second principal component on the y-axis. And the “PCA graph of variables” plots the 11 variables on the same two-dimensional space. However, this second plot also has a circle and arrows (aka. vectors). The angles between the arrows and the dashed axes indicate how strongly the variable is correlated with each of the first two principal components, such that, the smaller the angle, the stronger the correlation. For example, “P2” (i.e., second person pronouns) is strongly correlated with the first principal component (aka. loads onto the first principal component), while “past” is not closely correlated with the first principal component. Differently, “past” is strongly correlated with the second principal component (aka. second dimension) because its vector is very close to the dashed line representing that component, while “P2” is not correlated with that component. Two vectors that are close to each are strongly correlated, and may represent the same underlying theoretical construct (whatever that might be). The length of a vector represents how much variation in the variable is accounted for by these two principal components, with a max of 1 (i.e., the circle).\nLet’s draw a better plot of the individuals:\n\nplot(pca1, cex = 0.8, col.ind=\"gray\", col.quali=\"black\")\n\n\n\n\nThe eigenvalues indicate how much of the variance is explained by each principal component. (BTW, there are as many principal components as there are variable that go into the PCA.) Let’s look at the eigenvalues and the percentage of variance explained by each PC:\n\npca1$eig\n\n        eigenvalue percentage of variance cumulative percentage of variance\ncomp 1  5.06829360             46.0753964                          46.07540\ncomp 2  1.87221031             17.0200937                          63.09549\ncomp 3  1.37584355             12.5076686                          75.60316\ncomp 4  0.79007568              7.1825062                          82.78566\ncomp 5  0.64512706              5.8647914                          88.65046\ncomp 6  0.42171443              3.8337675                          92.48422\ncomp 7  0.30027045              2.7297314                          95.21396\ncomp 8  0.18666203              1.6969276                          96.91088\ncomp 9  0.14370361              1.3063965                          98.21728\ncomp 10 0.10877509              0.9888644                          99.20614\ncomp 11 0.08732419              0.7938563                         100.00000\n\n\nLevsina (2015, p. 355) mentions two rules of thumb about how many PCs to keep. The Kaiser Criterion proposes that PCs whose eigenvalue are greater than 1 be kept, while another rule of thumb is less strict, with a threshold of 0.7. Others take a percentage approach and propose that the first N number of PCs that explain M% of the variance be kept, for example, 80% or 90% of the variance.\nA scree plot is simply a barplot with the amount of variance explained by each PC. Here’s a base R way to draw one:\n\nbarplot(pca1$eig[,2], names = 1:nrow(pca1$eig), xlab = \"Components\", ylab = \"Percentage of variance explained\")\n\n\n\n\nAnd here’s a way with the factoextra package:\n\nlibrary(\"factoextra\")\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nfviz_screeplot(pca1, addlabels = T)\n\n\n\n\nNow, let’s get the contribution of each variable to the first five PCs:\n\nsuppressPackageStartupMessages(library(\"corrplot\"))\nvar &lt;- get_pca_var(pca1)\ncorrplot(var$cos2)\n\n\n\n\nHere’s another view of each PC:\n\nfor (i in 1:5) {\n  plot(fviz_contrib(pca1, choice = \"var\", axes = i))\n}"
  },
  {
    "objectID": "lessons/logistic_regression.html",
    "href": "lessons/logistic_regression.html",
    "title": "Logistic regression",
    "section": "",
    "text": "Students will fit a logistic regression model."
  },
  {
    "objectID": "lessons/logistic_regression.html#objective",
    "href": "lessons/logistic_regression.html#objective",
    "title": "Logistic regression",
    "section": "",
    "text": "Students will fit a logistic regression model."
  },
  {
    "objectID": "lessons/logistic_regression.html#background",
    "href": "lessons/logistic_regression.html#background",
    "title": "Logistic regression",
    "section": "Background",
    "text": "Background\nWhen your response variable is categorical with exactly two levels or groups (aka. a binary response variable), logistic regression is your go-to regression analysis."
  },
  {
    "objectID": "lessons/logistic_regression.html#assumptions",
    "href": "lessons/logistic_regression.html#assumptions",
    "title": "Logistic regression",
    "section": "Assumptions",
    "text": "Assumptions\n\nThe observations are independent of each other. If not, mixed-effect logistic regression (aka. mixed-effects generalized linear regression) is what you should use with random effects to account for the hierarchical nature of the data.\nThe continuous explanatory variables are (generally) linear.\nThere’s no multicollinearity between the explanatory variables."
  },
  {
    "objectID": "lessons/logistic_regression.html#how-many-observations",
    "href": "lessons/logistic_regression.html#how-many-observations",
    "title": "Logistic regression",
    "section": "How many observations",
    "text": "How many observations\nLevshina (2015, p. 257) mentions different rules of thumb for a minimum number of observations in order to fit a logistic regression. One is that the number of explanatory variables should not be larger than a tenth of the number of observations. So, we only really need to worry about this when we have few tokens and/or many explanatory variables."
  },
  {
    "objectID": "lessons/logistic_regression.html#terms",
    "href": "lessons/logistic_regression.html#terms",
    "title": "Logistic regression",
    "section": "Terms",
    "text": "Terms\nHere are some terms about come up a lot in the context of logistic regression (see Levshina 2015, p. 261).\n\nodds: simple division of one number divided by another number. For example, in the doenLaten dataset in the Rling package, there are 178 tokens (aka. observations) of doen and 277 tokens of laten, so the odds of doen to laten is \\(178/277≃0.64\\).\nprobability: one number divided by the total N, expressed as a proportion (between 0 and 1) or as a percentage (between 0% and 100%). For example, the probability of doen is \\(178/455≃0.39\\) as a proportion, or \\(39\\%\\) as a percentage.\nlog odds (aka. logit): this is simply the natural logarithm of odds above, which we can calculate with the log() function. The opposite of log() is the exponent, which is calculated with exp().\nodds ratio: the simple ratio of two odds.\nlog odds ratio: the natural logarithm of odds ratio."
  },
  {
    "objectID": "lessons/logistic_regression.html#dataset",
    "href": "lessons/logistic_regression.html#dataset",
    "title": "Logistic regression",
    "section": "Dataset",
    "text": "Dataset\nLet’s get the Dutch causative auxiliaries dataset with doen and laten (similar to English make/have/get/cause X (to) do Y), presented in Levshina (2015) chapter 12. First, let’s download two packages, one with the dataset and another with a function for logistic regression.\n\ninstall.packages(\"Rling\", repos = \"http://rstudio.org\")\ninstall.packages(\"rms\", repos = \"http://rstudio.org\")\n\nNow, let’s load in the dataset:\n\nlibrary(\"Rling\")\ndata(\"doenLaten\")\n\nLet’s look at the documentation for the dataset:\n\n# at the console\n?doenLaten\n\nThere are two (popular) functions to fit logistic regression. Here we go!"
  },
  {
    "objectID": "lessons/logistic_regression.html#functions",
    "href": "lessons/logistic_regression.html#functions",
    "title": "Logistic regression",
    "section": "Functions",
    "text": "Functions\n\nglm() in base R\nThe base R function glm() is a Swiss-army knife function which can do logistic regression if you specify family = binomial in the call. (BTW, It can also do linear regression with family = gaussian and other types of regression. See the docs: ?glm)\nLet’s fit a logistic regression with the doenLaten dataset, with Aux as the response variable (which has two levels: doen and laten), and three categorical explanatory variables:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nm1 &lt;- glm(Aux ~ Causation + EPTrans + Country, data = doenLaten, family = binomial)\nsummary(m1)\n\n\nCall:\nglm(formula = Aux ~ Causation + EPTrans + Country, family = binomial, \n    data = doenLaten)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)           1.8631     0.3771   4.941 7.79e-07 ***\nCausationInducive    -3.3725     0.3741  -9.015  &lt; 2e-16 ***\nCausationPhysical     0.4661     0.6275   0.743 0.457575    \nCausationVolitional  -3.7373     0.4278  -8.735  &lt; 2e-16 ***\nEPTransTr            -1.2952     0.3394  -3.816 0.000136 ***\nCountryBE             0.7085     0.2841   2.494 0.012633 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 609.05  on 454  degrees of freedom\nResidual deviance: 337.70  on 449  degrees of freedom\nAIC: 349.7\n\nNumber of Fisher Scoring iterations: 5\n\n\nFirst, the intercept estimate, which is given as the log odds of the second level of the response variable (i.e., doen) in comparison to the reference level (i.e., laten), when the categorical explanatory variables have their reference level and when continuous explanatory variables are zero. So, in this case, the log odds that doen is used is \\(1.8631\\) when the Causation is Affective, the EPTrans (i.e., effected predicate) is Intr (i.e., intransitive), and Country is NL (i.e., Netherlands).\nIf we remember that the opposite of the natural logarithm is the exponent, we can get the odds with exp(1.8631), which gives us \\(≃6.44\\). So, doen is 6.44 times more likely than laten with affective causation and intransitive effected predicates in the Netherlands.\nIf we’re not sure which is the second level and which is the reference level (first listed), we can use:\n\nlevels(doenLaten$Aux)\n\n[1] \"laten\" \"doen\" \n\n\nLet’s turn to the coefficients for the three categorical explanatory variables. The reference level of each isn’t listed, and the other levels that are listed are compared to that unlisted reference level. This is called treatments contrasts (like with linear regression). The coefficients below the Estimate column are log odds ratio. So, for example, the log odds ratio for doen in comparison to laten in Belgium is \\(0.7085\\). If we take the exponent of this number:\n\nexp(0.7085)\n\n[1] 2.030943\n\n\nwe end up with \\(2.030943\\). So, the likelihood of doen instead of laten in Belguim is more than 2 times higher than in the Netherlands when all other variables are held constant.\nLooking at p-values as well as the polarity (i.e., positive or negative signs) of the Causation coefficients, there is no significant difference (p-value = 0.457575) between the effect of Physical and Affective on doen and laten usage. Differently, Inducive and Volitional cause for less doen than Affective, given their negative coefficients and p-values below 0.05.\n\n\nlrm() in rms package\nAnother function to perform logistic regression is the lrm() in the rms package. It gives more results than the glm() function in base R. Let’s go!\n\nsuppressPackageStartupMessages(library(rms))\nm2 &lt;- lrm(Aux ~ Causation + EPTrans + Country, data = doenLaten)\nm2 # no need to call summary()\n\nLogistic Regression Model\n\nlrm(formula = Aux ~ Causation + EPTrans + Country, data = doenLaten)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs           455    LR chi2     271.35      R2       0.609    C       0.894    \n laten        277    d.f.             5      R2(5,455)0.443    Dxy     0.787    \n doen         178    Pr(&gt; chi2) &lt;0.0001    R2(5,325.1)0.559    gamma   0.817    \nmax |deriv| 1e-07                            Brier    0.112    tau-a   0.376    \n\n                     Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept             1.8631 0.3771  4.94  &lt;0.0001 \nCausation=Inducive   -3.3725 0.3741 -9.01  &lt;0.0001 \nCausation=Physical    0.4661 0.6275  0.74  0.4576  \nCausation=Volitional -3.7373 0.4278 -8.74  &lt;0.0001 \nEPTrans=Tr           -1.2952 0.3394 -3.82  0.0001  \nCountry=BE            0.7085 0.2841  2.49  0.0126  \n\n\nThe only thing Dr. Brown wants to point out is a goodness-of-fit statistic under Rank Discrimination Index: the concordance index C. In this case, we get a C value of \\(0.894\\). Levshina (2015, p. 259) gives the following table to know how to interpret C values:\n\nconcordance index\n\n\nConcordance index\nDescriptor\n\n\n\n\nC = 0.5\nno discrimination\n\n\n0.7 ≤ C &lt; 0.8\nacceptable discrimination\n\n\n0.8 ≤ C &lt; 0.9\nexcellent discrimination\n\n\nC ≥ 0.9\noutstanding discrimination\n\n\n\nSo, this logistic regression model has “excellent discrimination” (see this Q&A thread about goodness-of-fit metrics)."
  },
  {
    "objectID": "lessons/logistic_regression.html#activity",
    "href": "lessons/logistic_regression.html#activity",
    "title": "Logistic regression",
    "section": "Activity",
    "text": "Activity\nLoad up the regularity dataset in the languageR package:\n\nlibrary(\"languageR\")\ndata(\"regularity\")\n?regularity\n\nThe dataset has 13 columns, including Regularity, which indicates whether the verb is regular or irregular. Using that column as the response variable, fit a logistic regression model with the explanatory variables that you feel might affect whether a verb is regular or irregular.\nAfter a good-faith effort, if you need some help, take a look at Dr. Brown’s code below:\n\n\nCode\nlibrary(\"rms\")\nm3 &lt;- lrm(Regularity ~ WrittenFrequency + Auxiliary + LengthInLetters + FamilySize, data = regularity)\nm3"
  },
  {
    "objectID": "lessons/mixed-effects-linear.html",
    "href": "lessons/mixed-effects-linear.html",
    "title": "Mixed-effects linear regression",
    "section": "",
    "text": "Random effects\n“Random effects” in a linear regression model control for the variability attributable to the specific members of a population that happened to be selected for our random sample. To take an example, let’s say we have a dataset of 2,500 tokens of word durations that were extracted from the speech of six speakers. Each speaker contributed several hundreds tokens of word durations. A handful of predictor variables have been shown in the literature to affect word durations:\n\nword length: longer words take longer to articulate\nfrequency: more frequent words are generally spoken quicker\nspeech rate: when a speaker is speaking quickly at a given point in a conversation, their words will have shorter durations\nposition near the end of the utterance: speech rate at the end of utterances usually slows down\nprevious mention: words previously spoken in a conversation are usually spoken quicker than first mentions\n\nIn addition to those predictor variables, we should also control for that fact that not all speakers talk at the same speed. Some speakers talk faster than others, and some speakers are painfully slow with their speech. It could be the case that Bobby is simply a slow speaker, while Luisa is a fast talker, and as such, Bobby’s word durations are longer overall while Luisa’s are shorter when compared to the other speakers in the dataset.\nTo summarize, random effects help us control for the variability in the response variable that is attributable to the individuals sampled from the population, before measuring the effect, if any, from the explanatory variables in the model.\nThe non-random-effect explanatory variables are called “fixed effects”.\n\n\nWord durations\nLet’s look at a sample of word durations from the Buckeye Corpus of English. The “Word_dur_Buckeye.csv” file in the CMS has all word durations of content words spoken by six speakers (from among the total 40 speakers in the corpus). The dataset has the following columns:\n\nfile: the filename that acts as an ID of the speaker\nwd: orthographic representation of the word\nwd_dur: duration of the word in seconds\nsegments: the sound segments actually pronounced (phonetic representation, not phonemic)\nn_segments: word length as the number of segments pronounced\nn_syl: word length as the number of syllables\npos: part-of-speech tag\nspch_rate: speech rate in segments per second of the part of the utterance from after the word to the end of the utterance\nwd_freq: word frequency from the OpenSubtitles English data dump (only 1991-2000) as a Laplace smoothed numbers (see here)\ndist_end_iu: the distance in number of words that the word falls from the end of the utterance\npre_mention: whether the word was previously mentioned in the conversation (“yes” or “no”)\n\n\n\nLoad dataset\nLet’s load the dataset and print the first six rows (the default quantity of head()).\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nbuck &lt;- read_csv(\"/Users/ekb5/Documents/data_analysis/datasets/Word_dur_Buckeye.csv\")\n\nRows: 2537 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): file, wd, segments, pos, pre_mention\ndbl (6): wd_dur, n_segments, n_syl, spch_rate, wd_freq, dist_end_iu\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(buck)\n\n# A tibble: 6 × 11\n  file  wd       wd_dur segments        n_segments n_syl pos   spch_rate wd_freq\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 s01   show      0.195 ['sh', 'ow']             2     1 VBP       14.2  4.09e-4\n2 s06   everyone  0.418 ['eh', 'v', 'r…          7     3 NN        13.2  2.21e-4\n3 s01   is        0.103 ['uh', 'z']              2     1 VBZ       13.9  8.58e-3\n4 s01   national  0.394 ['n', 'ae', 's…          5     2 JJ        14.5  3.99e-5\n5 s01   catholic  0.402 ['k', 'ae', 't…          7     3 JJ         9.61 1.22e-5\n6 s06   man       0.229 ['m', 'ae', 'n…          3     1 NN         6.48 1.70e-3\n# ℹ 2 more variables: dist_end_iu &lt;dbl&gt;, pre_mention &lt;chr&gt;\n\n\nLet control for capitalization of words by uppercasing (v.) the words, and then printing the first six rows.\n\nbuck &lt;- buck %&gt;% \n  mutate(wd = str_to_upper(wd))\nhead(buck)\n\n# A tibble: 6 × 11\n  file  wd       wd_dur segments        n_segments n_syl pos   spch_rate wd_freq\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 s01   SHOW      0.195 ['sh', 'ow']             2     1 VBP       14.2  4.09e-4\n2 s06   EVERYONE  0.418 ['eh', 'v', 'r…          7     3 NN        13.2  2.21e-4\n3 s01   IS        0.103 ['uh', 'z']              2     1 VBZ       13.9  8.58e-3\n4 s01   NATIONAL  0.394 ['n', 'ae', 's…          5     2 JJ        14.5  3.99e-5\n5 s01   CATHOLIC  0.402 ['k', 'ae', 't…          7     3 JJ         9.61 1.22e-5\n6 s06   MAN       0.229 ['m', 'ae', 'n…          3     1 NN         6.48 1.70e-3\n# ℹ 2 more variables: dist_end_iu &lt;dbl&gt;, pre_mention &lt;chr&gt;\n\n\n\n\nWith random intercepts\nLet’s fit a linear regression with random intercepts for speaker and for word, that is, a unique intercept for each speaker and a unique intercept for each word.\n\nm1 &lt;- lmerTest::lmer(wd_dur ~ n_segments + spch_rate + wd_freq + dist_end_iu + pre_mention + (1 | file) + (1 | wd), data = buck)\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\nsummary(m1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nwd_dur ~ n_segments + spch_rate + wd_freq + dist_end_iu + pre_mention +  \n    (1 | file) + (1 | wd)\n   Data: buck\n\nREML criterion at convergence: -5656.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2602 -0.6204 -0.1445  0.4388  5.1113 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n wd       (Intercept) 0.0022806 0.04776 \n file     (Intercept) 0.0004753 0.02180 \n Residual             0.0050666 0.07118 \nNumber of obs: 2537, groups:  wd, 628; file, 6\n\nFixed effects:\n                 Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)     1.830e-01  1.403e-02  2.813e+01  13.042 1.90e-13 ***\nn_segments      4.944e-02  1.821e-03  9.637e+02  27.148  &lt; 2e-16 ***\nspch_rate      -4.570e-03  5.137e-04  2.337e+03  -8.896  &lt; 2e-16 ***\nwd_freq        -1.404e+01  2.671e+00  2.150e+02  -5.256 3.55e-07 ***\ndist_end_iu     2.182e-04  2.973e-04  2.323e+03   0.734    0.463    \npre_mentionyes -6.215e-03  4.574e-03  2.450e+03  -1.359    0.174    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) n_sgmn spch_r wd_frq dst_n_\nn_segments  -0.574                            \nspch_rate   -0.419  0.020                     \nwd_freq     -0.249  0.330  0.000              \ndist_end_iu -0.020  0.012 -0.254 -0.015       \npre_mentnys -0.209  0.049 -0.007 -0.177 -0.009\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n\n\nWe get a warning message stating that the scales of some of the continuous predictor variables are on very different scales and that we should consider rescaling. Let’s take a look at summary information about all columns.\n\nbuck %&gt;% \n  purrr::map(summary)\n\n$file\n   Length     Class      Mode \n     2537 character character \n\n$wd\n   Length     Class      Mode \n     2537 character character \n\n$wd_dur\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0280  0.1700  0.2333  0.2563  0.3208  0.7480 \n\n$segments\n   Length     Class      Mode \n     2537 character character \n\n$n_segments\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   3.000   3.000   3.432   4.000  10.000 \n\n$n_syl\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.304   2.000   5.000 \n\n$pos\n   Length     Class      Mode \n     2537 character character \n\n$spch_rate\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.841  10.557  12.567  12.530  14.484  33.333 \n\n$wd_freq\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n2.600e-08 2.149e-04 8.373e-04 1.778e-03 3.298e-03 8.583e-03 \n\n$dist_end_iu\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   3.000   5.000   6.339   8.000  45.000 \n\n$pre_mention\n   Length     Class      Mode \n     2537 character character \n\n\nIn general, it’s a good idea to use z-scores of continuous variables by using scale(). It comes at the cost of losing some of the interpretability, as all we can really say is whether there is a significant effect, and if so, in what direction. Let’s scale all the continuous variables and rerun the regression.\n\nm2 &lt;- lmerTest::lmer(scale(wd_dur) ~ scale(n_segments) + scale(spch_rate) + scale(wd_freq) + scale(dist_end_iu) + pre_mention + (1 | file) + (1 | wd), data = buck)\nsummary(m2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nscale(wd_dur) ~ scale(n_segments) + scale(spch_rate) + scale(wd_freq) +  \n    scale(dist_end_iu) + pre_mention + (1 | file) + (1 | wd)\n   Data: buck\n\nREML criterion at convergence: 5182\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2602 -0.6204 -0.1445  0.4388  5.1113 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n wd       (Intercept) 0.16474  0.4059  \n file     (Intercept) 0.03434  0.1853  \n Residual             0.36598  0.6050  \nNumber of obs: 2537, groups:  wd, 628; file, 6\n\nFixed effects:\n                     Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)         1.326e-01  8.828e-02  8.504e+00   1.502    0.169    \nscale(n_segments)   5.607e-01  2.065e-02  9.637e+02  27.148  &lt; 2e-16 ***\nscale(spch_rate)   -1.235e-01  1.388e-02  2.337e+03  -8.896  &lt; 2e-16 ***\nscale(wd_freq)     -2.411e-01  4.587e-02  2.150e+02  -5.256 3.55e-07 ***\nscale(dist_end_iu)  9.854e-03  1.343e-02  2.323e+03   0.734    0.463    \npre_mentionyes     -5.282e-02  3.887e-02  2.450e+03  -1.359    0.174    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) scl(n_) scl(s_) scl(w_) sc(__)\nscl(n_sgmn) -0.008                               \nscl(spch_r)  0.019  0.020                        \nscl(wd_frq)  0.316  0.330   0.000                \nscl(dst_n_) -0.003  0.012  -0.254  -0.015        \npre_mentnys -0.340  0.049  -0.007  -0.177  -0.009\n\n\nLet’s look at how much variability in the response variable (i.e., wd_dur) is accounted for by both the random effects and the fixed effects (i.e., conditional \\(R^{2}\\)) and how much is accounted for by only the fixed effects (i.e., mariginal \\(R^{2}\\)) with performance::r2_nakagawa().\n\n# install.packages(\"performance\")  # if needed, run this line first\nperformance::r2_nakagawa(m2)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.670\n     Marginal R2: 0.490\n\n\n\n\nWithout random intercepts\nFor comparison purposes, let’s fit a linear regression model without any random effects. Compare the \\(R^{2}\\) in this model with the marginal \\(R^{2}\\) in the mixed-effects model above.\n\nm3 &lt;- lm(scale(wd_dur) ~ scale(n_segments) + scale(spch_rate) + scale(wd_freq) + scale(dist_end_iu) + pre_mention, data = buck)\nsummary(m3)\n\n\nCall:\nlm(formula = scale(wd_dur) ~ scale(n_segments) + scale(spch_rate) + \n    scale(wd_freq) + scale(dist_end_iu) + pre_mention, data = buck)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7399 -0.4904 -0.1134  0.3680  4.1878 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         0.14341    0.03482   4.118 3.94e-05 ***\nscale(n_segments)   0.54739    0.01668  32.814  &lt; 2e-16 ***\nscale(spch_rate)   -0.15918    0.01488 -10.696  &lt; 2e-16 ***\nscale(wd_freq)     -0.16332    0.01716  -9.517  &lt; 2e-16 ***\nscale(dist_end_iu)  0.01454    0.01488   0.977    0.329    \npre_mentionyes     -0.17653    0.03909  -4.516 6.59e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7198 on 2531 degrees of freedom\nMultiple R-squared:  0.4829,    Adjusted R-squared:  0.4818 \nF-statistic: 472.7 on 5 and 2531 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nRandom slopes and intercepts\nIn addition to allowing for different intercepts for the specific individuals selected from the population (e.g., speakers, word), you can also allow a linear model to fit different slopes for the individuals. Let’s rerun our mixed-effects model from above (i.e., ‘m2’), but this time with random slopes on a fixed effect variable of our interest.\n\nm4 &lt;- lmerTest::lmer(scale(wd_dur) ~ scale(n_segments) + scale(spch_rate) + scale(wd_freq) + scale(dist_end_iu) + pre_mention + (1 + n_segments | file) + (1 + n_segments | wd), data = buck)\n\nboundary (singular) fit: see help('isSingular')\n\n\nWarning: Model failed to converge with 2 negative eigenvalues: -3.4e+00\n-2.1e+02\n\nsummary(m4)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nscale(wd_dur) ~ scale(n_segments) + scale(spch_rate) + scale(wd_freq) +  \n    scale(dist_end_iu) + pre_mention + (1 + n_segments | file) +  \n    (1 + n_segments | wd)\n   Data: buck\n\nREML criterion at convergence: 5166.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2941 -0.6246 -0.1515  0.4442  5.2959 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n wd       (Intercept) 0.000000 0.00000      \n          n_segments  0.010776 0.10381   NaN\n file     (Intercept) 0.000000 0.00000      \n          n_segments  0.003044 0.05517   NaN\n Residual             0.363106 0.60258      \nNumber of obs: 2537, groups:  wd, 628; file, 6\n\nFixed effects:\n                     Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)           0.15589    0.08631    7.08067   1.806    0.113    \nscale(n_segments)     0.59451    0.03813   11.77687  15.592 3.19e-09 ***\nscale(spch_rate)     -0.11936    0.01376 2329.14824  -8.676  &lt; 2e-16 ***\nscale(wd_freq)       -0.21256    0.03101   84.67876  -6.855 1.08e-09 ***\nscale(dist_end_iu)    0.01004    0.01333 2302.09156   0.753    0.451    \npre_mentionyes       -0.06194    0.03869 2472.54020  -1.601    0.110    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) scl(n_) scl(s_) scl(w_) sc(__)\nscl(n_sgmn)  0.743                               \nscl(spch_r)  0.019  0.026                        \nscl(wd_frq)  0.195  0.274  -0.003                \nscl(dst_n_) -0.004  0.002  -0.256  -0.030        \npre_mentnys -0.336  0.050  -0.014  -0.184  -0.005\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nperformance::r2_nakagawa(m4)\n\nWarning: Can't compute random effect variances. Some variance components equal\n  zero. Your model may suffer from singularity (see `?lme4::isSingular`\n  and `?performance::check_singularity`).\n  Solution: Respecify random structure! You may also decrease the\n  `tolerance` level to enforce the calculation of random effect variances.\n\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.607\n\n\nWe get a warning message stating that our model suffers from singularity. This is often the case when one of our random effects has at least one level that only has only one token. Let’s see if we can resolve this warning by removing the random slope on word, but keeping in the random intercept.\n\nm4 &lt;- lmerTest::lmer(scale(wd_dur) ~ scale(n_segments) + scale(spch_rate) + scale(wd_freq) + scale(dist_end_iu) + pre_mention + (1 + n_segments | file) + (1 | wd), data = buck)\nsummary(m4)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nscale(wd_dur) ~ scale(n_segments) + scale(spch_rate) + scale(wd_freq) +  \n    scale(dist_end_iu) + pre_mention + (1 + n_segments | file) +      (1 | wd)\n   Data: buck\n\nREML criterion at convergence: 5157.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.3273 -0.6248 -0.1617  0.4528  5.2103 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr \n wd       (Intercept) 0.1606510 0.40081       \n file     (Intercept) 0.0002832 0.01683       \n          n_segments  0.0034436 0.05868  -1.00\n Residual             0.3630763 0.60256       \nNumber of obs: 2537, groups:  wd, 628; file, 6\n\nFixed effects:\n                     Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)         1.362e-01  8.782e-02  8.474e+00   1.551    0.157    \nscale(n_segments)   5.822e-01  3.800e-02  7.157e+00  15.321 9.85e-07 ***\nscale(spch_rate)   -1.224e-01  1.373e-02  2.286e+03  -8.919  &lt; 2e-16 ***\nscale(wd_freq)     -2.429e-01  4.537e-02  2.128e+02  -5.353 2.23e-07 ***\nscale(dist_end_iu)  8.919e-03  1.335e-02  2.323e+03   0.668    0.504    \npre_mentionyes     -5.827e-02  3.861e-02  2.442e+03  -1.509    0.131    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) scl(n_) scl(s_) scl(w_) sc(__)\nscl(n_sgmn)  0.721                               \nscl(spch_r)  0.019  0.023                        \nscl(wd_frq)  0.314  0.178   0.000                \nscl(dst_n_) -0.002  0.003  -0.256  -0.014        \npre_mentnys -0.342  0.030  -0.015  -0.177  -0.008\n\nperformance::r2_nakagawa(m4)\n\nboundary (singular) fit: see help('isSingular')\n\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.670\n     Marginal R2: 0.523\n\n\nSure enough, that resolved the singularity warning message. As you can now see, by adding random slopes to the speakers, our marginal \\(R^{2}\\) value increases, indicating that allowing the slopes to vary by speaker allows the fixed effect n_segment to better fit the data point. Here’s a good explanation of how to understand random slopes. Takehome message: It’s best to specify both random intercepts and slopes as they allow the model to better fit the data. However, it can often happen that you get a singularity warning with a random slope that has few tokens, and in that case, keeping only the random intercept is necessary.\n\n\nActivity\nPurpose: Students gain experience with fitting and interpreting a mixed-effects linear regression.\nResearch question: What explanatory variables significantly predict voice onset time in the “Data_ptk.xlsx” dataset?\nInstructions: Students fit a mixed-effects linear regression with voice onset time (column name: VOT) as the response variable and a handful of fixed-effect explanatory variables, for example, L1 background (noted between the two underscores in the FILE), and language of production (LANG). Also, and importantly, students two random effects, one for speaker (column name: FILE) and one for word (column name: wdLOWER). It will likely be helpful to create some visualizations to explore the effect of the fixed-effect explanatory variables on the response variable.\nWrite-up: Students write a paragraph in which they interpret the results of their model and their visualizations for their layperson reader."
  },
  {
    "objectID": "lessons/r_rstudio.html",
    "href": "lessons/r_rstudio.html",
    "title": "R and RStudio",
    "section": "",
    "text": "Students will start to become familiar with the R programming language and the RStudio Integrated Development Environment (IDE)"
  },
  {
    "objectID": "lessons/r_rstudio.html#objective",
    "href": "lessons/r_rstudio.html#objective",
    "title": "R and RStudio",
    "section": "",
    "text": "Students will start to become familiar with the R programming language and the RStudio Integrated Development Environment (IDE)"
  },
  {
    "objectID": "lessons/r_rstudio.html#the-r-programming-language",
    "href": "lessons/r_rstudio.html#the-r-programming-language",
    "title": "R and RStudio",
    "section": "The R programming language",
    "text": "The R programming language\n\nR is a programming language specifically designed for statistical analysis and visualization.\nAs an open-source language, there are many third-party add-on packages that extend the use of R that are available on the Comprehensive R Archive Network (aka. CRAN).\nSo called “Task Views” collect and briefly describe packages related to specific fields, including one for Natural Language Processing."
  },
  {
    "objectID": "lessons/r_rstudio.html#rstudio-ide",
    "href": "lessons/r_rstudio.html#rstudio-ide",
    "title": "R and RStudio",
    "section": "RStudio IDE",
    "text": "RStudio IDE\n\nIntegrated Development Environments (IDEs) facilitate writing computer code.\nRStudio from the company Posit is the go-to IDE for R.\n\nHas many useful keyboard shortcuts, cf. Help &gt; Keyboard Shortcuts Help\n\nEarl’s favorites are:\n\nAssignment operator: ALT + -\nPipe operator in tidyverse: CTRL/CMD + SHIFT + m\n\n\nHas many cheat sheets for various tools within R and RStudio, cf. Help &gt; Cheat Sheets"
  },
  {
    "objectID": "lessons/r_rstudio.html#r-projects",
    "href": "lessons/r_rstudio.html#r-projects",
    "title": "R and RStudio",
    "section": "R projects",
    "text": "R projects\n\nR projects help keep things organized:\n\nData files, like CSV (.csv) and/or Excel (.xlsx) files\nSource code files, like R scripts (.r or .R files)\n\nTo create an R project:\n\nFile &gt; New Project..."
  },
  {
    "objectID": "lessons/r_rstudio.html#activity",
    "href": "lessons/r_rstudio.html#activity",
    "title": "R and RStudio",
    "section": "Activity",
    "text": "Activity\n\nStudents explore R and RStudio IDE, perhaps using the RStudio cheat sheet (Help &gt; Cheat Sheets &gt; RStudio IDE Cheat Sheet)"
  },
  {
    "objectID": "lessons/regression_assumptions.html",
    "href": "lessons/regression_assumptions.html",
    "title": "Regression assumptions",
    "section": "",
    "text": "Students will check three assumptions of regression analysis."
  },
  {
    "objectID": "lessons/regression_assumptions.html#objective",
    "href": "lessons/regression_assumptions.html#objective",
    "title": "Regression assumptions",
    "section": "",
    "text": "Students will check three assumptions of regression analysis."
  },
  {
    "objectID": "lessons/regression_assumptions.html#assumptions",
    "href": "lessons/regression_assumptions.html#assumptions",
    "title": "Regression assumptions",
    "section": "Assumptions",
    "text": "Assumptions\nAs mentioned in our lesson on linear regression, Levshina (2015, ch. 7) presents seven assumptions of linear regression. In this lesson, we’ll look at three assumptions in detail:\n\nThere shouldn’t be any multicollinearity of explanatory variables.\nThe residuals should be normally distributed, with a mean of zero.\nThe residuals of the model should vary constantly."
  },
  {
    "objectID": "lessons/regression_assumptions.html#no-multicollinearity-of-explanatory-variables",
    "href": "lessons/regression_assumptions.html#no-multicollinearity-of-explanatory-variables",
    "title": "Regression assumptions",
    "section": "No multicollinearity of explanatory variables",
    "text": "No multicollinearity of explanatory variables\nMulticollinearity refers to when two or more explanatory variables are correlated with each other. What that means is whether one explanatory variable can be predicted by one or more other variables. Multicollinearity is a problem for regression analysis and should be addressed. Fixing that problem may include removing one or more correlated variables, or using a multidimensional reduction algorithm like Principal Components Analysis. That’s a topic for another lesson.\nGetting back to simply detecting the level of multicollinearity, if any, that is present among explanatory variables, as presented in the previous lesson, we can use Variance Inflation Factors (VIF). Different rules of thumb exist about the threshold below which VIFs should be in order to trust the results of a regression model. Levshina (2015, p. 160) mentions that some researchers propose 10 as the threshold, while other researchers propose a stricter threshold of 5. Winter (2019, p. 114) mentions that he has used 3 or 4 in previous studies as the threshold.\nLet’s load up the dataset created by Dr. Brown to write this article about the effect of cumulative exposure to fast speech on the duration of words in English, as seen in the Buckeye Corpus. First, let’s make sure we have the third-party R packages that we’ll need:\n\ninstall.packages(c(\"readxl\", \"car\", \"lme4\", \"lmerTest\", \"performance\", \"Matrix\", \"moments\", \"MASS\"), repos = \"http://cran.rstudio.com/\")\n\nNow, download the “dataset_FRC_Buckeye.xlsx” from the LMS and load it up into our R session:\n\nlibrary(\"readxl\")\nbuck &lt;- read_excel(\"/Users/ekb5/Documents/LING_440/datasets/dataset_FRC_Buckeye.xlsx\", sheet = \"dataset\")\n\nNow, let’s fit a mixed-effects linear regression:\n\nlibrary(\"lmerTest\")\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nm1 &lt;- lmerTest::lmer(\n    scale(wd_dur) ~  # use the raw word durations\n      scale(n_phon)+\n      scale(log(wd_freq))+\n      scale(spch_rate)+  \n      scale(frc)+\n      scale(frc):scale(log(wd_freq))+  # interaction term\n      scale(forw_predict)+\n      pre_mention+\n      scale(dist_end_iu)+\n      scale(iu_len_wds)+\n      scale(back_predict)+\n      scale(frc):scale(wd_freq)+  # interaction term\n      (1 + frc | file) +  \n      (1 | wd_upper),\n    control = lmerControl(optimizer =\"optimx\", calc.derivs = F, optCtrl = list(method = \"nlminb\", starttests = F, kkt = F)),\n    data = buck)\n\nLoading required namespace: optimx\n\n\nNow, let’s check the level of multicollinearity of the explanatory variables. First, we’ll need to remove the interaction terms in the model and then ask for the VIFs:\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::expand() masks Matrix::expand()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tidyr::pack()   masks Matrix::pack()\n✖ tidyr::unpack() masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nm1 %&gt;% \n  # remove interaction terms before getting VIFs\n  update(~. - scale(log(wd_freq)):scale(frc)) %&gt;%\n  update(~. - scale(frc):scale(wd_freq)) %&gt;% \n  # calculate VIFs\n  car::vif() \n\n      scale(n_phon) scale(log(wd_freq))    scale(spch_rate)          scale(frc) \n           1.100819            1.151842            1.071580            1.020065 \nscale(forw_predict)         pre_mention  scale(dist_end_iu)   scale(iu_len_wds) \n           1.076477            1.004757            1.984318            1.939911 \nscale(back_predict) \n           1.020173 \n\n\nMarvelous! All the VIFs are below the thresholds mentioned above. If this were not the case, we’d need to inspect the explanatory variables in details to figure out if we would be justified in removing the variable with the highest VIF, or somehow combining it with another variable. If that’s not possible, we might look into Principal Components Analysis."
  },
  {
    "objectID": "lessons/regression_assumptions.html#residuals-should-be-normally-distributed",
    "href": "lessons/regression_assumptions.html#residuals-should-be-normally-distributed",
    "title": "Regression assumptions",
    "section": "Residuals should be normally distributed",
    "text": "Residuals should be normally distributed\nThe next assumption of regression analysis that we’ll check here is whether the residuals are normally distributed. Let’s try three different things to determine this.\nFirst, a histogram of the residuals:\n\nlibrary(\"tidyverse\")\ntibble(our_residuals = residuals(m1)) %&gt;% \n  ggplot(aes(our_residuals))+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nTalk about right skew, amirite?! Yeah, from just the histogram we can safely say that the residuals are not normally distributed. But for good measure, let’s calculate the skewness score for the residuals. Skewness is a measure of how symmetrical a distribution is, not necessary a measure of normality (e.g., a perfectly symmetrical bimodal distribution would have a skewness score of 0):\n\nm1 %&gt;% \n  residuals() %&gt;% \n  moments::skewness()\n\n[1] 1.261311\n\n\nThe rule of thumb that was taught to Dr. Brown is that a skewness score between -0.5 and 0.5 is good enough to call a set of number symetrical distributed. However, that is not the case here as our skewness score is ~1.26.\nA third way to determine normality of numbers is with the Shapiro-Wilk test. A drawback of that test is that, the larger the number of data points, the more likely the test is to detect a non-normal distribution. It is because of this fact that the test simply refuses to run if there are more than 5,000 observations, which is the case here. However, for good measure, here’s the code:\n\nm1 %&gt;% \n  residuals() %&gt;% \n  shapiro.test()\n\nWhile we’re here, let’s calculate the AIC score for this model with raw word duration so that we can compare it to the AIC for the model with Box-Cox transformed response variable below. Also, let’s get the conditional \\(R^2\\) and the marginal \\(R^2\\). The conditional \\(R^2\\) tells us how much of the variance in the response variable is explained by both the random effects and the fixed effects, while the marginal \\(R^2\\) tells us how much of the variance in the response variable is explained by only the fixed effects.\n\nAIC(m1)\n\n[1] 73649.04\n\nperformance::r2_nakagawa(m1)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.667\n     Marginal R2: 0.434\n\n\nBecause the residuals are not normally distributed, let’s do a Box-Cox transformation of the response variable:\n\n# define a function to do the Box-Cox transformation\nboxcox_transform &lt;- function(input_vector) {\n  # param input_vector: a vector of numeric values\n  # return: a list of two elements:\n    # lambda: the lambda chosen by the Box-Cox algorithm\n    # output_vector: a vector of Box-Cox transformed values\n  \n  bc &lt;- MASS::boxcox(input_vector ~ 1, lambda = seq(-3, 3, 1/100), plotit=FALSE)\n  lambda &lt;- bc$x[which.max(bc$y)]\n  output_vector &lt;- (input_vector ^ lambda - 1) / lambda\n  output &lt;- list(lambda = lambda, output_vector = output_vector)\n  return(output)\n}\n\n# Get Box-Cox transformed word durations and the corresponding lambda\nbc_list &lt;- buck %&gt;% \n  pull(wd_dur) %&gt;% \n  boxcox_transform()\ncat(\"The lambda is:\", bc_list$lambda)\n\nThe lambda is: 0.24\n\n# Make a new column with the Box-Cox transformed word durations\nbuck &lt;- buck %&gt;% \n  mutate(wd_dur_bc = bc_list$output_vector)\n\nNow, let’s fit another regression model with the Box-Cox transformed response variable (wd_dur_bc):\n\n# linear regression with Box-Cox transformed word durations\nf2 &lt;- scale(wd_dur_bc) ~  # use the Box-Cox transformed word durations\n      scale(n_phon)+\n      scale(log(wd_freq))+\n      scale(spch_rate)+  \n      scale(frc)+\n      scale(frc):scale(log(wd_freq))+  # interaction term\n      scale(forw_predict)+\n      pre_mention+\n      scale(dist_end_iu)+\n      scale(iu_len_wds)+\n      scale(back_predict)+\n      scale(frc):scale(wd_freq)+  # interaction term\n      (1 + frc | file) +  \n      (1 | wd_upper)\nm2 &lt;- lmerTest::lmer(formula = f2, control = lmerControl(optimizer =\"optimx\", calc.derivs = F, optCtrl = list(method = \"nlminb\", starttests = F, kkt = F)),\n    data = buck)\n\nNow, let’s look at the residuals of this second Box-Cox-transformed model:\n\ntibble(our_residuals = residuals(m2)) %&gt;% \n  ggplot(aes(our_residuals))+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThat’s more like it! Yeah, that looks like a nice bell curve. Let’s calculate the skewness score too:\n\nm2 %&gt;% \n  residuals() %&gt;% \n  moments::skewness()\n\n[1] 0.479374\n\n\nThe skewness score (i.e., ~0.479) falls within the rule of thumb. Cool! Let’s keep going.\nAnd let’s calculate the AIC and the \\(R^2\\) scores of this second Box-Cox transformed model:\n\nAIC(m2)\n\n[1] 73520.87\n\nperformance::r2_nakagawa(m2)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.630\n     Marginal R2: 0.437\n\n\nWouldn’t you know it! The AIC has gone down, which is a good thing, as the lower the AIC, the better. The \\(R^2\\) scores have not changed much."
  },
  {
    "objectID": "lessons/regression_assumptions.html#residuals-vary-constantly",
    "href": "lessons/regression_assumptions.html#residuals-vary-constantly",
    "title": "Regression assumptions",
    "section": "Residuals vary constantly",
    "text": "Residuals vary constantly\nAnother assumption of regression analysis is that the residuals vary constantly, called homoscedasticity. The opposite is called heteroscedasticity is the condition when the residuals do not vary constantly.\nA good way to determine whether a model has homoscedasticity or heteroscedasticity is by creating a scatterplot of the residuals of the model on one axis with the fitted (aka. predicted) values of the response variable of the model on the other axis. If the cluster of dots looks like a funnel or a megaphone (i.e., little spread to large spread when moving from left to right, or vice versa) or an American football (i.e., little to large to little spread when moving from left to right), you have heteroscedasticity, which is a problem that needs to be fixed. (Here’s a good blog post on R-bloggers about detecting heteroscedasticity and how to fix it.)\nLet’s use our Box-Cox transformed model from above to see create a scatterplot with the residuals and the predicted (aka. fitted) values:\n\ntemp_df &lt;- tibble(\n  our_residuals = residuals(m2), \n  our_fitted = fitted(m2))\ntemp_df %&gt;% \n  ggplot(aes(our_residuals, our_fitted))+\n  geom_point()\n\n\n\n\nUh-oh! We have a stubby American-football-shaped cluster of dots. This suggest heteroscedasticity.\nAs a remedy, we would need to use a bootstrapping approach, which is a topic for another day. See Dr. Brown’s article in Language Variation and Change."
  },
  {
    "objectID": "lessons/t-test.html",
    "href": "lessons/t-test.html",
    "title": "t-test, Wilcoxon test",
    "section": "",
    "text": "Students will perform a t-test and a Wilcoxon test"
  },
  {
    "objectID": "lessons/t-test.html#objective",
    "href": "lessons/t-test.html#objective",
    "title": "t-test, Wilcoxon test",
    "section": "",
    "text": "Students will perform a t-test and a Wilcoxon test"
  },
  {
    "objectID": "lessons/t-test.html#formative-quiz",
    "href": "lessons/t-test.html#formative-quiz",
    "title": "t-test, Wilcoxon test",
    "section": "Formative quiz",
    "text": "Formative quiz\nMatch each term with its corresponding definition.\n\nGood ol’ fashioned matching quiz\n\n\n\n\n\n\nsample\nThe mean of the sample\n\n\npopulation\nA value that is the result of converting a value into units of standard deviations from the mean\n\n\npoint estimate\nA Greek letter that usually stands for the standard deviation of a continuous variable of the population\n\n\nconfidence interval\nA Romanized letter that usually stands for the explanatory variable\n\n\nstandard error\nSomething that might affect the thing under study\n\n\nt-test\nThe thing that is being studied (aka. dependent thing of study)\n\n\nWilcoxon test\na Greek letter that usually stands for the mean of a continuous variable of a population\n\n\nσ (sigma)\nAn ideal spread of data points around the central tendency; looks like a good ol’ fashioned bell (aka. bell curve)\n\n\nμ (meu)\nthe number of data points (aka. observations)\n\n\nn\nThe standard deviation of the sample\n\n\nx\nThe group that the researcher want to know something about\n\n\ny\nA non-parametric test\n\n\nresponse variable\nThe group that is used to infer about a larger group\n\n\nexplanatory variable\nThe proposition that there is likely an effect of the explanatory variable on the response variable\n\n\nz-score\nA Romanized letter that usually stands for the response variable.\n\n\nμ̂ (meu hat)\na single number that characterizes an aspect of a dataset\n\n\nσ̂ (sigma hat)\nA range of values that is likely to contain the population value\n\n\nalpha level\nThe threshold below which the null hypothesis can be rejected\n\n\nnull hypothesis\nA parametric test with a continuous response variable and a categorical variable with exactly two levels\n\n\nalternative hypothesis\nA measure of how much uncertainty there is in the estimate of the sample mean\n\n\nnormal distribution\nThe proposition that there is no effect of the explanatory variable(s) on the response variable\n\n\n\n\nKey\n\n\n\n\n\n\nμ̂ (meu hat)\nThe mean of the sample\n\n\nz-score\nA value that is the result of converting a value into units of standard deviations from the mean\n\n\nσ (sigma)\nA Greek letter that usually stands for the standard deviation of a continuous variable of the population\n\n\nx\nA Romanized letter that usually stands for the explanatory variable\n\n\nexplanatory variable\nSomething that might affect the thing under study\n\n\nresponse variable\nThe thing that is being studied (aka. dependent thing of study)\n\n\nμ (meu)\na Greek letter that usually stands for the mean of a continuous variable of a population\n\n\nnormal distribution\nAn ideal spread of data points around the central tendency; looks like a good ol’ fashioned bell (aka. bell curve)\n\n\nn\nthe number of data points (aka. observations)\n\n\nσ̂ (sigma hat)\nThe standard deviation of the sample\n\n\npopulation\nThe group that the researcher want to know something about\n\n\nWilcoxon test\nA non-parametric test\n\n\nsample\nThe group that is used to infer about a larger group\n\n\nalternative hypothesis\nThe proposition that there is likely an effect of the explanatory variable on the response variable\n\n\ny\nA Romanized letter that usually stands for the response variable.\n\n\npoint estimate\na single number that characterizes an aspect of a dataset\n\n\nconfidence interval\nA range of values that is likely to contain the population value\n\n\nalpha level\nThe threshold below which the null hypothesis can be rejected\n\n\nt-test\nA parametric test with a continuous response variable and a categorical variable with exactly two levels\n\n\nstandard error\nA measure of how much uncertainty there is in the estimate of the sample mean\n\n\nnull hypothesis\nThe proposition that there is no effect of the explanatory variable(s) on the response variable"
  },
  {
    "objectID": "lessons/t-test.html#t-test",
    "href": "lessons/t-test.html#t-test",
    "title": "t-test, Wilcoxon test",
    "section": "t-test",
    "text": "t-test\nThe response variable is continuous (aka. quantitative) and the explanatory variable is categorical with exactly two levels (aka. values or groups).\n\nAssumptions\nThe distribution of the response is normal, and/or there are more than 30 data points in each level (aka. group) of the explanatory variable.\nThe variances of the response variables in both levels of the explanatory are equal. However, this is often not the case and our new friend Mr. Welch proposed an adjustment that provides a correction for unequal variances. The default behavior of the t.test() in R (i.e., the var.equal argument) is to use Welch’s adjustment.\n\n\nOne-tailed or two-tailed\nA one-tailed test should be used when the alternative hypothesis is directional, that is, it hypothesizes that the mean of one group is greater than the mean of the other group, or less than the other group. For example, the null and alternative hypotheses would look something like this:\n\\[\nH_0:μ_A = μ_B\n\\] \\[\nH_a:μ_A&gt;μ_B\n\\]\nWe use a two-tailed t-test when our alternative hypothesis is not directional, that is, it hypothesizes that the mean of one group does not equal the mean of the other group, but it doesn’t say anything about direction. For example:\n\\[\nH_0:μ_A = μ_B\n\\] \\[\nH_a:μ_A ≠ μ_B\n\\]"
  },
  {
    "objectID": "lessons/t-test.html#wilcoxon-test",
    "href": "lessons/t-test.html#wilcoxon-test",
    "title": "t-test, Wilcoxon test",
    "section": "Wilcoxon test",
    "text": "Wilcoxon test\nWhen the data are not normally distributed (and you don’t have &gt;30 observations in each level of the categorical explanatory variable, so that you can use the t-test), the Wilcoxon test is the way to go. This test is also the way to go when you have ordinal data, like responses to a Likert-scale survey.\nThe Wilcoxon test is apparently super similar to, or in most cases identifical to, the Mann-Whitney test; see Q&A thread. Our ol’ friend Jamovi has a check box for the “Mann-Whitney” test, while the function in R run these test is wilcox.test()."
  },
  {
    "objectID": "lessons/t-test.html#question",
    "href": "lessons/t-test.html#question",
    "title": "t-test, Wilcoxon test",
    "section": "Question",
    "text": "Question\nLet’s look at the speech rate in syllables per second of 25 Spanish speakers and 25 Italian speakers. First, let’s propose a null hypothesis and an alternative hypothesis. As we don’t have any previous information about the speech rate of Spanish and Italian speakers in comparison to each other, our alternative hypothesis will simply be “not equal to” rather than something directional like “greater than” or “less than”:\n\\[\nH_0:μ_S=μ_I\n\\] \\[\nH_a:μ_S≠μ_I\n\\]\nFirst, a good ol’ fashioned boxplot:\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nesit &lt;- read_csv(\"/Users/ekb5/Documents/data_analysis/datasets/span_ital_speaking_rate.csv\")\n\nRows: 50 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): language\ndbl (2): participant_ID, syllables\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nesit %&gt;% \n  ggplot(aes(x = language, y = syllables))+\n  geom_boxplot(notch = TRUE)\n\n\n\n\nThe non-overlapping notches on the boxes suggest that the difference in the means of the two groups will likely be statistically significantly different.\nEnter an inferential statistical test.\nFirst, let’s see if the data points are normally distributed. A simple Shapiro-Wilk normality test:\n\nesit %&gt;% \n  group_by(language) %&gt;% \n  do(\n    shapiro.test(.$syllables) %&gt;% \n      broom::tidy()\n  )\n\n# A tibble: 2 × 4\n# Groups:   language [2]\n  language statistic p.value method                     \n  &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                      \n1 Italian      0.971  0.663  Shapiro-Wilk normality test\n2 Spanish      0.926  0.0710 Shapiro-Wilk normality test\n\n\nBoth p-values are above 0.05, so that’s good.\nNow, let’s also plot the distribution in a density plot:\n\nesit %&gt;% \n  ggplot(aes(syllables, color = language))+\n  geom_density()\n\n\n\n\nNot the prettiest (normal) density plots.\nAnd for good measure, let’s calculate the skewness of each group:\n\ninstall.packages(\"moments\", repos = \"http://rstudio.org\")\n\nWarning: unable to access index for repository http://rstudio.org/src/contrib:\n  cannot open URL 'http://rstudio.org/src/contrib/PACKAGES'\n\n\nWarning: package 'moments' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n\nWarning: unable to access index for repository http://rstudio.org/bin/macosx/big-sur-arm64/contrib/4.4:\n  cannot open URL 'http://rstudio.org/bin/macosx/big-sur-arm64/contrib/4.4/PACKAGES'\n\n\n\nesit %&gt;% \n  group_by(language) %&gt;% \n  do(\n    moments::skewness(.$syllables) %&gt;% \n      broom::tidy()\n  )\n\nWarning in tidy.numeric(.): 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\nWarning in tidy.numeric(.): 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\n\n# A tibble: 2 × 2\n# Groups:   language [2]\n  language      x\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Italian  -0.216\n2 Spanish  -0.923\n\n\nA good rule of thumb is that if the skewness score is between -0.5 and 0.5, the data is normal enough to continue with a parametric test.\nBecause the Shapiro-Wilk gave us the green light to continue with a t-test, let’s go!\n\nesit %&gt;% \n  t.test(syllables~language, data = ., alternative = \"two.sided\") -&gt; result\nprint(result)\n\n\n    Welch Two Sample t-test\n\ndata:  syllables by language\nt = -6.34, df = 47.793, p-value = 7.695e-08\nalternative hypothesis: true difference in means between group Italian and group Spanish is not equal to 0\n95 percent confidence interval:\n -1.1748003 -0.6090245\nsample estimates:\nmean in group Italian mean in group Spanish \n             7.013432              7.905345 \n\nprint(result$stderr)\n\n[1] 0.1406801\n\n\nThe p-value suggests that the mean speech rate of Spanish speakers is different from the mean speech rate of Italian speakers.\n\nHow to report in paper\n“An independent two-tailed t-test compared the speech rate of Spanish speakers (n = 25) and Italian speakers (n = 25). We reject the null hypothesis and conclude that there is a signficant difference in speech rate between these two groups: t(47.793) = -6.34, p ≤ 0.001.”"
  },
  {
    "objectID": "lessons/t-test.html#activity",
    "href": "lessons/t-test.html#activity",
    "title": "t-test, Wilcoxon test",
    "section": "Activity",
    "text": "Activity\nYour turn! Use a dataset of your choice and see if there is a statistical difference between the means of a continuous variable based on two categories in a categorical variable."
  },
  {
    "objectID": "lessons/outliers.html",
    "href": "lessons/outliers.html",
    "title": "Detecting outliers",
    "section": "",
    "text": "Students will detect and remove outliers of continuous variables."
  },
  {
    "objectID": "lessons/outliers.html#objective",
    "href": "lessons/outliers.html#objective",
    "title": "Detecting outliers",
    "section": "",
    "text": "Students will detect and remove outliers of continuous variables."
  },
  {
    "objectID": "lessons/outliers.html#extreme-values-vs.-outliers",
    "href": "lessons/outliers.html#extreme-values-vs.-outliers",
    "title": "Detecting outliers",
    "section": "Extreme values vs. outliers",
    "text": "Extreme values vs. outliers\n(Thanks to Joey Stanley for some of this material!)\nOutliers of continuous or quantitative variables have extreme values, either extremely low or extremely high. Let’s consider this vector of (human) babies heights in inches: c(21, 19, 20, 18, 20, 73, 22, 18, 19). Wait, what? A baby that is 73 inches tall?! That must be an adult. That height is an outlier because it doesn’t come from the same population (i.e., babies).\nHow about this vector of (modern, human) ages of eight siblings: c(39, 38, 37, 356, 34, 33, 33, 31, 25). A modern human being with an age of 365 years?! This is probably a simple data entry mistake (probably the 5 and 6 keys pressed together). The best thing to do here would be to track down the right age, and if that’s not possible, consider this extreme age as an outlier and remove it.\nLet’s consider a third vector of number, this time of the number of children that each of the previously mentioned siblings have: c(0, 2, 8, 1, 0, 2, 2, 2, 0). Wait, someone has eight kids?! Oh yeah, nevermind, that’s not uncommon in certain religious groups. So, in this case the eight is an extreme value but not an outlier, and it should be kept in the dataset.\nMoral of the story: It’s a (really) good idea to investigate extreme values in order to make sure they actually are outliers that should be excluded, rather than simply extreme values that should still be kept in."
  },
  {
    "objectID": "lessons/outliers.html#how-to-remove-outliers",
    "href": "lessons/outliers.html#how-to-remove-outliers",
    "title": "Detecting outliers",
    "section": "How to remove outliers",
    "text": "How to remove outliers\nThere are a couple ways to identify (possible) outliers. One is based on the mean and the standard deviation of the observations. Another way is based on the interquartile range of the observations.\n\nMean and standard deviation\nA common way in linguistics to remove outliers is to remove data points that lay outside of the first two standard deviations above or below the mean value of a given continuous variable. Here’s the mathematical formula to find the threshold below which or above which possible outliers are detected:\n\\[\nthreshold = μ ± 2 * σ\n\\]\n…where \\(μ\\) is the mean of all observations, and \\(σ\\) is the standard deviation of all observations.\nLet’s go back to our baby heights example above and calculate the mean and standard deviation:\n\nlibrary(\"tidyverse\")\nbaby_heights &lt;- c(21, 19, 20, 18, 20, 73, 22, 18, 19)\ntibble(baby_heights) |&gt; \n  summarize(mean(baby_heights), sd(baby_heights))\n\nSo, the mean is \\(25.6\\) and the standard deviation is \\(17.8\\). So, values below the mean minus two times the standard deviation (i.e., \\(25.6-2*17.8=-10\\)), and values above the mean plus two times the standard deviation (i.e., \\(25.6+2*17.8=61.2\\)), might safely be considered outliers and can be removed.\nOf course, we don’t have to do this math by hand:\n\ntibble(baby_heights) |&gt; \n  filter(\n    baby_heights &gt; mean(baby_heights) - 2 * sd(baby_heights) & \n    baby_heights &lt; mean(baby_heights) + 2 *sd(baby_heights)\n  )\n\nPerhaps an easier way to do the previous filtering is to first create a new column with z-score values of the original ages, and then simply filter on \\(±2\\):\n\ntibble(baby_heights) |&gt; \n  mutate(z_height = as.vector(scale(baby_heights))) |&gt;\n  filter(z_height &gt; -2 & z_height &lt; 2)\n\nWhat is a z-score, you ask? It is a centered and scaled (aka. normalized) set of values. It is calculated by taking the mean and the standard deviation of a continuous variable, and then taking each value and subtracting the mean, and then dividing that difference by the standard deviation. Here’s the mathematical formula to do that:\n\\[\nz=\\frac{x-μ}{σ}\n\\]\n…where \\(x\\) is each observation, \\(μ\\) is the mean of all observations, and \\(σ\\) is the standard deviation of all observations.\n\n\nInterquartile range\nAnother way to identify (possible) outliers is by finding the first quartile (i.e., the 25th percentile, aka. Q1) and the third quartile (i.e., the 75th percentile, aka. Q3), and then calculating the difference between those two quartiles. The difference between Q3 and Q1 is called the interquartile range (aka. IQR). Then, outliers are identified as observations above 1.5 times IQR above Q3 or 1.5 times IQR below Q1. Here’s the mathematical notation:\n\\[\nQ_3+1.5*IQR\\quad\\text{or}\\quad Q_1-1.5* IQR\n\\]\nLet’s try this in R. First, let’s get Q1, Q3, and IQR. Then, we’ll use these three values in filter() within a good ol’ fashioned dplyr pipeline:\n\nquartiles &lt;- tibble(baby_heights) |&gt; \n  pull(baby_heights) |&gt; \n  quantile(probs = c(0.25, 0.75))\nq1 &lt;- quartiles[1]\nq3 &lt;- quartiles[2]\niqr &lt;- q3 - q1\n\n# here we go with the actual filtering\ntibble(baby_heights) |&gt; \n  filter(\n    baby_heights &gt; q1 - 1.5 * iqr & \n    baby_heights &lt; q3 + 1.5 * iqr)"
  },
  {
    "objectID": "lessons/outliers.html#activity",
    "href": "lessons/outliers.html#activity",
    "title": "Detecting outliers",
    "section": "Activity",
    "text": "Activity\nLoad up a dataset of your choice, for example, from the Dataset module in Canvas, and test several different continuous variables to see if any of them have outliers, and if so, how many outliers. Choose either method to identify (possible) outliers.\nAfter a good-faith effort, if you need help take a look at Dr. Brown’s code below:\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"readxl\")\nptk &lt;- read_excel(\"/Users/ekb5/Documents/data_analysis/datasets/Data_ptk.xlsx\", sheet = \"data\")\n\n# get the number of rows and columns\nptk |&gt; dim() # rows, columns\n\n# filter outliers below or above two standard deviations from the mean and then get the number of rows and columns again\nptk |&gt; \n  mutate(VOT_z = as.vector(scale(VOT))) |&gt; \n  filter(VOT_z &gt; -2 & VOT_z &lt; 2) |&gt; \n  dim()"
  },
  {
    "objectID": "lessons/outliers.html#two-continuous-variables",
    "href": "lessons/outliers.html#two-continuous-variables",
    "title": "Detecting outliers",
    "section": "Two continuous variables",
    "text": "Two continuous variables\nWhen you’re looking for outliers across two variables, visual inspection is probaby the easiest way. Here’s a toy example:\n\nlibrary(\"tidyverse\")\nx &lt;- runif(n = 100, min = 23, max = 67)\ny &lt;- jitter(x * 3.1415926, amount = 20)\ndf &lt;- tibble(x, y) |&gt; \n  bind_rows(tibble(x = 59, y = 100))\ndf |&gt; \n  ggplot(aes(x = x, y = y))+\n  geom_point()\n\nAs seen, there is an outlier below the majority of the data points (aka. observations). It would be good to filter the data frame in order to only see that particular observation. For example, we could filter so that only tokens with an x value greater than, say, 55 and a y value less than, say, 125 are kept:\n\ndf |&gt; \n  filter(x &gt; 55, y &lt; 125)\n\nIt would be wise to go look at this particular token to determine if it’s simply an extreme value and should be kept, or if it’s an outlier that should be removed."
  },
  {
    "objectID": "lessons/python.html",
    "href": "lessons/python.html",
    "title": "Other languages",
    "section": "",
    "text": "Students will write R code that calls other languages."
  },
  {
    "objectID": "lessons/python.html#objective",
    "href": "lessons/python.html#objective",
    "title": "Other languages",
    "section": "",
    "text": "Students will write R code that calls other languages."
  },
  {
    "objectID": "lessons/python.html#multilingual-programming-world",
    "href": "lessons/python.html#multilingual-programming-world",
    "title": "Other languages",
    "section": "Multilingual programming world",
    "text": "Multilingual programming world\nJust like human languages and all their variety and beauty, the variety of programming languages gives beauty to the computer world. More to the point, some programming languages are better suited than others for specific tasks. So, rather than having to be monolingual with only one programming language, we can use several languages as needed."
  },
  {
    "objectID": "lessons/python.html#python",
    "href": "lessons/python.html#python",
    "title": "Other languages",
    "section": "Python",
    "text": "Python\nIt is Dr. Brown’s opinion that Python is the best language for text processing (and is an all-around great language for many tasks). Thus, when an R user has a need to do text processing, it might be better to call on Python for that specific task.\n\nThe reticulate R package\nThe reticulate R package allows R users to call Python from within R. Here’s a simple example:\n\n# R code here\nlibrary(\"reticulate\")\nuse_python(\"/usr/local/bin/python3\")\n\n\ntxt &lt;- \"The quick brown fox jumped over the lazy dog.\"\n\nnltk &lt;- import(\"nltk\")\nnltk$download(\"punkt\")\n\n[1] TRUE\n\nnltk$download(\"averaged_perceptron_tagger\")\n\n[1] TRUE\n\ntokens &lt;- nltk$word_tokenize(txt)\nprint(tokens)\n\n [1] \"The\"    \"quick\"  \"brown\"  \"fox\"    \"jumped\" \"over\"   \"the\"    \"lazy\"  \n [9] \"dog\"    \".\"     \n\ntagged &lt;- nltk$pos_tag(tokens)\nprint(tagged)\n\n[[1]]\n[[1]][[1]]\n[1] \"The\"\n\n[[1]][[2]]\n[1] \"DT\"\n\n\n[[2]]\n[[2]][[1]]\n[1] \"quick\"\n\n[[2]][[2]]\n[1] \"JJ\"\n\n\n[[3]]\n[[3]][[1]]\n[1] \"brown\"\n\n[[3]][[2]]\n[1] \"NN\"\n\n\n[[4]]\n[[4]][[1]]\n[1] \"fox\"\n\n[[4]][[2]]\n[1] \"NN\"\n\n\n[[5]]\n[[5]][[1]]\n[1] \"jumped\"\n\n[[5]][[2]]\n[1] \"VBD\"\n\n\n[[6]]\n[[6]][[1]]\n[1] \"over\"\n\n[[6]][[2]]\n[1] \"IN\"\n\n\n[[7]]\n[[7]][[1]]\n[1] \"the\"\n\n[[7]][[2]]\n[1] \"DT\"\n\n\n[[8]]\n[[8]][[1]]\n[1] \"lazy\"\n\n[[8]][[2]]\n[1] \"JJ\"\n\n\n[[9]]\n[[9]][[1]]\n[1] \"dog\"\n\n[[9]][[2]]\n[1] \"NN\"\n\n\n[[10]]\n[[10]][[1]]\n[1] \".\"\n\n[[10]][[2]]\n[1] \".\"\n\n\nIf you know Python (and NLTK), you might remember that the output of this call in Python is a Python list of two-item tuples. Differently, here we get an R list (which is different from a Python list). We could continue to process this R list as needed, for example, looping over it in a for loop to find particular part-of-speech tags:\n\n# R code here\nfor (tag in tagged) {\n  if (tag[2] == \"NN\") {\n    print(tag[1])\n  }\n}\n\n\n\nWithin Quarto documents\nWe can also write Python code within code blocks in a Quarto document. When working in a Quarto document, you can type a forward slash and get a drop-down menu of possible things to insert into your document, one of which is a Python code block.\nThe following code blocks are written in a Quarto document (as Dr. Brown uses Quarto document for his lesson plans). The code is pure Python, as if he were writing Python code in a .py file. Here’s a first simple example:\n\n# Python code here\nprint(\"hello Python!\")\n\nhello Python!\n\n\nA more meaningful example might be to use the Natural Language Toolkit (NLTK) to tokenize word and perform part-of-speech tagging, like with did above within R using reticulate:. Differently, here, we don’t need reticulate because we calling Python directly on the computer, rather than through R.\n\n# more Python code here\nimport nltk\nnltk.download(\"punkt\")\n\nTrue\n\nnltk.download(\"averaged_perceptron_tagger\")\n\nTrue\n\ntxt = \"The quick brown fox jumped over the lazy dog.\"\ntokens = nltk.word_tokenize(txt)\nprint(tokens)\n\n['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.']\n\n\nThe above code tokenizes the famous sentence in linguistics into a Python list of words. Now, let’s ask NLTK to\n\nimport sys\nprint(sys.version)\n\n3.14.0 (v3.14.0:ebf955df7a8, Oct  7 2025, 08:20:14) [Clang 16.0.0 (clang-1600.0.26.6)]\n\n\ntag for part-of-speech:\n\n# Python code here\ntagged = nltk.pos_tag(tokens)\nprint(tagged)\n\n[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n\n\nThose with experience with Python may notice that the output of the above code is a Python list with two-item tuples. We’re back in the comfort of the Python world! (The R world is comfortable too, though.)\n\n\nActivity\nYour turn! Write some (simple) R code that calls Python using the reticulate package. If you’re not sure what to write, just try to get the examlpes on the reticulate docs working."
  },
  {
    "objectID": "lessons/python.html#other-languages",
    "href": "lessons/python.html#other-languages",
    "title": "Other languages",
    "section": "Other languages",
    "text": "Other languages\nMany other languages can be called from within R. Some of the most likely to be called from with R are C++ and Julia.\n\nC++\nThe Rcpp R package is an awesome way to have R call C++. This is especially useful for bottlenecks that can be sped up with C++ (which runs much, much faster than R). First, let’s install Rcpp:\n\ninstall.packages(\"Rcpp\", repos = \"http://cran.rstudio.com/\")\n\nNext, let’s\n\nlibrary(\"Rcpp\")\ncppFunction('int add(int x, int y, int z) {\n  int sum = x + y + z;\n  return sum;\n}')\n# add works like a regular R function\nadd(1, 2, 3)\n\nIn addition to defining C++ function within an R script (i.e., an R file) with cppFunction(), we can write the C++ code with a .cpp file and call that file into an R script with the sourceCpp() function. Take a look at the example in Hadley Wickham’s book, in chapter 25 “Rewriting R code in C++”.\n\n\nJulia\nThe Julia programming language is a language oriented towards data science and statistics (very much like R). When you’re writing a Quarto document, you can insert a Julia code block by typing a forward slash “/” and selecting the Julia code block option. First, we need to install the JuliaCall R package:\n\ninstall.packages(\"JuliaCall\", repos = \"http://cran.rstudio.com/\")\n\n\n# Julia code here\nprintln(\"hello Julia!\")\n\nUsing JuliaCall, you can also call Julia directly from within R code. See some examples on their docs.\nOther programming languages can be called from within R, but Python, C++ and Julia are probably the most common (in Dr. Brown’s uninformed estimation)."
  },
  {
    "objectID": "lessons/linear_regression.html",
    "href": "lessons/linear_regression.html",
    "title": "Linear regression",
    "section": "",
    "text": "Students will build a linear regression model."
  },
  {
    "objectID": "lessons/linear_regression.html#objective",
    "href": "lessons/linear_regression.html#objective",
    "title": "Linear regression",
    "section": "",
    "text": "Students will build a linear regression model."
  },
  {
    "objectID": "lessons/linear_regression.html#ordinary-least-squares-regression",
    "href": "lessons/linear_regression.html#ordinary-least-squares-regression",
    "title": "Linear regression",
    "section": "Ordinary Least Squares Regression",
    "text": "Ordinary Least Squares Regression\nA very common form of regression is Ordinary Least Squares (OLS) linear regression. If no modifiers are put before the term “linear regression” then Ordinary Least Squares linear regression is being referred to.\nThe basic idea is to try to fit a straight line (aka. regression line) through data points such that you end up with the lowest possible sum of squared residuals."
  },
  {
    "objectID": "lessons/linear_regression.html#residuals",
    "href": "lessons/linear_regression.html#residuals",
    "title": "Linear regression",
    "section": "Residuals",
    "text": "Residuals\nA residual in regression is the distance between an observed data point (aka. observation) and the regression line. While there is a mathematical formula to find where exactly that line should go, that is, to find its slope and the value of \\(y\\) when the line crosses \\(x=0\\), for pedagogical purposes, let’s do a bit of trial-n-error."
  },
  {
    "objectID": "lessons/linear_regression.html#toy-example",
    "href": "lessons/linear_regression.html#toy-example",
    "title": "Linear regression",
    "section": "Toy example",
    "text": "Toy example\nLet’s create a simple data frame:\n\nsuppressPackageStartupMessages(library(\"tidyverse\"))\nmorenos &lt;- tibble(\n  person = c(\"Sam\", \"Eloisa\", \"Lola\", \"Heidi\", \"Ellen\"),\n  height = c(72, 70, 69, 63, 62),\n  age = c(18, 17, 14, 13, 11)\n)\nprint(morenos)\n\n# A tibble: 5 × 3\n  person height   age\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Sam        72    18\n2 Eloisa     70    17\n3 Lola       69    14\n4 Heidi      63    13\n5 Ellen      62    11\n\n\nLet’s plot height by age of the kids in the above data frame:\n\np1 &lt;- morenos %&gt;% \n  ggplot(aes(x = age, y = height))+\n  geom_point(size = 3)+\n  theme_minimal()\nplot(p1)\n\n\n\n\nIt looks like there is a relationship between height and age of these kids, such that as the kids get older, their heights increase (unsurprisingly)."
  },
  {
    "objectID": "lessons/linear_regression.html#trial-n-error",
    "href": "lessons/linear_regression.html#trial-n-error",
    "title": "Linear regression",
    "section": "Trial-n-error",
    "text": "Trial-n-error\nLet’s plot some lines and calculate the sum of the squares of the residuals for each line. In the plots below, the black points are observed values while the red points are predicted values. The vertical lines connecting the observed values (black points) and the predicted values (red points) are the residuals (aka. errors).\n\n# define a helper function\nplot_dots_and_line &lt;- function(df, input_slope, input_intercept) {\n  temp &lt;- df %&gt;% \n    mutate(hypothetical = age * input_slope + input_intercept)\n  p1 &lt;- temp %&gt;%\n    ggplot(aes(x = age, y = height))+\n    \n    # draw a straight line with the slope and y-intercept\n    geom_abline(slope = input_slope, intercept = input_intercept, color = \"blue\", linetype=3)+\n    \n    geom_point(aes(y = hypothetical), color = \"red\", size = 3)+\n    geom_segment(aes(xend = age, yend = hypothetical), alpha = 0.5)+\n    geom_point(size = 3)+\n    ggtitle(str_interp(\"Slope = ${input_slope}; y-intercept = ${input_intercept}; Sum of squares of residuals = ${sum((temp$height - temp$hypothetical)^2)}\"))+\n    theme_minimal()\n  return(p1)\n}\n\n# Make a list of guesses of slopes and y-intercepts\nguesses &lt;- list(\n  list(g_slope = 0, g_intercept = 65),\n  list(g_slope = 0.5, g_intercept = 65),\n  list(g_slope = 0.5, g_intercept = 60),\n  list(g_slope = 1, g_intercept = 50),\n  list(g_slope = 1, g_intercept = 52)\n)\n\n# Loop over the guesses\nfor (guess in guesses) {\n  p1 &lt;- plot_dots_and_line(morenos, guess$g_slope, guess$g_intercept)\n  plot(p1)\n}"
  },
  {
    "objectID": "lessons/linear_regression.html#the-formula",
    "href": "lessons/linear_regression.html#the-formula",
    "title": "Linear regression",
    "section": "The formula",
    "text": "The formula\nOkay, enough horsin’ around. Let’s use the formula (actually formulas, or formulae if you must) to get the best-fit regression line, that is, the line with the lowest sum of squares of residuals.\nHere’s the formula to get the predicted y-value for a given x-value.\n\\[\n\\hat{y}=slope*x+intercept\n\\]\nThe \\(\\hat{y}\\) in the formula is referred to as “y-hat” and is the predicted (aka. fitted) y-value for a given x value. And how do you get the slope and intercept, you ask? Math (don’t freak out, it’s not that bad):\n\\[\nslope = \\frac{N * \\sum{(x * y)} - \\sum{x} * \\sum{y}}{N * \\sum{(x^{2})} - (\\sum{x})^{2}}\n\\]\nWhere:\n\\(N\\) = number of observations in dataset\n\\(*\\) = Good ol’ fashioned multiplication\n\\(\\sum\\) = The sum of the numbers to the right\n\\(x\\) = The explanatory (aka. independent or predictor) variable (here age)\n\\(y\\) = The response (aka. dependent) variable (here height)\nLet’s look at each piece of this formula with color:\n\\[\nslope = \\frac{N * \\color{red} \\sum{(x * y)} \\color{black} - \\color{orange} \\sum{x} \\color{black} * \\color{green} \\sum{y}}{N * \\color{blue} \\sum{(x^{2})} \\color{black} - \\color{purple} (\\sum{x})^{2}}\n\\]\nSo:\n\\(N=5\\) \\(\\color{red} \\sum{(x*y)} \\color{black} =(18*72+17*70+14*69+13*63+11*62)=(1296+1190+966+819+682) = 4953\\)\n\\(\\color{orange} \\sum{x} \\color{black} =(18+17+14+13+11) = 73\\)\n\\(\\color{green} \\sum{y} \\color{black} =(72+70+69+63+62) = 336\\)\n\\(\\color{blue} \\sum{(x^{2})} \\color{black} =(18^{2}+17^{2}+14^{2}+13^{2}+11^{2})=(324+289+196+169+121) = 1099\\)\n\\(\\color{purple} (\\sum{x})^{2} \\color{black} =(18+17+14+13+11)^{2} = 73^{2} = 5329\\)\nNow, let’s put the numbers into the formula and finally get that rascally slope:\n\n\\[slope=\\frac{5* \\color{red} 4953 \\color{black} - \\color{orange} 73 \\color{black} * \\color{green} 336}{5* \\color{blue} 1099 \\color{black} - \\color{purple} 5329}=\\frac{24765-24528}{5495-5329}=\\frac{237}{166}= 1.427711\n\\]\nLovely, the slope of the least squares line (aka. line of best fit) is 1.427711.\nNow, here’s the formula for the y-intercept (which uses the slope that we just calculated):\n\n\\[intercept = \\frac{\\color{green} \\sum{y} \\color{black} - slope * \\color{orange} \\sum{x}}{N}\n\\]\nWe already calculated above all the numbers in this formula, so let’s get crackin’!:\n\\[intercept = \\frac{\\color{green} 336 \\color{black} - 1.427711 * \\color{orange} 73}{5}=\\frac{336-104.2229}{5}=\\frac{231.7771}{5}=46.35542\n\\]\nTerrific, the y-intercept of the least squares line is 46.35542.\nNow, let’s use the slope and the y-intercept we just calculated to draw a line through our data points and calculate the sum of the squares of residuals.\n\nplot_dots_and_line(morenos, 1.427711, 46.35542)"
  },
  {
    "objectID": "lessons/linear_regression.html#fit-a-linear-regression",
    "href": "lessons/linear_regression.html#fit-a-linear-regression",
    "title": "Linear regression",
    "section": "Fit a linear regression",
    "text": "Fit a linear regression\nAs I’m sure you have likely guessed, there’s a function to do all the math we just did by hand: lm(). Because the dependent variable (aka. response variable) is continuous, we will fit a linear regression (rather than a logistic regression). When we have only one explanatory variable, the term “simple linear regression” is often used. Differently, when we have two or more explanatory variables, the term “multiple linear regression” is used. We see multiple linear regression below.\n\nfitted_model &lt;- morenos %&gt;% \n  lm(height ~ age, data = .)\nprint(fitted_model)\n\n\nCall:\nlm(formula = height ~ age, data = .)\n\nCoefficients:\n(Intercept)          age  \n     46.355        1.428  \n\n\nThe summary() function gives you the above info as well as more info:\n\nsummary(fitted_model)\n\n\nCall:\nlm(formula = height ~ age, data = .)\n\nResiduals:\n       1        2        3        4        5 \n-0.05422 -0.62651  2.65663 -1.91566 -0.06024 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  46.3554     4.9552   9.355  0.00259 **\nage           1.4277     0.3342   4.272  0.02355 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.926 on 3 degrees of freedom\nMultiple R-squared:  0.8588,    Adjusted R-squared:  0.8117 \nF-statistic: 18.25 on 1 and 3 DF,  p-value: 0.02355\n\n\nThe Call section and the Residuals sections are transparently named. With small datasets, the residuals are given here in the summary, but when there are many data points (and therefore an equal number of many residuals), only summary information is given about the residuals, specifically the min and max, and the first and third quartiles, and the median. Ideally, the residuals should be normally distributed and centered on zero. If the summary info given isn’t enough to determine those assumptions, you can retrieve all the residuals with the residuals() function:\n\nresiduals(fitted_model)\n\n          1           2           3           4           5 \n-0.05421687 -0.62650602  2.65662651 -1.91566265 -0.06024096 \n\n\nThe Coefficients section gives a lot of numbers. The Estimate column gives the intercept, which again, is the predicted or fitted value of \\(y\\) when \\(x\\) is zero (i.e., the predicted value of the response variable when the explanatory variable has a value of zero). The slope of age is given in the age row, under the Estimate column. What this means, is that for each unit increase of the the explanatory (i.e., age), there is on average a 1.4277 unit increase in the response variable. So, in this toy example, this means that for each year older, a kid is 1.4277 inches taller. And, this general trend is statistically significant because the p-value (i.e., 0.02355) in the far right column (i.e., Pr(&gt;|t|)) is smaller than the alpha level of 0.05 (is we consider that alpha level to be our threshold below this statistical significance occurs)."
  },
  {
    "objectID": "lessons/linear_regression.html#predicted-values",
    "href": "lessons/linear_regression.html#predicted-values",
    "title": "Linear regression",
    "section": "Predicted values",
    "text": "Predicted values\nWe can get the predicted (aka. fitted) values (i.e., \\(\\hat{y}\\)) from the model and add them to the original dataframe.\n\nmorenos &lt;- morenos %&gt;% \n  mutate(predicted = predict(fitted_model))\nprint(morenos)\n\n# A tibble: 5 × 4\n  person height   age predicted\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Sam        72    18      72.1\n2 Eloisa     70    17      70.6\n3 Lola       69    14      66.3\n4 Heidi      63    13      64.9\n5 Ellen      62    11      62.1"
  },
  {
    "objectID": "lessons/linear_regression.html#residuals-1",
    "href": "lessons/linear_regression.html#residuals-1",
    "title": "Linear regression",
    "section": "Residuals",
    "text": "Residuals\nWe can get the residuals from the model and add them to the original dataframe. The residuals are the distances between the observed values and their corresponding predicted values.\n\nmorenos &lt;- morenos %&gt;% \n  mutate(residual = residuals(fitted_model)) \nprint(morenos)\n\n# A tibble: 5 × 5\n  person height   age predicted residual\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 Sam        72    18      72.1  -0.0542\n2 Eloisa     70    17      70.6  -0.627 \n3 Lola       69    14      66.3   2.66  \n4 Heidi      63    13      64.9  -1.92  \n5 Ellen      62    11      62.1  -0.0602\n\n\nLet’s now (more easily) plot the observed data points \\(y\\), the predicted values (i.e., \\(\\hat{y}\\)), and the residuals.\n\nmorenos %&gt;% \n  ggplot(aes(x = age, y = height))+\n  geom_smooth(method = lm, se = FALSE, linetype = 3)+\n  geom_point(aes(y = predicted), color = \"red\")+\n  geom_point()+\n  theme_minimal()+\n  geom_segment(aes(xend = age, yend = predicted), alpha = 0.5) \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/linear_regression.html#assumptions-of-linear-regression",
    "href": "lessons/linear_regression.html#assumptions-of-linear-regression",
    "title": "Linear regression",
    "section": "Assumptions of linear regression",
    "text": "Assumptions of linear regression\nLinear regression (like most inferential statistical tests) has a number of assumptions that should be met in order to obtain reliable results. I’m borrowing liberally from Levshina (2015, p. 155) here.\nAssumptions:\n\nThe observations are independent from one another. If the observations are related to each other (e.g., some/many tokens come from the same person or from the same word), then mixed-effect regression should be used with person and/or word specified as what’s called a random effect.\nThe response variable is continuous. If ordinal, then ordinal regression should be used instead. If categorical with two levels in the response variable, then logistic regression should be used. If categorical with three or more levels in the response variable, then multinomial regression should be used.\nThe relationship between the response and explanatory variable is linear. If not, then a transformation of the response and/or explanatory variables should be performed (e.g., taking the logarithm of word frequency).\nThe residuals of the model vary constantly. This is called homoscedasticity of variance. In other words, the variability of the residuals does not increase or decrease with the response variable nor with the explanatory variable(s). Fitting a scatterplot of the residuals by the predicted (aka. fitted) values of the linear model is a good way to detect if there is heteroscedasticity of variance. If the model was produced by lm(), you can simply call plot(variable_hold_model, which = 1). See p. 157 of Levshina (2015). Also, the leveneTest() function in the car package tests for homoscedasticity. If there is heteroscedasticity, a Box-Cox transformation of the response variable can help. If still problematic, a boostrapping procedure is a/the next step.\nThere is no multicollinearity of the explanatory variables, that is, they are not (overly) correlated with each other. Value Inflation Factors (VIF) scores below 5 or 10 indicate not much multicollinearity going on. The car package has a vif() function (and there are other packages with a similar functions.) See pp. 159-161 of Levshina (2015).\nThe residuals are not autocorrelated. A p-value below 0.05 in the durbinWatsonTest() function in the car package indicates autocorrelation. This is rarely a problem.\nThe residuals should be normally distributed, with a mean of zero. This assumption becomes less important as sample size increases. You can plot the residuals of a model with as a histogram or a density plot, after pulling the residuals out with residuals(). Also, you can use a Shapiro-Wilk test with shapiro.test() (p-values above [!] 0.05 suggest a normally distributed distribution), if the dataset has fewer than 5,000 observations.\n\nDetailed explanations of these assumptions and how to test for are offered by Levshina (2015, pp. 155 - 162)."
  },
  {
    "objectID": "lessons/linear_regression.html#activity",
    "href": "lessons/linear_regression.html#activity",
    "title": "Linear regression",
    "section": "Activity",
    "text": "Activity\nFrom Chapter 4 “Linear Regression 1” in Regression Modeling for Linguistic Data v1.1 by Sonderegger et al. (2022).\n\nUsing the languageR::english dataset, create a linear regression model with RTlexdec as the response variable and WrittenFrequency as the only explanatory variable.\nThen, figure out what the predicted RTlexdec for each of the following written frequencies. First, calculate the predicted RTlexdec values manually using the y-intercept and the slope given by the linear regression model (and probably using R as a basic calculator, e.g., \\(y\\text{-}intercept + slope * x\\)). Second, calculate the predicted RTlexdec values by using the predict.lm() function.\n\nWhat is the predicted RTlexdec value when WrittenFrequency is 5?\nHow about when WrittenFrequency is 10?\n\n\nAfter a good-faith effort, if you need help take a look at Dr. Brown’s code below.\n\n\nCode\n# fit linear regression model\nlanguageR::english %&gt;% \n  lm(RTlexdec ~ WrittenFrequency, data = .) -&gt; m1\nsummary(m1)\n\n\n\nCall:\nlm(formula = RTlexdec ~ WrittenFrequency, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45708 -0.11657 -0.00109  0.10403  0.56085 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       6.735931   0.006067 1110.19   &lt;2e-16 ***\nWrittenFrequency -0.037010   0.001134  -32.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1413 on 4566 degrees of freedom\nMultiple R-squared:  0.1891,    Adjusted R-squared:  0.1889 \nF-statistic:  1065 on 1 and 4566 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# save the intercept and slope to variables\nintercept &lt;- m1$coefficients[\"(Intercept)\"]\nslope &lt;- m1$coefficients[\"WrittenFrequency\"]\n\n# semi-manual math\nprint(intercept + slope * 5)\n\n\n(Intercept) \n    6.55088 \n\n\nCode\nprint(intercept + slope * 10)\n\n\n(Intercept) \n   6.365829 \n\n\nCode\n# use the predict.lm() function\npredict.lm(m1, newdata = tibble(WrittenFrequency = c(5, 10)))\n\n\n       1        2 \n6.550880 6.365829"
  },
  {
    "objectID": "lessons/linear_regression.html#more-than-one-explanatory-variable",
    "href": "lessons/linear_regression.html#more-than-one-explanatory-variable",
    "title": "Linear regression",
    "section": "More than one explanatory variable",
    "text": "More than one explanatory variable\nThe real usefulness of regression analysis is its ability to measure the influence of several explanatory variables at once in order to determine which variables significantly predict or explain the response variable.\nLet’s look at a multiple linear regression with three explanatory variables. Within the Rling package there is a dataset called ELP, which contains a subset of the English Lexicon Project. Let’s download the package from the companion website of the textbook How to Do Linguistics with R by Levshina. Then change the pathway in the following code and run it:\n\ninstall.packages(\"/pathway/to/Rling_1.0.tar.gz\", repos = NULL, type = \"source\")\n\nAnd then load up the dataset and inspect it a bit:\n\nlibrary(\"tidyverse\")\nlibrary(\"Rling\")\ndata(ELP)\nglimpse(ELP)\n\nRows: 880\nColumns: 5\n$ Word    &lt;fct&gt; rackets, stepmother, delineated, swimmers, umpire, cobra, vexe…\n$ Length  &lt;int&gt; 7, 10, 10, 8, 6, 5, 5, 8, 8, 6, 8, 12, 8, 6, 7, 3, 3, 10, 9, 4…\n$ SUBTLWF &lt;dbl&gt; 0.96, 4.24, 0.04, 1.49, 1.06, 3.33, 0.10, 0.06, 0.43, 5.41, 0.…\n$ POS     &lt;fct&gt; NN, NN, VB, NN, NN, NN, VB, NN, NN, NN, VB, NN, JJ, NN, NN, VB…\n$ Mean_RT &lt;dbl&gt; 790.87, 692.55, 960.45, 771.13, 882.50, 645.85, 760.29, 682.26…\n\nsummary(ELP)\n\n           Word         Length         SUBTLWF         POS     \n abbreviation:  1   Min.   : 3.00   Min.   :   0.020   JJ:159  \n abortions   :  1   1st Qu.: 7.00   1st Qu.:   0.180   NN:532  \n abrupt      :  1   Median : 8.00   Median :   0.570   VB:189  \n absentee    :  1   Mean   : 8.22   Mean   :   8.603           \n abutment    :  1   3rd Qu.:10.00   3rd Qu.:   2.105           \n accomplice  :  1   Max.   :20.00   Max.   :2556.730           \n (Other)     :874                                              \n    Mean_RT      \n Min.   : 517.5  \n 1st Qu.: 695.7  \n Median : 764.5  \n Mean   : 786.8  \n 3rd Qu.: 853.0  \n Max.   :1324.6  \n                 \n\n\nThe Word column is transparently named. The Length column is the length of the word in number of letters (as an integer). The SUBTLWF column gives the frequency (i.e., float or double or numeric) of the word normalized to per million words, as attested in a corpus of movie subtitles. The POS is transparently named (for a linguist, at least) and has three levels: JJ for adjective, NN for noun, and VB for verb. The Mean_RT column gives the mean reaction time in milliseconds (i.e., a float), and will the response variable in our regression.\nLet’s go with the linear regression!\nThe response variable (i.e., Mean_RT here) goes to the left of the formula operator (i.e., a tilde ~), and the explanatory variables to the right of that operator, separated by a plus sign +. The data frame with the dataset goes with the data argument. As is (very) common practice, we’ll take the log of frequency to put it on a more linear scale (from it’s original zipfian scale; see Vsauce’s video about lexical frequency).\nBehold, a linear regression and its results:\n\nm1 &lt;- lm(Mean_RT ~ Length + log(SUBTLWF) + POS, data = ELP)\nsummary(m1)\n\n\nCall:\nlm(formula = Mean_RT ~ Length + log(SUBTLWF) + POS, data = ELP)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-213.70  -62.55   -9.71   53.87  389.00 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   622.466     14.191  43.864  &lt; 2e-16 ***\nLength         19.555      1.433  13.645  &lt; 2e-16 ***\nlog(SUBTLWF)  -29.288      1.784 -16.420  &lt; 2e-16 ***\nPOSNN          -6.115      8.506  -0.719  0.47238    \nPOSVB         -29.184     10.154  -2.874  0.00415 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 93.29 on 875 degrees of freedom\nMultiple R-squared:  0.4565,    Adjusted R-squared:  0.454 \nF-statistic: 183.7 on 4 and 875 DF,  p-value: &lt; 2.2e-16\n\n\n\nInterpretation\nLet’s review the interpretation of the output. The Call section simply shows the lm() call. The Residuals section gives summary info about the residuals (but we could see the individual residuals by using the residuals() function). The Coefficients section gives the y-intercept and the slopes of the three explanatory variables.\nFirst, the Length row shows below the Estimate column that, on average, with each unit increases of the Length variable (i.e., each additional letter in word), there is a 19.555 milliseconds increase in Mean_RT (i.e., mean reaction time), and the p-value below the Pr(&gt;|t|) column is below 0.0000000000000002 (R’s default cutoff with p-values in this summary display). In simple terms, as the lengths of words increase, it takes humans significantly longer to react.\nThe log(SUBTLWF) row shows below the Estimate column that, on average, with each unit increase of frequency (on a log scale, given our use of log()), mean reaction times decrease by 29.288 milliseconds (note the negative sign before the number), and this happens at a statistically significant rate given the p-value.\nThe POS variable has two rows: POSNN and POSVB. But wait! What happened to JJ?! Because JJ comes first in the alphabet in comparison to NN and VB, it was chosen by R as the reference level against which the other levels in that categorical variable are compared. So, we compare each of the listed levels (i.e., NN and VB) to JJ, which is not listed. First things first, POSNN does not have a p-value below 0.05, so we conclude that there is no significant difference between NN and JJ in this dataset, and then we move on. Because POSVB’s p-value is below our alpha level of 0.05, we conclude that there is a significant difference between VB and JJ, such that, on average, reaction times to VB (i.e., verbs) is 29.184 milliseconds shorter (i.e., -29.184) than reaction times to JJ (i.e., adjectives).\nBy default, R chooses the level that comes first in alphabetical order to serve as the reference level. If we would like a different level to be the reference level, we can relevel the categorical variable so that another level is first with the fct_relevel() function. We saw this function previously in the context of boxplots, when we changed the order of the boxes."
  },
  {
    "objectID": "lessons/linear_regression.html#sum-contrasts",
    "href": "lessons/linear_regression.html#sum-contrasts",
    "title": "Linear regression",
    "section": "Sum contrasts",
    "text": "Sum contrasts\nThe comparison of a reference level to the other level(s) of a categorical variable is called “treatment contrasts”, and this is the default behavior of lm() in R. Another option is called “sum contrasts”, which is based on the mean of means of the response variable by each level in the categorical variable. The coefficients given indicate how much above or below that mean of means each level in the categorical is. For more info about sum contrasts, see this website or see p. 146 of Levshina (2015) or p. 201 of Regression Modeling for Linguistic Data v1.1 (among other good resources)."
  },
  {
    "objectID": "lessons/linear_regression.html#references",
    "href": "lessons/linear_regression.html#references",
    "title": "Linear regression",
    "section": "References",
    "text": "References\nLevshina, Natalia. 2015. How to do Linguistics with R: Data Exploration and Statistical Analysis. Amsterdam / Philadelphia: John Benjamins.\nSonderegger, Morgan. 2022. Regression Modeling for Linguistic Data. Available online.\nAlso, I thank the creators of these websites, which I relied on heavily: https://drsimonj.svbtle.com/visualising-residuals\nhttps://www.mathsisfun.com/data/least-squares-regression.html"
  },
  {
    "objectID": "lessons/multicollinearity.html",
    "href": "lessons/multicollinearity.html",
    "title": "Multicollinearity",
    "section": "",
    "text": "Students measure the amount of multicollinearity among explanatory variables."
  },
  {
    "objectID": "lessons/multicollinearity.html#elc-l2-english-written-corpus",
    "href": "lessons/multicollinearity.html#elc-l2-english-written-corpus",
    "title": "Multicollinearity",
    "section": "ELC L2 English Written Corpus",
    "text": "ELC L2 English Written Corpus\nLexical diversity has been shown to positively correlate with L2 proficiency. That is, as L2 learners become more proficient, they use more unique words. Put the other way around, they repeat the same words less often. The lex_div_ELC.csv file in the CMS contains 4k+ observations of texts written by L2 English writers in BYU’s English Language Center (ELC). At the end of each semester, ELC students write a response to a 10-minute prompt and another response to a 30-minute prompt, and those two responses are rated by human readers. After Rasch Analysis, the FairAverage score is created. That FairAverage score is a measure of the students’ proficiency. Kelly Woods (while an MA Linguistics student here), Brett Hashimoto, and I (or is it: “me, Kelly and Brett”?) used this corpus to study the effect of lexical diversity on the FairAverage scores as well as to study the multicollinearity of the many lexical diversity measures that have been created since the early 20th century (see paper). We used Variance Inflation Factors and Principal Component Analysis during the process of analyzing the multicollinearity of the lexical diversity measures. So, we know that there is lots of multicollinearity between these measures, and so it’s a great dataset to explore the negative effects of multicollinearity on regression results."
  },
  {
    "objectID": "lessons/multicollinearity.html#prep-the-data",
    "href": "lessons/multicollinearity.html#prep-the-data",
    "title": "Multicollinearity",
    "section": "Prep the data",
    "text": "Prep the data\nDownload the CSV file from the CMS and update the working directory for your computer.\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nld &lt;- read_csv(\"/Users/ekb5/Documents/data_analysis/datasets/lex_div_ELC.csv\")\n\nRows: 4207 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): id, sem, test_type, timing, sem_yr, Sex, L1, country, L1_clean, L1...\ndbl (15): yr, n_wds, ttr, hdd, mtld, mattr, mtld_ma_w, mtld_ma_bid, root_ttr...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(ld)\n\n# A tibble: 6 × 25\n  id           yr sem   test_type timing n_wds   ttr   hdd  mtld mattr mtld_ma_w\n  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 000305437  2020 Fall  LAT       10 mi…   130 0.5   0.723  43.3 0.717      40.5\n2 000305437  2020 Fall  LAT       30 mi…   384 0.353 0.773  51.4 0.722      51.0\n3 000305437  2020 Fall  Placement 10 mi…   147 0.707 0.864 108.  0.817     120. \n4 000305437  2020 Fall  Placement 30 mi…   362 0.426 0.775  57.7 0.766      62.0\n5 000346838  2018 Fall  LAT       10 mi…   141 0.507 0.708  40.8 0.656      38.8\n6 000346838  2018 Fall  LAT       30 mi…   443 0.256 0.685  41.4 0.672      39.1\n# ℹ 14 more variables: mtld_ma_bid &lt;dbl&gt;, root_ttr &lt;dbl&gt;, log_ttr &lt;dbl&gt;,\n#   maas_ttr &lt;dbl&gt;, msttr &lt;dbl&gt;, sem_yr &lt;chr&gt;, Age &lt;dbl&gt;, Sex &lt;chr&gt;, L1 &lt;chr&gt;,\n#   FairAverage &lt;dbl&gt;, FitStat &lt;dbl&gt;, country &lt;chr&gt;, L1_clean &lt;chr&gt;,\n#   L1_final &lt;chr&gt;"
  },
  {
    "objectID": "lessons/multicollinearity.html#draw-some-scatterplots",
    "href": "lessons/multicollinearity.html#draw-some-scatterplots",
    "title": "Multicollinearity",
    "section": "Draw some scatterplots",
    "text": "Draw some scatterplots\nDraw scatterplots of FairAverage by the predictor variables that are selected as significant in the linear regression below. The lexical diversity measurements were obtained with the lexical_diversity Python module.\n\npredictors &lt;- c(\"n_wds\", \"hdd\", \"mattr\", \"mtld_ma_bid\", \"log_ttr\", \"maas_ttr\", \"msttr\")\nfor (predictor in predictors) {\n  p1 &lt;- ld %&gt;% \n    ggplot(aes(.data[[predictor]], FairAverage))+\n    geom_point(alpha = 0.5)+\n    geom_smooth(formula = y ~ x, method = lm)+\n    ggtitle(predictor)+\n    theme_minimal()\n  print(p1)\n}"
  },
  {
    "objectID": "lessons/multicollinearity.html#linear-regression",
    "href": "lessons/multicollinearity.html#linear-regression",
    "title": "Multicollinearity",
    "section": "Linear regression",
    "text": "Linear regression\nFairAverage is the response (aka. dependent or outcome) variable, and a bunch of lexical diversity measures (as well as text length as number of words) are explanatory (aka. independent or predictor) variables.\nQuestion: How do the signs (i.e., positive or negative) of the coefficient estimates compare with the sign of the slopes of the regression lines in the scatterplots above?\n\nm1 &lt;- lm(FairAverage ~ n_wds + ttr + hdd + mtld + mattr + mtld_ma_w + mtld_ma_bid + root_ttr + log_ttr + maas_ttr + msttr, data = ld)\nsummary(m1)\n\n\nCall:\nlm(formula = FairAverage ~ n_wds + ttr + hdd + mtld + mattr + \n    mtld_ma_w + mtld_ma_bid + root_ttr + log_ttr + maas_ttr + \n    msttr, data = ld)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7226 -0.6490 -0.0980  0.5721  3.0593 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.580e+01  7.775e+00   5.891 4.14e-09 ***\nn_wds       -1.336e-03  4.044e-04  -3.304 0.000962 ***\nttr         -1.953e+00  2.758e+00  -0.708 0.478987    \nhdd         -6.921e+00  1.135e+00  -6.096 1.19e-09 ***\nmtld        -4.367e-03  3.226e-03  -1.354 0.175841    \nmattr        3.260e+00  1.042e+00   3.129 0.001765 ** \nmtld_ma_w   -1.060e-03  4.679e-03  -0.227 0.820737    \nmtld_ma_bid  1.078e-02  2.956e-03   3.646 0.000270 ***\nroot_ttr     1.592e-01  9.488e-02   1.678 0.093359 .  \nlog_ttr     -3.671e+01  1.061e+01  -3.459 0.000548 ***\nmaas_ttr    -1.069e+02  1.093e+01  -9.778  &lt; 2e-16 ***\nmsttr       -1.577e+00  7.211e-01  -2.187 0.028766 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9048 on 4195 degrees of freedom\nMultiple R-squared:  0.3051,    Adjusted R-squared:  0.3033 \nF-statistic: 167.4 on 11 and 4195 DF,  p-value: &lt; 2.2e-16\n\nAIC(m1)  # the lower the AIC, the better\n\n[1] 11110.72"
  },
  {
    "objectID": "lessons/multicollinearity.html#correlation-matrix",
    "href": "lessons/multicollinearity.html#correlation-matrix",
    "title": "Multicollinearity",
    "section": "Correlation matrix",
    "text": "Correlation matrix\nWe can see the Pearson’s r correlation scores for all pairwise comparisons of the continuous variables.\n\ncorrelations &lt;- ld %&gt;% \n  select(n_wds, ttr, hdd, mtld, mattr, mtld_ma_w, mtld_ma_bid, root_ttr, log_ttr, maas_ttr, msttr) %&gt;% \n  cor()\nprint(correlations)\n\n                 n_wds         ttr        hdd       mtld      mattr  mtld_ma_w\nn_wds        1.0000000 -0.82953071  0.2298356  0.1372262  0.1450162  0.1982883\nttr         -0.8295307  1.00000000  0.1977583  0.2465650  0.2462666  0.1992991\nhdd          0.2298356  0.19775826  1.0000000  0.8417526  0.8911481  0.8809807\nmtld         0.1372262  0.24656498  0.8417526  1.0000000  0.8699218  0.9324991\nmattr        0.1450162  0.24626656  0.8911481  0.8699218  1.0000000  0.8967546\nmtld_ma_w    0.1982883  0.19929915  0.8809807  0.9324991  0.8967546  1.0000000\nmtld_ma_bid  0.2616791  0.05079727  0.7622931  0.8237424  0.8495410  0.8884906\nroot_ttr     0.5437391 -0.13775588  0.7889107  0.6664880  0.6906216  0.7342239\nlog_ttr     -0.6879840  0.96178988  0.4145887  0.4171176  0.4351096  0.3874840\nmaas_ttr     0.2622747 -0.70642548 -0.7608001 -0.6803896 -0.7259851 -0.6864229\nmsttr        0.1849228  0.19003573  0.8642273  0.8439068  0.9006630  0.8589579\n            mtld_ma_bid   root_ttr    log_ttr   maas_ttr      msttr\nn_wds        0.26167910  0.5437391 -0.6879840  0.2622747  0.1849228\nttr          0.05079727 -0.1377559  0.9617899 -0.7064255  0.1900357\nhdd          0.76229310  0.7889107  0.4145887 -0.7608001  0.8642273\nmtld         0.82374243  0.6664880  0.4171176 -0.6803896  0.8439068\nmattr        0.84954103  0.6906216  0.4351096 -0.7259851  0.9006630\nmtld_ma_w    0.88849056  0.7342239  0.3874840 -0.6864229  0.8589579\nmtld_ma_bid  1.00000000  0.7101344  0.2357790 -0.5257504  0.7830501\nroot_ttr     0.71013441  1.0000000  0.1318997 -0.5725447  0.6848563\nlog_ttr      0.23577898  0.1318997  1.0000000 -0.8644347  0.3775065\nmaas_ttr    -0.52575043 -0.5725447 -0.8644347  1.0000000 -0.6747840\nmsttr        0.78305011  0.6848563  0.3775065 -0.6747840  1.0000000"
  },
  {
    "objectID": "lessons/multicollinearity.html#correlation-plot",
    "href": "lessons/multicollinearity.html#correlation-plot",
    "title": "Multicollinearity",
    "section": "Correlation plot",
    "text": "Correlation plot\nVisualizing a correlation matrix can be helpful.\n\ncorrplot::corrplot(correlations, type = \"lower\", diag = FALSE)"
  },
  {
    "objectID": "lessons/multicollinearity.html#enter-vifs",
    "href": "lessons/multicollinearity.html#enter-vifs",
    "title": "Multicollinearity",
    "section": "Enter VIFs",
    "text": "Enter VIFs\nVariance Inflation Factors (VIFs) report how much multicollinearity there is associated with each explanatory variable in a regression model. In other words, “these measure the degree to which one predictor can be accounted for by the other predictors” (Winter 2020, p. 114). There are different rules of thumbs about the threshold above which a VIF should not exceed. See p. 160 of Levshina (2015) here and p. 114 of Winter (2020) here. The car R package has a vif() function to get VIFs. (There are other packages that have functions to get VIFs, for example, regclass::VIF() and olsrr::ols_vif_tol().)\nQuestion: How do the VIF scores in our linear regression compare to the proposed thresholds mentioned by Levshina and Winter?\n\ncar::vif(m1)\n\n      n_wds         ttr         hdd        mtld       mattr   mtld_ma_w \n  13.268787  411.109798   10.953516    8.572724   11.686276   16.902308 \nmtld_ma_bid    root_ttr     log_ttr    maas_ttr       msttr \n   7.463975   42.139729  520.055206   60.315979    6.278952"
  },
  {
    "objectID": "lessons/multicollinearity.html#activity",
    "href": "lessons/multicollinearity.html#activity",
    "title": "Multicollinearity",
    "section": "Activity",
    "text": "Activity\nThrough trial and error, choose predictor variables to remove in order to reduce the VIFs. Refit a linear regression each time you remove a predictor and compare the (adjusted) R2 values of new models with the original model (the bigger, the better) and AIC scores (the lower, the better), and also get new VIF scores."
  },
  {
    "objectID": "lessons/transform.html",
    "href": "lessons/transform.html",
    "title": "Transform continuous variables",
    "section": "",
    "text": "Students will transform continuous variables in several ways."
  },
  {
    "objectID": "lessons/transform.html#objective",
    "href": "lessons/transform.html#objective",
    "title": "Transform continuous variables",
    "section": "",
    "text": "Students will transform continuous variables in several ways."
  },
  {
    "objectID": "lessons/transform.html#why-transform",
    "href": "lessons/transform.html#why-transform",
    "title": "Transform continuous variables",
    "section": "Why transform?",
    "text": "Why transform?\nWhen continuous are not normally distributed, it is often useful to transform them to (try to) them more normally distributed."
  },
  {
    "objectID": "lessons/transform.html#how-to-transform",
    "href": "lessons/transform.html#how-to-transform",
    "title": "Transform continuous variables",
    "section": "How to transform?",
    "text": "How to transform?\nThere are several common transformations of continuous variables."
  },
  {
    "objectID": "lessons/transform.html#natural-logarithm",
    "href": "lessons/transform.html#natural-logarithm",
    "title": "Transform continuous variables",
    "section": "Natural logarithm",
    "text": "Natural logarithm\nA (very) common transformation of continuous variables, especially word frequency, is to take the natural logarithm (with the log() function). Word frequencies often follow a Zipfian distribution (see the first minute and a half of this V-Sauce video to learn what this means). Taking the natural logarithm (often abbreviated as “log”) makes the Zipfian distribution more normally distributed. Let’s take a look at the frequency of words in the CELEX corpus, as given in the english dataset made available in the languageR R package. Note: Because the frequencies are already log-transformed, we’ll untransform them by taking the exponent (which is the opposite of the log) in order to see the original distribution of frequencies.\n\nGet the data\n\n#install.packages(\"languageR\")  # if needed, uncomment and run (in console)\neng &lt;- languageR::english\n\n\n\nPlot raw frequencies\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\neng %&gt;% \n  ggplot(aes(exp(WrittenFrequency)))+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAs we can see, the distribution of frequencies is not normally distributed. For fun, let’s also do a Shapiro-Wilk normality test (because we have fewer than 5,000 tokens in the dataset).\n\ns1 &lt;- eng %&gt;% \n  pull(WrittenFrequency) %&gt;% \n  exp() %&gt;% \n  shapiro.test()\ns1\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.21867, p-value &lt; 2.2e-16\n\ncat(\"p-value of Shapiro-Wilk test of raw frequencies: \", s1$p.value)\n\np-value of Shapiro-Wilk test of raw frequencies:  9.007604e-88\n\n\n\n\nPlot the log frequencies\n\neng %&gt;% \n  ggplot(aes(WrittenFrequency))+  # already log-transformed by dataset creator\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAs we can see, the distribution looks a lot more normal than the original, raw frequencies. For kicks, let’s do a Shapiro-Wilk normality test on the log-transformed frequencies.\n\neng %&gt;% \n  pull(WrittenFrequency) %&gt;% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.99281, p-value = 2.047e-14\n\n\nAs we see, the p-value is below 0.05 and thus still suggests a non-normal distribution. However, it should be noted that as the number of observations increases, the likelihood of getting a (supposed) non-normal distribution increases. It’s for this reason that the Shapiro-Wilk test doesn’t allow more than 5,000 tokens."
  },
  {
    "objectID": "lessons/transform.html#box-cox-transformation",
    "href": "lessons/transform.html#box-cox-transformation",
    "title": "Transform continuous variables",
    "section": "Box-Cox transformation",
    "text": "Box-Cox transformation\nAnother transformation of continuous variables is the Box-Cox transformation, named after George Box and David Cox who proposed the transformation in a paper in 1964. See an explanation of the Box-Cox transformation here.\n\nRling package\nLet’s look at another dataset. Download the Rling R package from this website and install it by following the directions there.\n\n# after downloading the 'Rling' R package with a web browser...\n# install.packages(\"/Users/ekb5/Downloads/Rling_1.0.tar.gz\", repos = NULL, type = \"source\")\n\nLet’s get the ELP (English Lexicon Project) dataset from that package.\n\nlibrary(\"Rling\")\ndata(ELP)\n\n\n\nPlot raw mean reaction times\n\nELP %&gt;% \n  ggplot(aes(Mean_RT))+\n  geom_histogram()+\n  labs(title = \"Histogram of raw mean reaction times\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLooks somewhat normally distributed, but with right skew (a right tail).\nLet’s look at the distribution when transformed with the natural logarithm.\n\nELP %&gt;% \n  ggplot(aes(log(Mean_RT)))+\n  geom_histogram()+\n  labs(title = \"Histogram of natural logarithm of mean reaction times\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThat moved the mass of the bell curve to the right a bit, and shortened the right tail.\nNow let’s do a Box-Cox transformation of the mean reaction times using the boxcox() function from the MASS R package and draw a histogram.\n\nbc &lt;- MASS::boxcox(ELP$Mean_RT ~ 1, lambda = seq(-10, 10, 1/100))\n\n\n\nlambda &lt;- bc$x[which.max(bc$y)]\nprint(lambda)\n\n[1] -1.05\n\nELP %&gt;% \n  mutate(bc_rt = (Mean_RT ^ lambda - 1) / lambda) %&gt;% \n  ggplot(aes(bc_rt))+\n  geom_histogram()+\n  labs(title = \"Box-Cox transformed mean reaction times\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/transform.html#raw-residuals",
    "href": "lessons/transform.html#raw-residuals",
    "title": "Transform continuous variables",
    "section": "Raw residuals",
    "text": "Raw residuals\nFirst, let’s fit a linear model with the raw mean reaction times.\n\nm1 &lt;- ELP %&gt;% \n  lm(Mean_RT ~ Length + log(SUBTLWF) + POS, data = .)\n\n\nMean raw residual\nWhat’s the mean raw residual? How close is it to zero?\n\ncat(\"Mean of the residuals = \", mean(residuals(m1)))\n\nMean of the residuals =  1.195004e-15\n\n\n\n\nHistogram of raw residuals\n\ntibble(res = residuals(m1)) %&gt;% \n  ggplot(aes(res))+\n  geom_histogram()+\n  ggtitle(\"Histogram of raw residuals\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nQ-Q plot of raw residuals\nQ-Q (quantile-quantile) plot are also useful for visualizing residuals of a linear regression model. We can use two functions here of our ol’ friend ggplot. To read about the nitty-gritty of Q-Q plots, knock yourself out here and/or here. The quick-and-dirty interpretation is that we want the points to fall along the diagonal line, because that diagonal line represents a normal distribution.\n\nres &lt;- residuals(m1)\n\nggplot(tibble(residuals(m1)), aes(sample = res))+\n  stat_qq()+\n  stat_qq_line()+\n  ggtitle(\"Q-Q plot of raw residuals\")"
  },
  {
    "objectID": "lessons/transform.html#box-cox-residuals",
    "href": "lessons/transform.html#box-cox-residuals",
    "title": "Transform continuous variables",
    "section": "Box-Cox residuals",
    "text": "Box-Cox residuals\nLet’s fit a linear model with Box-Cox transformed mean reaction times.\n\nm2 &lt;- ELP %&gt;% \n  mutate(bc_rt = (Mean_RT ^ lambda - 1) / lambda) %&gt;% \n  lm(bc_rt ~ Length + log(SUBTLWF) + POS, data = .)\n\n\nMean Box-Cox residual\n\ncat(\"Mean of the residuals = \", mean(residuals(m2)))\n\nMean of the residuals =  -1.570861e-21\n\n\n\n\nHistogram of Box-Cox residuals\n\ntibble(res = residuals(m2)) %&gt;% \n  ggplot(aes(res))+\n  geom_histogram()+\n  ggtitle(\"Histogram of Box-Cox residuals\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nQ-Q plot of Box-Cox residuals\n\nres = residuals(m2)\n\nggplot(tibble(res), aes(sample = res))+\n  stat_qq()+\n  stat_qq_line()+\n  ggtitle(\"Q-Q plot of Box-Cox residuals\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\nMy name is Earl Kjar Brown and I’m a Dane."
  },
  {
    "objectID": "demo.html",
    "href": "demo.html",
    "title": "demo",
    "section": "",
    "text": "Students will learn about Quarto."
  },
  {
    "objectID": "demo.html#objective",
    "href": "demo.html#objective",
    "title": "demo",
    "section": "",
    "text": "Students will learn about Quarto."
  },
  {
    "objectID": "demo.html#lets-get-rollin",
    "href": "demo.html#lets-get-rollin",
    "title": "demo",
    "section": "Let’s get rollin’!",
    "text": "Let’s get rollin’!\nWhat should I write? Here’s an example of some code:\n\n\nCode\nprint(\"hola mundo\")\n\n\n[1] \"hola mundo\"\n\n\nI just said “hello” in Spanish, with the code print(\"hola mundo\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ling_data_analysis",
    "section": "",
    "text": "Website with Earl Kjar Brown’s lesson plans for Linguistic Data Analysis with R. Click on the lesson plans in the navigation menu."
  },
  {
    "objectID": "lessons/webscrape_freq.html",
    "href": "lessons/webscrape_freq.html",
    "title": "Webscrape frequencies",
    "section": "",
    "text": "Students will use R to webscrape an already-created frequency list on the internet."
  },
  {
    "objectID": "lessons/webscrape_freq.html#objective",
    "href": "lessons/webscrape_freq.html#objective",
    "title": "Webscrape frequencies",
    "section": "",
    "text": "Students will use R to webscrape an already-created frequency list on the internet."
  },
  {
    "objectID": "lessons/webscrape_freq.html#overview-of-the-internet",
    "href": "lessons/webscrape_freq.html#overview-of-the-internet",
    "title": "Webscrape frequencies",
    "section": "Overview of the internet",
    "text": "Overview of the internet\nHere’s a super simplified overview of how the internet works: Clients (e.g., computers, smart phones) that are connected to the internet make HTTP requests to web servers (e.g., byu.edu, npr.org, instagram.com, etc.), and those web servers send back HTTP responses. Here’s a diagram to illustrate this basic idea."
  },
  {
    "objectID": "lessons/webscrape_freq.html#r-as-web-browser",
    "href": "lessons/webscrape_freq.html#r-as-web-browser",
    "title": "Webscrape frequencies",
    "section": "R as web browser",
    "text": "R as web browser\nR can act like a web browser by making HTTP requests and receiving HTTP responses from web servers. The rvest package here makes it easy to have R interact with the internet. That package also contains useful functions to parse the HTML in the HTTP response in order to extract information using either CSS selectors or XPath expressions.\nIt’s a good idea, and actually necessary with some websites, to make R look like a normal web browser when making the request to the web server. We do this by setting the user-agent to a common one that many web browsers use:\n\n# set the user-agent to make R look like a normal web browser to the web server\nhttr::set_config(httr::user_agent(\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"))"
  },
  {
    "objectID": "lessons/webscrape_freq.html#frequency-lists-on-the-internet",
    "href": "lessons/webscrape_freq.html#frequency-lists-on-the-internet",
    "title": "Webscrape frequencies",
    "section": "Frequency lists on the internet",
    "text": "Frequency lists on the internet\nThere are many frequency lists on the internet, and the free online dictionary wikitionary makes available a plethora of them here.\nSometimes, the frequency lists are in HTML tables, for example:\n\nThe 1,000 most frequent words in TV and Movie scripts and transcripts in English here.\nThe 1,900 most frequent Hindi words here.\nThe 10,000 most frequency Serbian words based on TV and Movie subtitles here.\n\nOther times, the frequency lists are presented as HTML lists (whether ordered or unordered), for example:\n\nThe 5,000 most frequent words in Danish here.\n1,000 Japanese basic words here.\nThe 2,000 most frequent words in fiction in English here."
  },
  {
    "objectID": "lessons/webscrape_freq.html#html-tables",
    "href": "lessons/webscrape_freq.html#html-tables",
    "title": "Webscrape frequencies",
    "section": "HTML tables",
    "text": "HTML tables\nLet’s webscrape a frequency list that is sitting in an HTML table. Let’s use the list of the 1,000 most frequent words in TV and Movie scripts in English here.\n\n# if need, use install.packages() first to download the following packages to your harddrive\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\nurl &lt;- \"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/TV/2006/1-1000\"\n\n# request the page\npage &lt;- read_html(url)\n\n# get the HTML table holding the frequency list\nfreqs_table &lt;- html_element(page, \"table\") \n\n# convert the HTML table into a data frame (i.e., tibble)\nfreqs_df &lt;- html_table(freqs_table, header = TRUE)\n\n# print the frequency table to the console\nprint(freqs_df)\n\nYou may have noticed that the above code could be put into a single pipeline by using the pipe operator repeatedly:\n\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/TV/2006/1-1000\" %&gt;% \n  read_html() %&gt;% \n  html_element(\"table\") %&gt;% \n  html_table(header = TRUE) %&gt;% \n  print()\n\n\nActivity\nYour turn! Find a frequency list of your choice online that is in an HTML table and webscrape it into a data frame in R. If you’ve having trouble finding one that is in an HTML table, try the 1,900 most frequent Hindi words here or the 10,000 most frequent Serbian words here.\nAfter a good-faith effort, if you need help, see Dr. Brown’s code below:\nHindi\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Hindi_1900\" %&gt;%  # put URL in a string\n  read_html() %&gt;%  # request page\n  html_element(\"table\") %&gt;%  # find first \"table\" HTML element\n  html_table(header = TRUE) %&gt;%  # convert HTML table to data frame (i.e., tibble)\n  print()  # print data frame to console\n\n\nSerbian\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n# extra exercise: write in appropriate comments below\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Serbian_wordlist\" %&gt;% \n  read_html() %&gt;% \n  html_element(\"table\") %&gt;% \n  html_table(header = TRUE) %&gt;% \n  print()"
  },
  {
    "objectID": "lessons/webscrape_freq.html#html-lists",
    "href": "lessons/webscrape_freq.html#html-lists",
    "title": "Webscrape frequencies",
    "section": "HTML lists",
    "text": "HTML lists\nWebscraping HTML lists, whether ordered (i.e., with numbers) or unordered (i.e., with bullets) is more work because we can’t use the slick html_table() function in rvest to convert an HTML table into a data frame in R. Rather, we have to identify which part of the data is the word and which part is the frequency. Further, we have to parse each frequency list separately, as we can’t assume that all frequency lists are formatted the same way.\nLet’s inspect the HTML of the frequency list of the 5,000 most frequency words in Danish in TV and Movie subtitles here. (Seriously, go inspect the HTML of that list before moving on.)\nWe see that the frequency list is an ordered list (HTML tag &lt;ol&gt;) and that each word and its corresponding frequency is in a list item (HTML tag &lt;li&gt;) and that the word and its frequency are separated by a space. We can use this information to scrape out the frequency list, and then use the space to identify the word and the frequency in order to put them into separate columns in a data frame in R.\nPro-tip: SelectorGadget (here) is an absolutely super helpful Google Chrome add-on extension that helps to quick identify the CSS Selector of elements of a webpage.\nLet’s get rolling with the code:\n\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n# put the URL in a string\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Danish_wordlist\" %&gt;% \n  \n  # request the HTML page\n  read_html() %&gt;% \n  \n  # extract the HTML elements &lt;li&gt; that are children of HTML elements &lt;ol&gt;\n  html_elements(css = \"ol li\") %&gt;% \n  \n  # keep only the text, that is, remove the HTML tags\n  html_text() %&gt;% \n  \n  # create a data frame with one column named \"both\"\n  # note: the dot below pipes the result of the previous step into where the dot is (rather than at the beginning of the function, which is the default behavior of the pipe operator)\n  tibble(both = .) %&gt;% \n  \n  # separate the \"both\" column into two columns (i.e., \"wd\" and \"freq\") on the space between the word and the frequency\n  separate_wider_delim(cols = \"both\", delim = \" \", names = c(\"wd\", \"freq\")) %&gt;% \n  \n  # print the data frame to the console\n  print()\n\n\nActivity\nIt’s that time of the class period: your turn!\nFind a frequency (or rank) list of your choice on the internet (probably at wikitionary.org) that is not in an HTML table and parse it into a data frame in R. If you need help finding one, try the 2,000 most frequent words in fiction in English here or the 1,000 Japanese basic words here.\nAfter a good-faith effort, if you need some help, see Dr. Brown’s code below:\nEnglish\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Contemporary_fiction\" %&gt;% \n  read_html() %&gt;% \n  html_elements(css = \"ol li\") %&gt;% \n  html_text() %&gt;% \n  tibble(wd = .) %&gt;% \n  print()\n\n\nJapanese\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\"https://en.wiktionary.org/wiki/Appendix:1000_Japanese_basic_words\" %&gt;% \n  read_html() %&gt;% \n  html_elements(css = \".mw-parser-output &gt; ul li\") %&gt;% \n  html_text() %&gt;% \n  tibble(entry = .) %&gt;% \n  print()"
  },
  {
    "objectID": "lessons/webscrape_freq.html#question",
    "href": "lessons/webscrape_freq.html#question",
    "title": "Webscrape frequencies",
    "section": "Question",
    "text": "Question\nIn most of the code in this lesson, the frequency or rank lists were simply printed to the console. How could we modify the code so that instead of printing to the console, we write the lists out to CSV files, one list per CSV file?"
  },
  {
    "objectID": "lessons/regexes.html",
    "href": "lessons/regexes.html",
    "title": "Regular expressions",
    "section": "",
    "text": "Students will become proficient with writing regular expressions, including with capture groups and lookaround."
  },
  {
    "objectID": "lessons/regexes.html#objective",
    "href": "lessons/regexes.html#objective",
    "title": "Regular expressions",
    "section": "",
    "text": "Students will become proficient with writing regular expressions, including with capture groups and lookaround."
  },
  {
    "objectID": "lessons/regexes.html#regular-expressions-aka.-regexes",
    "href": "lessons/regexes.html#regular-expressions-aka.-regexes",
    "title": "Regular expressions",
    "section": "Regular expressions (aka. regexes)",
    "text": "Regular expressions (aka. regexes)\n\nRegular expressions are used to match strings. \n\nNote: In R, you must use double backslashes, e.g., \\\\w+\n\nOnline regex checker are useful, such as here (for Python), here, and here.\nLetters represent themselves: \"ed\" returns ed anywhere in the string, for example, Ed studied in the education building."
  },
  {
    "objectID": "lessons/regexes.html#character-classes",
    "href": "lessons/regexes.html#character-classes",
    "title": "Regular expressions",
    "section": "Character classes",
    "text": "Character classes\n\n\\\\w = alphanumeric character; \\\\W = non-alphanumeric character\n\\\\s = whitespace (i.e., spaces, tab breaks, newlines); \\\\S = non-whitespace\n\\\\d = Arabic numeral (i.e., 0-9); \\\\D = non-Arabic numeral\n[] = character class finds one of the characters between the square brackets: \n\n[aeiou] finds one of the five orthographic vowels\n[Aa] find either uppercase or lowercase a \n[a-z] finds one lowercase English character\n[a-zA-Z] returns one lowercase English character or one uppercase English character \nExample: \"latin[aox]\" returns latina, latino, latinx."
  },
  {
    "objectID": "lessons/regexes.html#the-pipe-which-is-just-above-the-return-key-on-my-keyboard-is-an-or-operator",
    "href": "lessons/regexes.html#the-pipe-which-is-just-above-the-return-key-on-my-keyboard-is-an-or-operator",
    "title": "Regular expressions",
    "section": "| (the “pipe” which is just above the return key on my keyboard) is an “or” operator:  ",
    "text": "| (the “pipe” which is just above the return key on my keyboard) is an “or” operator:  \n\nExample: \"\\\\bth(is|at|ese|ose) \\\\w+\" returns an English demonstrative determiner followed by a space, followed by a contiguous span or one or more of alphanumeric character, for example, this bag, that cat, these plants, those buildings.\nQuantifiers\n\n{min, max} = returns between min and max number of the previous character: \"\\\\w{2,5}\" returns between two and five alphanumeric characters. Note that \"\\\\w{,5}\" returns up to five alphanumeric characters, and \"\\\\w{2,}\" finds two or more alphanumeric characters.\n{integer} = returns the exact number of the previous character: \"\\\\d{4}\" returns exactly four Arabic numerals (for example, to find four-digit years in a text or corpus)\nShortcut quantifiers:\n\n? means the same as {0, 1}, meaning it returns zero or one of the previous pattern, that is, the previous character is optional\n* is the same as {0,} and returns zero or more of the previous pattern: yes\\\\!* returns yes, followed by any number of exclamation points, including none at all: yes, yes!, yes!!!, etc.. \n+ means {1,} and returns one or more of the previous pattern, for example, \"go+l\" returns gol, goool, gooooooool\n\n\n\n\nActivity\n\nWhat do the following regexes match? See example in Section 2.1 here.\n\n\"\\\\b[Tt]he\\\\b\\\\s+\\\\b[Ii]nternet\\\\b\"\n\"\\\\w+ed\\\\b\"\n\"\\\\bcent(er|re)\\\\b\"\n\"\\\\bwalk(s|ed|ing)?\\\\b\"\n\"\\\\b[^aeiou\\\\W]{2,}\\\\w+\"\n\"\\\\b[^aeiou\\\\W][aeiou]\\\\w+\""
  },
  {
    "objectID": "lessons/regexes.html#capture-groups",
    "href": "lessons/regexes.html#capture-groups",
    "title": "Regular expressions",
    "section": "Capture groups",
    "text": "Capture groups\n\nWarning: This gets wild. \nYou can have a regular expression remember what it captured in order to search for that same sequence of characters.\nYou can encapsulate a pattern in parentheses to capture, and then refer to that same sequence of characters with \\\\1 for the first capture group, or \\\\2 for the second capture group (if you have more than one capture group in the same regex), etc.\nExample: \"\\\\w+(\\\\w) \\\\1\\\\w+\" returns a bigram whose first word ends with the same letter that the second word begins with, e.g., walked down\n\n\nActivity\n\nWhat do the following regexes match?\n\n\"([aeiou])\\\\1\"\n\"\\\\w*([aeiou])\\\\1\\\\w*\"\n\"the (\\\\w+)er they were, the \\\\1er they will be\"\n\"[Tt]he (\\\\w+)er they (\\\\w+),? the \\\\1er we \\\\2\"\n\"\\\\w+(\\\\w{2,})\\\\W+\\\\w+\\\\1\\\\b\""
  },
  {
    "objectID": "lessons/regexes.html#lookaround",
    "href": "lessons/regexes.html#lookaround",
    "title": "Regular expressions",
    "section": "Lookaround",
    "text": "Lookaround\n\nLookaround allows you to use surrounding characters to find other characters, but to not consume those surrounding characters.\n\nSee lookahead examples in Section 2.1.7 here.\n\n\n\nActivity\n\nDownload at least several TXT files of your choice (perhaps from Project Gutenberg or Saints from the LMS).\nLoop over the files and search for a regex of your choice with a capture group, and print to screen the results. Use several different regex functions from the stringr package, for example, str_match_all(), str_extract_all(), str_locate_all().\nCreate a tabular dataset of your choice with a regex of your choice. As a first step, you might simple create a data frame with two columns: filename, and regex match.\nRamp it up by creating more columns, perhaps the number of characters in the match, or the number of (orthographic) vowels in the match, etc."
  },
  {
    "objectID": "lessons/wrangling.html",
    "href": "lessons/wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Students will manipulate or wrangle a data frame."
  },
  {
    "objectID": "lessons/wrangling.html#objective",
    "href": "lessons/wrangling.html#objective",
    "title": "Data Wrangling",
    "section": "",
    "text": "Students will manipulate or wrangle a data frame."
  },
  {
    "objectID": "lessons/wrangling.html#the-dplyr-package",
    "href": "lessons/wrangling.html#the-dplyr-package",
    "title": "Data Wrangling",
    "section": "The dplyr package",
    "text": "The dplyr package\nThe dplyr R package is a core package of tidyverse, and has a handful of super useful functions for manipulating or wrangling a data frame. Most of the functions are verbs (e.g., filter(), select(), etc.) and so the documentation for the package often refers to the main functions of dplyr as verbs."
  },
  {
    "objectID": "lessons/wrangling.html#rows",
    "href": "lessons/wrangling.html#rows",
    "title": "Data Wrangling",
    "section": "Rows",
    "text": "Rows\nThere are three (I guess four) main functions (aka. verbs) for working with rows in dplyr: filter(), arrange() (and it’s helper function desc()), and distinct().\nPop (formative) quiz!\nInstructions: Match each function with its corresponding purpose.\n\n\n\n\n\n\n\nfilter()\nA. orders the rows based on values in one or more columns\n\n\narrange()\nB. inverts the order so that the rows are in big-to-small order\n\n\ndesc()\nC. keeps only unique rows\n\n\ndistinct()\nD. keeps rows that evaluate to TRUE in a conditional statement based on one or more columns\n\n\n\n\nActivity\nLet’s follow the examples given in Chapter 3 “Data transformation” of the book R for Data Science (2e). First, let’s download the nycflights13 R package to our harddrive:\n\ninstall.packages(\"nycflights13\", repos = \"https://cran.rstudio.com\")\n\nNow, let’s load the nycflights13 data frame into our R session or script, and our ol’ friend tidyverse:\n\nlibrary(\"nycflights13\")\nlibrary(\"tidyverse\")\n\nFinally, and most importantly, let’s try to solve the exercises in section 3.2.5 here. After some good-faith work, if you need some help, take a look at Dr. Brown’s code below:\n“Had an arrival delay of two or more hours”\n\n\nCode\nflights |&gt; \n  filter(arr_delay &gt;= 120)\n\n\n“Flew to Houston (IAH or HOU)”\n\n\nCode\nflights |&gt;  \n  filter(dest %in% c(\"IAH\", \"HOU\"))\n\n\n“Were operated by United, American, or Delta”\n\n\nCode\nflights |&gt;  \n  filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n\n“Departed in summer (July, August, and September)”\n\n\nCode\nflights |&gt;  \n  filter(month %in% c(7, 8, 9))  # note: the month column has a data type of integer, and therefore you don't need quotes around the numbers\n\n\nHere’s another way to filter for only summer flights:\n\n\nCode\nflights |&gt;  \n  filter(month &gt;= 7 & month &lt;= 9)\n\n\n“Arrived more than two hours late, but didn’t leave late”\n\n\nCode\nflights |&gt;  \n  filter(arr_delay &gt; 120 & dep_delay &lt;= 0)\n\n\n“Were delayed by at least an hour, but made up over 30 minutes in flight”\n\n\nCode\nflights |&gt;  \n  filter(dep_delay &gt;= 60 & arr_delay &lt; 30)\n\n\n“Sort flights to find the flights with longest departure delays.”\n\n\nCode\nflights |&gt;  \n  arrange(desc(dep_delay))\n\n\n“Find the flights that left earliest in the morning.”\n\n\nCode\nflights |&gt;  \n  arrange(dep_time)\n\n\n“Sort flights to find the fastest flights. (Hint: Try including a math calculation inside of your function.)”\n\n\nCode\nflights |&gt;  \n  arrange(desc(distance / air_time))\n\n\nHere’s another way, and a preview of the mutate function that we’ll see below:\n\n\nCode\nflights |&gt;  \n  mutate(speed = distance / air_time) %&gt;% \n  arrange(desc(speed))\n\n\n“Was there a flight on every day of 2013?”\n\n\nCode\nflights |&gt;  \n  distinct(year, month, day)\n\n\n“Which flights traveled the farthest distance?”\n\n\nCode\nflights |&gt;  \n  arrange(desc(distance))\n\n\n“Which traveled the least distance?”\n\n\nCode\nflights |&gt;  \n  arrange(distance)\n\n\n“Does it matter what order you used filter() and arrange() if you’re using both? Why/why not? Think about the results and how much work the functions would have to do.”\nEarl speculates that it’s probably best to filter first so that arrange has fewer rows to sort."
  },
  {
    "objectID": "lessons/wrangling.html#columns",
    "href": "lessons/wrangling.html#columns",
    "title": "Data Wrangling",
    "section": "Columns",
    "text": "Columns\nThere are four main functions (i.e., verbs) for working with columns: mutate(), select() (and its handful of helper functions here), rename(), and relocate().\nPop quiz time!\nInstructions: Match each function (i.e., verb) with its corresponding purpose.\n\n\n\n\n\n\n\nmutate()\nA. change the name of the specified column\n\n\nselect()\nB. creates new columns based on one or more already existing columns\n\n\nrename()\nC. moves the position of the specified columns\n\n\nrelocate()\nD. keeps only the specified columns\n\n\n\nLet’s try out the exercises provided in 3.3.5 of Chapter 3 “Data transformation” here. After a good-faith effort, if you need take a look at Dr. Brown’s code below each exercise.\n“Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?”\n“Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights”\n\n\nCode\nflights |&gt; \n  select(\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\")\n\n\n\n\nCode\nflights |&gt; \n  select(matches(\"^(dep_|arr_)\"))\n\n\n“Does the result of running the following code surprise you? How do the select helpers deal with upper and lower case by default? How can you change that default?”\n\n\nCode\nflights |&gt; \n  select(contains(\"TIME\"))\n\n\n“Rename air_time to air_time_min to indicate units of measurement and move it to the beginning of the data frame.”\n\n\nCode\nflights |&gt; \n  rename(air_time_min = air_time) |&gt; \n  relocate(air_time_min, .before = 1)\n\n\n“Why doesn’t the following work, and what does the error mean?”\n\nflights |&gt; \n  select(tailnum) |&gt; \n  arrange(arr_delay)\n\n\nActivity\nNow for some linguistic data. Download the “data_FRC_spch_rate.xlsx” Excel file from the Datasets module in the LMS (here). This dataset was used to write the article in the journal Corpus Linguistics and Linguistic Theory here.\nOpen it up in Excel (or Google Sheets) and inspect the columns in the data sheet and read what type of data each column holds in the legend sheet.\nNow, read in the data sheet into R as a data frame (probably a tibble). Take some time to practice using the functions (i.e., verbs) that work with rows and the functions that work with columns.\n\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\nsetwd(\"/pathway/to/dir\")\nsibilants &lt;- read_excel(\"data_FRC_spch_rate.xlsx\", sheet = \"data\")\n\nThen, try the following exercises. After a good-faith effort to complete these exercises on your own, if you need help take a look at Dr. Brown’s code.\n\nKeep only the rows with tokens of /s/ that were maintained as a sibilant.\n\n\nCode\nsibilants |&gt; \n  filter(s == \"maintained\")\n\n\nKeep only the rows of tokens of /s/ that were deleted (no sibilance) in word final position.\n\n\nCode\nsibilants |&gt; \n  filter(s == \"deleted\", wd_pos == \"wd_final\")\n\n\nCreate an alphabetized list of unique words with /s/ at the beginning of the word and following by high vowels.\n\n\nCode\nsibilants |&gt; \n  filter(wd_pos == \"wd_initial\", sound_post == \"HiV\") |&gt; \n  distinct(word) |&gt; \n  arrange(word)\n\n\nSort the data frame in descending order by lexical frequency, and then in ascending order by word.\n\n\nCode\nsibilants |&gt; \n  arrange(desc(lex_freq), word)\n\n\nKeep only rows with tokens with a speech rate of between 8 and 12 segments per second. Hint: The data type of the column with speech rate (i.e., spch_rate) may not have been read in as a numeric (aka. float) value. So, first you may need to coerce that column to a numeric data type with the base R function as.numeric().\n\n\nCode\nsibilants |&gt; \n  mutate(spch_rate = as.numeric(spch_rate)) |&gt; \n  filter(spch_rate &gt;= 8 & spch_rate &lt;= 12)\n\n\nCreate an alphabetized list of unique words with /s/ in which /s/ occurred in a tonic syllable.\n\n\nCode\nsibilants |&gt; \n  filter(stress == \"tonic\") |&gt; \n  distinct(word) |&gt; \n  arrange(word)"
  },
  {
    "objectID": "lessons/wrangling.html#group-by",
    "href": "lessons/wrangling.html#group-by",
    "title": "Data Wrangling",
    "section": "Group by",
    "text": "Group by\nAnother super useful function in dplyr is group_up(). It doesn’t change the data frame itself, but it performs subsequent functions based on the levels of a column or columns. It is often used in conjugation with the summarize() function to get group-level information.\nAs an example, let’s first calculate mean departure delay of all flights in the nycflights13 dataset, then next, let’s calculate the mean departure delay by airline (i.e., carrier column). However, as a preprocessing step, we need to remove rows what don’t have a number in dep_delay, but rather an NA, because the mean() function errs out with missing values. There are two ways to do this: (1) use filter() to keep only rows that don’t have an NA in the dep_delay column, or (2) tell the mean() function to remove the NAs with the na.rm argument.\nHere’s how to use filter() for that purpose:\n\nlibrary(\"nycflights13\")\nlibrary(\"tidyverse\")\n\nflights |&gt; \n  filter(!is.na(dep_delay)) |&gt;  # mind the exclamation point\n  summarize(average_departure_delay = mean(dep_delay))\n\nAnd here we use the na.rm argument within the mean() function:\n\nflights |&gt; \n  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE))\n\nSo, the mean departure delay across all airlines is 12.6 minutes. The natural follow-up question is whether some airlines have longer delays than others. Enter our new friend group_by(). Now, let’s perform this mean operation based on the levels (aka. values) of the carrier column in order to get the mean departure delay time by airline:\n\nflights |&gt; \n  group_by(carrier) |&gt; # we group before getting the mean\n  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE))\n\nMicro-activity: Modify the previous code block to sort the resulting data frame so that the airline with the shortest average delay is at the top of the resulting data frame and the airline with the longest average delay is at the bottom. After a good-faith effort, if you need some help, take a look at Dr. Brown’s code.\n\n\nCode\nflights |&gt; \n  group_by(carrier) |&gt; \n  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE)) |&gt; \n  arrange(average_departure_delay)\n\n\nIf you need to group by several columns, just add the additional column names to the group_by() call, for example:\n\nflights |&gt; \n  group_by(carrier, origin) |&gt; \n  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE))\n\nYou don’t have to use group_by() only with summarize(), but it’s a common use case. And, importantly, after summarize() has finished its work, it ungroups the data frame. If you were to use group_by() with other functions, they don’t automatically ungroup the data frame, and you would have explicitly do so with the ungroup() function. Here’s an example:\n\nflights |&gt; \n  group_by(carrier) |&gt; \n  mutate(aver_dep_delay_airline = mean(dep_delay, na.rm = TRUE)) |&gt; \n  select(carrier, dep_delay, aver_dep_delay_airline)\n\nNotice that the output says Groups: carrier [16]. This means that the data frame is still grouped by the 16 unique levels (aka. values) in the carrier column. In order to ungroup it, we can call ungroup() after we don’t need the groups anymore:\n\nflights |&gt; \n  group_by(carrier) |&gt; \n  mutate(aver_dep_delay_airline = mean(dep_delay, na.rm = TRUE)) |&gt; \n  select(carrier, dep_delay, aver_dep_delay_airline) |&gt; \n  ungroup()\n\n\nActivity\nUse the data sheet within the data_FRC_spch_rate.xlsx Excel workbook to get familiar with the group_by() and summarize().\n\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\nsetwd(\"/pathway/to/dir/\")\nsibilants &lt;- read_excel(\"data_FRC_spch_rate.xlsx\", sheet = \"data\")\n\nPerhaps you might try getting the mean and/or median speech rate (i.e., column name spch_rate) based on whether the /s/ was maintained as a sibilant or deleted (using the column s). Note: You’ll need to remove rows in the data frame that have NaN in spch_rate and then you need to convert spch_rate to a numeric data type. You can do both of these preprocessing steps right within the pipeline before getting the central tendency measures, if you want. If you get stuck, take a look at Dr. Brown’s code below.\n\n\nCode\nsibilants |&gt; \n  filter(spch_rate != \"NaN\") |&gt;  # more rows with \"NaN\"\n  mutate(spch_rate = as.numeric(spch_rate)) |&gt;  # convert data type from character to numeric\n  group_by(s) |&gt; \n  summarize(\n    mean_spch_rate = mean(spch_rate),\n    median_spch_rate = median(spch_rate))\n\n\nActivity: Try some other analyses with group_by() and summarize() and be prepared to share with a neighbor and/or the class."
  },
  {
    "objectID": "lessons/wrangling.html#joining-data-frames",
    "href": "lessons/wrangling.html#joining-data-frames",
    "title": "Data Wrangling",
    "section": "Joining data frames",
    "text": "Joining data frames\nJoining or merging two tables into one is often a necessary step in data analysis, including in linguistic data analysis. For example, let’s say we have one data frame (perhaps coming from an Excel file) that has tokens of sounds in words, and we have another data frame (perhaps from a CSV file) with frequencies of words from a large corpus. It would be slick to get the frequencies of word from the frequency data frame and put the appropriate frequency next to the words in the sound data frame. Enter the family of join functions in dplyr here.\nLet’s start with a super simple example in order to understand the concept, then we’ll apply this to bigger datasets. Let’s create a data frame (i.e., tibble) with two columns: a person, and the fruit that they said during a conversation:\n\nlibrary(tidyverse)\nmain_df &lt;- tibble(\n  speaker = c(\"Bob\", \"Bob\", \"Billy\", \"Billy\", \"Britta\", \"Britta\", \"Bonnie\"), \n  word = c(\"apple\", \"banana\", \"orange\", \"mango\", \"apple\", \"manzana\", \"kiwi\")\n)\nprint(main_df)\n\nNow, let’s create a data frame with some of the speakers’ ages:\n\nages &lt;- tibble(\n  person = c(\"Bonnie\", \"Billy\", \"Bob\"), \n  age = c(47, 6, 95)\n)\nprint(ages)\n\nYou may have noticed that while Britta said a couple words in our main_df data frame, she isn’t listed in the ages data frame. We’ll see what happens below when a value in a column in the main data frame doesn’t have a corresponding value in the to-be-joined data frame.\nIn order to create a new column with the age of each of the speakers in main_df, we can use the left_join() function to join main_df and ages. This function returns every row in the left-hand or first data frame in the function call (which, if we’re using a pipe operator, is the data frame to the left of the pipe), and only the rows in the right-hand or second data frame that have a corresponding value based on the columns specified with the by argument. By default, left_join joins by common column names, but we can explicitly specify which column in the left-hand data frame corresponds to which column in the right-hand data frame with the by argument. Let’s take a look:\n\nmain_df |&gt; \n  left_join(ages, by = join_by(speaker == person))\n\nQuestion: Why do we have speaker to the left of the double equal sign and person to the right?\nYou probably noticed that Britta’s age is NA because she isn’t listed in the ages data frame. If that’s a deal-breaker, then we can remove rows with NAs with filter() as a final step in the pipeline (mind the exclamation point before is.na()):\n\nmain_df |&gt; \n  left_join(ages, by = join_by(speaker == person)) |&gt; \n  filter(!is.na(age))\n\n\nToy activity\nLet’s create a toy data frame with frequencies of the words said by the speakers in main_df:\n\nfreqs &lt;- tibble(\n  wd = c(\"apple\", \"banana\", \"kiwi\", \"mango\"), \n  freq = c(123, 234, 345, 456)\n)\nprint(freqs)\n\nNow, join the freqs data frame to the main_df so that the frequency of each word is in a new column to the right of the column holding the age of the speakers, so that you end up with a data frame with four columns: speaker, word, age, freq. Give it a try, but if you get stuck, take a look at Dr. Brown’s code below. Hint: Make sure you are super aware of the column names in the various data frames that you need to join, so that you can give the correct names to the by argument.\n\n\nCode\nmain_df |&gt; \n  left_join(ages, by = join_by(speaker == person)) |&gt; \n  left_join(freqs, by = join_by(word == wd))\n\n\n\n\nActivity (with a review)\nLet’s ramp it up to a real activity. Try the following:\n\nReview: Create a data frame with at least one column named word (i.e., that has one word per row) using text files of your choice (e.g., Saints or texts from Project Gutenberg). You might like to use a regular expression to find different words (e.g., \"\\\\w+ed\\\\b\" or \"\\\\bre\\\\w+\"), as the next steps will be boring if all rows have the same word.\nReview: Create a data frame of frequencies of words from text files of your choice or webscrape a frequency list from the internet (e.g., Wikitionary).\nNote: If after a good-faith effort to complete the previous two steps, you can’t remember how to do these tasks, download and use two files in the LMS labeled Dataset_for_over_dan.csv and freqs_dan.csv. The first file has a keyword-in-context display of words in Danish that start with for or over while the second file has frequencies of words in Danish.\nHere’s the main exercise of the activity: Join the data frame with words to the data frame with frequencies, so that you have a new column (called freq) with the frequency of the word in the word (or node) column. Again, I can’t stress enough the importance of taking the time to double check that you know what the names are of the columns in the several data frames, so that you can correctly pass them to the by argument."
  },
  {
    "objectID": "lessons/wrangling.html#separate-and-unite-columns",
    "href": "lessons/wrangling.html#separate-and-unite-columns",
    "title": "Data Wrangling",
    "section": "Separate and unite columns",
    "text": "Separate and unite columns\nOther useful functions, this time from the tidyr package within tidyverse, are the 3-member family of separate_* functions (i.e., separate_wider_delim(), separate_wider_position(), and separate_wider_regex()) and the unite() function.\nThe separate_* functions are transparently named and split up one column into two or more columns.\n\nThe separate_wider_delim() function splits on a delimiter given to the delim argument;\nThe separate_wider_position()function splits at fixed character widths given to the widths argument;\nThe separate_wider_regex() function splits on a regular expression given to the patterns argument.\n\nLet’s create a data frame with one column with two pieces of information in each row: the speaker who said something, following by a semi-colon, followed by what the speaker said:\n\nlibrary(tidyverse)\nhungry &lt;- tibble(\n  conversation = c(\"Bobby: I'm hungry!\", \"Cathy: Let's stop at In-n-Out. What do you say to that?!\", \"Bobby: Sounds good to me. What do you say João?\", \"João: Eu digo que sim!\", \"Roberto: Yo también.\")\n)\nprint(hungry)\n\nLet’s start with separate_wider_delim(). Pay attention to the arguments in the function.\n\nhungry |&gt; \n  separate_wider_delim(cols = conversation, delim = \": \", names = c(\"person\", \"utterance\"))\n\nThe function separate_wider_regex() takes a different approach. Rather than splitting on the given fixed-width delimiter given to the delim argument in the previous function, this function takes regular expressions that are used to extract matches out of the string in the row. Specifically, you give it a named character vector in which the names become column names and the elements of the vector are regular expressions to be matched. If no name is given, then that match doesn’t make it into the resulting data frame. Let’s use this function on the same hungry data frame from above. Notice that the regex \": \" doesn’t have a name, and therefore isn’t returned in the resulting data frame.\n\nhungry |&gt; \n  separate_wider_regex(cols = conversation, patterns = c(person = \"^[^:]+\", \": \", utterance = \"[^:]+$\"))\n\nDr. Brown hasn’t needed separate_wider_position() yet in his life, so let’s move on! If you think you might need it, take a look at the docs here.\nThe unite() function is the inverse of the separate_* functions, that is, it merges into one column two or more columns. It takes was input the data frame (probably piped in with the pipe operator), the col argument which specifies the name of the new column, then the names of the two or more columns to be merged, and the sep argument which specifies the character(s) used to join the values in the new column. There are other arguments; take a look at the docs here.\nLet’s continue with our hungry data frame from above. Let’s put the person and utterance columns back together, but this time with a dash separating the speakers and their corresponding utterances.\n\ndf_separated &lt;- hungry |&gt; \n  separate_wider_delim(cols = conversation, delim = \": \", names = c(\"person\", \"utterance\"))\nprint(df_separated)\n\ndf_separated |&gt; \n  unite(col = \"combined\", person:utterance, sep = \" - \")\n\n\nActivity\nUse some of the separate_*() functions and unite() on the data frame of your choice."
  },
  {
    "objectID": "lessons/wrangling.html#pivoting-data-frames",
    "href": "lessons/wrangling.html#pivoting-data-frames",
    "title": "Data Wrangling",
    "section": "Pivoting data frames",
    "text": "Pivoting data frames\nAnother useful, and often necessary, step in (linguistic) data analysis is changing the shape of a data frame before running an analysis. Some functions in R and in other languages (e.g., Python and Julia) and in other software (e.g., Excel) require the data frame to be in a specific shape in order to do certain analyses. Being able to transform the data to that shape is an important preprocessing step.\nThe two main functions for changing the shape of a data frame within tidyverse are pivot_longer() and pivot_wider(). The function pivot_longer() increases the number of rows and decreases the number of columns, while pivot_wider() decreases the number of rows and increases the number of columns.\nLet’s take a super simple example. Let’s create a toy data frame with one row and five columns with the ages of people in a sample:\n\nlibrary(\"tidyverse\")\nages &lt;- tibble(Sammy = 20, Edna = 18, Luisa = 16, Heidi = 14, Evelyn = 12)\nprint(ages)\n\nAs we see, each column represents one person, with the age in the row. Let’s reshape the data frame so that we end up with five rows and two columns, with each row giving info about one person, and the first column giving the person’s name and the second column giving the person’s age:\n\nages |&gt; \n  pivot_longer(cols = Sammy:Evelyn, names_to = \"person\", values_to = \"age\")\n\nThe resulting data frame is a “tidy” data frame because each column has only one variable (e.g., person or age) and the rows represent individual observations. The pre-transformed data frame above was not tidy because each column had more than one variable (e.g., person and age). Read Hadley Wickham’s paper for a full explanation of tidy data.\nThe pivot_wider() does the opposite, that is, it make a long(er) data frame wide(r). Let’s create another toy data frame, this time in long format and make it wide:\n\nheights &lt;- tibble(\n  person = c(\"Sammy\", \"Edna\", \"Luisa\", \"Heidi\", \"Evelyn\"),\n  height_in = c(72, 70, 69, 63, 64))\nprint(heights)\n\nLet’s pivot this data frame to a wide format so that there are five columns, with each person’s name was the column name, and the rows are the corresponding heights in inches:\n\nheights |&gt; \n  pivot_wider(names_from = person, values_from = height_in)\n\n\nToy activity and review\nCreate one data frame with the ages and heights of the five people in the above toy examples. Hint: In addition to pivot_longer() and/or pivot_wider(), you’ll also need our ol’ friend left_join() that we saw above.\nAfter a good-faith effort, if you need help take a look at Dr. Brown’s code below:\n\n\nCode\nages |&gt; \n  pivot_longer(cols = Sammy:Evelyn, names_to = \"person\", values_to = \"age\") |&gt; \n  left_join(heights, by = join_by(person == person))\n\n\n\n\nVowels\nLet’s take a look at a linguistic example from Dr. Stanley. Download the sample_vowel_data.csv file from the LMS &gt; Datasets. Currently, each row represents one time point for each vowel. There are 27 rows containing F1, F2, and F3 measurements from three tokens of three vowels each, at three timepoints.\nLet’s load the dataset into our R session as a data frame:\n\nlibrary(\"tidyverse\")\nvowels &lt;- read_csv(\"/pathway/to/sample_vowel_data.csv\")\nprint(vowels)\n\nLet’s change it into a wider format so that the unit of observation (i.e., row) is one vowel. There are 9 rows. Also, save it to a variable named widest so we can work with it later.\n\nwidest &lt;- vowels |&gt; \n  pivot_wider(names_from = percent, values_from = c(F1, F2, F3))\nprint(widest)\n\nLet’s change it to a longer format. Now the unit of measurement is one format measurement, per time point, per vowel token. There are 81 rows. Note that I’ll need to specify which columns should be affected with the cols argument.\n\nlongest &lt;- vowels |&gt; \n    pivot_longer(cols = c(F1, F2, F3), \n                 names_to = \"formant\", \n                 values_to = \"hz\") |&gt; \n    arrange(formant)\nprint(longest)\n\nLet’s use our widest data frame and pivot it longer. Also, as a shorthand for specifying the columns I want to pivot, I can use matches() and a regex:\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = \"formant_percent\", \n                 values_to = \"hz\")\n\nHere, because each column name contained two pieces of information (F1_25), the resulting data set after pivoting contains a single column with two pieces of information: formant and percent.\nEnter our ol’ friend separate():\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = \"formant_percent\", \n                 values_to = \"hz\") |&gt; \n    separate(formant_percent, \n             into = c(\"formant\", \"percent\"), \n             sep = \"_\")\n\nBut the separation above can happen within pivot functions directly. Below, we provide a vector of names as the value of the names_to argument. This then requires the use of the names_sep argument, which is where you specify the character that separates the two names.\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = c(\"formant\", \"percent\"), \n                 names_sep = \"_\", \n                 values_to = \"hz\")\n\n\n\nRamping it up\nLet’s say we start off with what we see as the widest version of the data in the widest data frame. This is not a hypothetical: this is how a lot of sociophonetics software returns the data to the user! We want to pivot it so that it looks like our original version in the vowels data frame. One way is to make it longer and then wider again.\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = c(\"formant\", \"percent\"),\n                 names_sep = \"_\",\n                 values_to = \"hz\") |&gt; \n    pivot_wider(names_from = formant,\n                values_from = hz)\n\nThis produces the correct output, but it’s a little clunky to do two pivots.\nEnter the black magic that is .value. So instead, we replace the formant column name (in the names_to argument) with .value (mind the period). This then, somehow, takes what would have been the formant column and pivots it wider. Note that when we do this, we don’t need the values_from argument anymore.\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = c(\".value\", \"percent\"),\n                 names_sep = \"_\")\n\n\n\nBig badboy activity\nDownload the “RPM_2019_for_Reliability.xlsx” Excel file from LMS &gt; Datasets module and transform (aka. transpose) the dataset into the shape asked for in the email below that Dr. Brown received from his brother Alan:\n“I’m sending along that data set that I had originally sent that had those problems. Stayc cleaned it all up and we took some students out who spoke other languages at home. The sheet of interest is the first one labeled “For Earl”. As we had talked about, we just need the data oriented horizontally by item number so across the top the columns would run from 1 to 36 left to right. Each row, then, would represent a different student and ’1’s or ’0’s in the columns would indicate correct or incorrect for each item/column. The initial three columns to the left would be the demographic information like gender, grade, immersion/non-immersion. Let me know if that doesn’t make sense.”\nThe first ten rows (i.e., students, of the 104 students) of the transposed dataset should look like the following:\n\n\n\nTransposed dataset\n\n\nA couple words to the wise:\n\nAs is often the case in the wild, this dataset is squirrelly. Take a look at the bottom of the dataset (in Excel) and you’ll see that there is “104” below the last row of the first column. You’ll need to use the range argument in the read_excel() function (within the readxl package) to read in only the cells with actual student data, that is, rows with something in all five columns.\nThe “104” indicates that there are 104 students in the dataset. The answers to the 36 questions that each student responded to are in sets of 36 rows, with the question number indicated by in the Page/Item column. You’ll see that the numbers in that column go from 1 to 36, and then start over at 1 with the next student.\nThere is no unique identifier (e.g., ID number) for each student. Before transposing the data frame, you’ll need to create a new column with a unique identifier. You can use the rep() function, with its each argument, to do this. See Stack Overflow thread for an example.\nAfter transposing the data frame, you should move the column with the unique identifier to the far left.\n\nAfter a good-faith effort to complete this activity, if you need help take a look at Dr. Brown’s code below:\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"readxl\")\n\"/pathway/to/RPM_2019_for_Reliability.xlsx\" |&gt; \n  read_excel(range = \"For Earl!A1:E3745\") |&gt; \n  mutate(id = rep(1:104, each = 36)) |&gt; \n  pivot_wider(names_from = \"Page/Item\", values_from = \"Correct/incorrect\") |&gt; \n  relocate(id, .before = 1)"
  },
  {
    "objectID": "lessons/wrangling.html#working-with-categorical-variables",
    "href": "lessons/wrangling.html#working-with-categorical-variables",
    "title": "Data Wrangling",
    "section": "Working with categorical variables",
    "text": "Working with categorical variables\nThere are a few handy functions in the forcats package within tidyverse for working with categorical variables (aka. factors).\nThe fct_recode() function allows the user to manually change or combine levels (aka. values) of a categorical variable. Let’s create a toy data frame with words and vowels:\n\nlibrary(\"tidyverse\")\nwords &lt;- tibble(\n  word = c(\"wasp\", \"fleece\", \"beat\", \"bit\", \"pat\", \"bus\", \"hot\", \"cool\", \"firm\", \"father\"),\n  vowel = c(\"[ɑ]\", \"[i]\", \"[i]\", \"[ɪ]\", \"[æ]\", \"[ʌ]\", \"[ɔ]\", \"[u]\", \"[ɚ]\", \"[ɑ]\")\n)\nprint(words)\n\nNow, let’s create a new column in which we describe (some of) the vowels:\n\nwords |&gt; \n  mutate(description = fct_recode(vowel, \"high front closed\" = \"[i]\", \"r colored schwa\" = \"[ɚ]\", \"stressed schwa (wedge)\" = \"[ʌ]\"))\n\nThe fct_collapse() function allows the user to put levels into groups:\n\nwords |&gt; \n  mutate(height = fct_collapse(vowel, \n                               hi_V = c(\"[i]\", \"[ɪ]\", \"[u]\"), \n                               lo_V = c(\"[ɑ]\", \"[æ]\"),\n                               other_level = \"mid_V\"))\n\nQuestion: What does the other_level argument do in the function call above?\nThe fct_relevel() function allows the user to reorder the levels of a categorical variable. This is often useful when plotting data and the user wants an order of levels different from alphabetical order (the default).\nLet’s create a toy data frame of voiceless plosives and their voice onset times (VOT):\n\nlibrary(tidyverse)\nvoiceless_plosives &lt;- tibble(\n  plosive = c(\"t\", \"k\", \"p\", \"p\", \"k\", \"t\", \"p\", \"k\", \"t\"),\n  vot = c(23, 34, 45, 56, 67, 78, 89, 91, 21)\n)\nprint(voiceless_plosives)\n\nIf we create a boxplot (preview of what’s coming soon!), we see that “k” is on the left, followed by “p” and “t”, because the default way to sort levels is by alphabetical order:\n\nvoiceless_plosives |&gt; \n  ggplot(aes(x = plosive, y = vot)) +\n  geom_boxplot()\n\nWe probably want “p” followed by “t” and then “k”, as that’s how they are presented in linguistic literature, based on the place of articulation. Let’s use fct_relevel() to change the internal order of the levels, and then replot their VOTs:\n\nvoiceless_plosives |&gt;\n  mutate(plosive = fct_relevel(plosive, \"p\", \"t\", \"k\")) |&gt; \n  ggplot(aes(x = plosive, y = vot)) +\n  geom_boxplot()\n\n\nActivity\nLet’s create a toy data frame with Spanish words with a dental or alveolar word-final sibilant:\n\nlibrary(\"tidyverse\")\nsibilants &lt;- tibble(\n  person = c(\"Raúl\", \"Raúl\", \"José\", \"José\", \"María\"),\n  target_wd = c(\"árboles\", \"mesas\", \"lápiz\", \"es\", \"pues\"),\n  next_wd = c(\"de\", \"en\", \"y\", \"que\", \".\"),\n  next_segment = c(\"d\", \"e\", \"i\", \"k\", \"#\")\n)\nprint(sibilants)\n\nYour task is to create a new column that groups the vowels into one level, the consonants into their own group, and then all other following segments should be placed into an other level.\nAfter a good-faith effort, if you need help, take a look at Dr. Brown’s code below:\n\n\nCode\nsibilants |&gt;  \n  mutate(next_sound_type = fct_collapse(next_segment, vowel = c(\"i\",\"e\"), consonant = c(\"d\", \"k\"), other_level = \"other\"))"
  },
  {
    "objectID": "lessons/anova.html",
    "href": "lessons/anova.html",
    "title": "ANOVA",
    "section": "",
    "text": "Students perform an ANOVA analysis and interpret the results."
  },
  {
    "objectID": "lessons/anova.html#whats-in-a-name",
    "href": "lessons/anova.html#whats-in-a-name",
    "title": "ANOVA",
    "section": "What’s in a name?",
    "text": "What’s in a name?\nANOVA stands for Analysis of Variance. It’s very much like the t-test and Wilcoxon test, but with three or more levels (aka. groups) in the categorical variable.\n\n\n\n\n\n\n\nt-test or Wilcoxon test\nANOVA\n\n\nold v. young\nold v. middle age v. young\n\n\nwomen v. men\nwomen v. men. v. kids\n\n\nMandarin v. Cantonese\nMandarin v. Cantonese v. Wu v. Hokkien v. Gan v. Hunanese\n\n\n\nAn ANOVA calculates an F statistic which, at a high level is calculated with the following formula:\n\\[\n\\frac{variation\\ between\\ groups}{variation\\ within\\ groups}\n\\]\nThe hypotheses are:\n\\[\nH_0:μ_1=μ_2=μ_3...=μ_n\n\\]\n\\[\nH_a:At\\ least\\ one\\ of\\ the\\ groups\\ is\\ different\\ from\\ the\\ others.\n\\]\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"joeysvowels\")\nfront_vowels &lt;- joeysvowels::midpoints %&gt;%\n  filter(vowel %in% c(\"TRAP\", \"FACE\", \"DRESS\", \"FLEECE\", \"KIT\")) %&gt;% \n  mutate(dur = end - start)\nfront_vowels %&gt;% \n  ggplot(aes(x = vowel, y = dur))+\n  geom_boxplot(notch = TRUE)\n\nNotch went outside hinges\nℹ Do you want `notch = FALSE`?\n\n\n\n\n\nThose notches are trying to tell a story, amirite?!\nLet’s see what an ANOVA says:\n\nfront_vowels %&gt;% \n  aov(dur~vowel, data = .) -&gt; result\nsummary(result)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nvowel         4 0.2875 0.07188   31.23 &lt;2e-16 ***\nResiduals   211 0.4856 0.00230                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOkay, the ANOVA feels that there is something goin’ on here. Just look at that p-value well below 0.05. But, the immediate follow-up question is which groups are significantly different from other groups.\nEnter the post-hoc test. Actually, there are quite a few post-hoc test, but we’ll use the Tukey Honest Significant Difference post-hoc test (because that’s what Dr. Brown sees a lot in publications):\n\nTukeyHSD(result)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = dur ~ vowel, data = .)\n\n$vowel\n                     diff         lwr          upr     p adj\nFACE-TRAP    -0.016746918 -0.04303958  0.009545744 0.4042499\nDRESS-TRAP   -0.076731562 -0.10441226 -0.049050863 0.0000000\nFLEECE-TRAP  -0.022437110 -0.05725583  0.012381610 0.3920587\nKIT-TRAP     -0.084665180 -0.10918029 -0.060150069 0.0000000\nDRESS-FACE   -0.059984644 -0.08942775 -0.030541539 0.0000006\nFLEECE-FACE  -0.005690191 -0.04192579  0.030545407 0.9927010\nKIT-FACE     -0.067918262 -0.09440724 -0.041429282 0.0000000\nFLEECE-DRESS  0.054294452  0.01703945  0.091549460 0.0007958\nKIT-DRESS    -0.007933618 -0.03580086  0.019933621 0.9352192\nKIT-FLEECE   -0.062228070 -0.09719527 -0.027260869 0.0000190\n\n\nSimple enough. The p-value tell of which pairwise comparisons are statistically significantly different from each other."
  },
  {
    "objectID": "lessons/anova.html#activity",
    "href": "lessons/anova.html#activity",
    "title": "ANOVA",
    "section": "Activity",
    "text": "Activity\nYour turn! Find a dataset of your choice and load it up, and find a continuous variable and a categorical variable with at least three levels, and … you guessed, run an ANOVA."
  },
  {
    "objectID": "lessons/quarto.html",
    "href": "lessons/quarto.html",
    "title": "Quarto documents",
    "section": "",
    "text": "Students will write a Quarto document that integrates prose and code, and the output of the code."
  },
  {
    "objectID": "lessons/quarto.html#objective",
    "href": "lessons/quarto.html#objective",
    "title": "Quarto documents",
    "section": "",
    "text": "Students will write a Quarto document that integrates prose and code, and the output of the code."
  },
  {
    "objectID": "lessons/quarto.html#reproducible-research",
    "href": "lessons/quarto.html#reproducible-research",
    "title": "Quarto documents",
    "section": "Reproducible research",
    "text": "Reproducible research\nClearing laying out methods is an important consideration when publishing original research so that others can reproduce or replicate your research. One useful way to do that is to make available the computer code that goes into the data collection and data analysis parts of the research process.\nEnter documents that knit together code and prose. The current cool-kid-on-the-block is the Quarto document. All the lesson plans that Dr. Brown has created have been produced as Quarto documents. The RStudio IDE makes it super easy to write Quarto document and integrate R code (and Python and Julia code too!) with prose around the code. Also, the writer of the document can specify whether the output of the code is rendered. For example, Dr. Brown usually does not render the output of the R code."
  },
  {
    "objectID": "lessons/quarto.html#getting-started",
    "href": "lessons/quarto.html#getting-started",
    "title": "Quarto documents",
    "section": "Getting started",
    "text": "Getting started\nSteps:\n\nWithin RStudio, click on File &gt; New File &gt; Quarto Document…\nGive the document a name\nStart writing prose and/or code!"
  },
  {
    "objectID": "lessons/quarto.html#choose-your-own-adventure",
    "href": "lessons/quarto.html#choose-your-own-adventure",
    "title": "Quarto documents",
    "section": "Choose your own adventure",
    "text": "Choose your own adventure\n…Or jump between adventures, as needed.\nWithin RStudio, you can use the Source editor or the Visual editor, or jump back and forth as needed. For example, Dr. Brown uses the Visual editor the vast majority of the time, but occasionally jumps over to the Source editor in order to have more control over formatting.\nIn order to insert a code block on a new line, you can type a forward slash “/”, and a drop-down menu will appear from which you can select the type of code you want to insert (most commonly R code). There’s also a keyboard short to insert an R code block: Mac &gt; Cmd + Opt + i; Windows &gt; Ctrl + Alt + i."
  },
  {
    "objectID": "lessons/quarto.html#markdown-syntax",
    "href": "lessons/quarto.html#markdown-syntax",
    "title": "Quarto documents",
    "section": "Markdown syntax",
    "text": "Markdown syntax\nQuarto documents accept Markdown syntax to format the prose. It provides the basics of word processing documents, like different levels of headers, bold font, italics, underlined, etc. Importantly and usefully, it also provides a way to format code within the prose, for example, print(\"hello world\") returns hello world.\n\nActivity\nTake a look at the two cheatsheets that RStudio has about Markdown syntax (Help &gt; Cheat Sheets &gt; R Markdown Cheat Sheet & R Markdown Reference Guide) or websites about Markdown, for example, markdownguide.org.\nNow, create a Quarto document and use the Source editor only to create a (simple) HTML output document.\nNext, create another Quarto document, but this time, use the Visual editor as much as possible.\nBe prepared to share your documents with the members of the class."
  },
  {
    "objectID": "lessons/mixed-effects-logistic.html",
    "href": "lessons/mixed-effects-logistic.html",
    "title": "Mixed-effects logistic regression",
    "section": "",
    "text": "Students fit a mixed-effects logistic regression model."
  },
  {
    "objectID": "lessons/mixed-effects-logistic.html#objective",
    "href": "lessons/mixed-effects-logistic.html#objective",
    "title": "Mixed-effects logistic regression",
    "section": "",
    "text": "Students fit a mixed-effects logistic regression model."
  },
  {
    "objectID": "lessons/mixed-effects-logistic.html#logistic-regression-with-random-effects",
    "href": "lessons/mixed-effects-logistic.html#logistic-regression-with-random-effects",
    "title": "Mixed-effects logistic regression",
    "section": "Logistic regression with random effects",
    "text": "Logistic regression with random effects\nAs we saw in a previous lesson, random effects control for the variability in the response variable that comes from the individual members of a population that we happen to get into our sample. The lme4 R package has a glmer() function that can be used to fit a mixed-effects logistic regression when you specify `family = “binomial”`.\nLet’s take a look at a dataset used to write an article about the effect of fast speech on word durations and the durations of the /s/ segment in a sample of Colombian Spanish. It’s named “Dataset_fast_spch_Colombia.xlsx” and is in the LMS.\n\nsuppressPackageStartupMessages(library(\"tidyverse\"))\nsuppressPackageStartupMessages(library(\"lme4\"))\nlibrary(\"readxl\")\n\ncolombia &lt;- read_excel(\"/Users/ekb5/Documents/LING_440/datasets/Dataset_fast_spch_Colombia.xlsx\", sheet = \"dataset\")\n\n# make the response variable a factor\ncolombia &lt;- colombia %&gt;% \n  mutate(pronunciation = as.factor(pronunciation))\n\nIn order to make sure we understand what the coefficients in the Estimate column mean in the regression results below, let’s print out the levels of the response variable:\n\nlevels(colombia$pronunciation)\n\n[1] \"deleted\"    \"maintained\"\n\n\nSo, because the second level (i.e., maintained) is compared to the first (i.e., deleted), positive coefficients in the Estimate column mean that maintained is favored, while negative coefficients in that column mean that deleted is favored.\nLet’s fit a mixed-effects logistic regression model!\n\n### generalized linear regression (aka. logistic regression)\nin_formula &lt;- formula(pronunciation ~ \n            (1 | file) + (1 | wd) + # random effects (intercepts only)\n            frc + # fixed effects\n            spch_rate +\n            wd_freq +\n            pre_sound +\n            wd_len +\n            position)\n\nm1 &lt;- lme4::glmer(formula = in_formula, data = colombia, family = \"binomial\", control=glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e4)))\n\nsummary(m1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: pronunciation ~ (1 | file) + (1 | wd) + frc + spch_rate + wd_freq +  \n    pre_sound + wd_len + position\n   Data: colombia\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 20000))\n\n     AIC      BIC   logLik deviance df.resid \n  1090.6   1147.5   -534.3   1068.6     1293 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0170  0.0796  0.2026  0.5487  2.9414 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n wd     (Intercept) 0.78330  0.8850  \n file   (Intercept) 0.05723  0.2392  \nNumber of obs: 1304, groups:  wd, 159; file, 8\n\nFixed effects:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)            -1.15826    0.61119  -1.895  0.05808 .  \nfrc                     0.99993    0.33039   3.027  0.00247 ** \nspch_rate              -0.02254    0.08113  -0.278  0.78116    \nwd_freq                 0.10580    0.05335   1.983  0.04735 *  \npre_soundpause          0.67143    1.07899   0.622  0.53376    \nwd_len                 -0.09870    0.08373  -1.179  0.23848    \npositionwd_ini          2.57585    0.46539   5.535 3.11e-08 ***\npositionwd_med_syl_fin  1.93652    0.44308   4.371 1.24e-05 ***\npositionwd_med_syl_ini  2.47641    0.35902   6.898 5.29e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n                (Intr) frc    spch_r wd_frq pr_snd wd_len pstnw_\nfrc             -0.388                                          \nspch_rate       -0.045  0.044                                   \nwd_freq         -0.580 -0.167  0.039                            \npre_soundps      0.002  0.015 -0.007 -0.014                     \nwd_len          -0.135  0.103 -0.057 -0.572  0.005              \npositinwd_n     -0.284 -0.022 -0.011  0.123 -0.184  0.109       \npstnwd_md_syl_f -0.105 -0.067  0.017  0.152 -0.005 -0.167  0.144\npstnwd_md_syl_n -0.069 -0.093 -0.012  0.035 -0.006 -0.028  0.181\n                pstnwd_md_syl_f\nfrc                            \nspch_rate                      \nwd_freq                        \npre_soundps                    \nwd_len                         \npositinwd_n                    \npstnwd_md_syl_f                \npstnwd_md_syl_n  0.179         \n\nperformance::r2_nakagawa(m1)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.491\n     Marginal R2: 0.361"
  },
  {
    "objectID": "lessons/mixed-effects-logistic.html#activity",
    "href": "lessons/mixed-effects-logistic.html#activity",
    "title": "Mixed-effects logistic regression",
    "section": "Activity",
    "text": "Activity\nRun the above code to fit a mixed-effects logistic regression."
  },
  {
    "objectID": "lessons/visualization.html",
    "href": "lessons/visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "Students visualize data with easy-to-interpret plots."
  },
  {
    "objectID": "lessons/visualization.html#objective",
    "href": "lessons/visualization.html#objective",
    "title": "Visualization",
    "section": "",
    "text": "Students visualize data with easy-to-interpret plots."
  },
  {
    "objectID": "lessons/visualization.html#background",
    "href": "lessons/visualization.html#background",
    "title": "Visualization",
    "section": "Background",
    "text": "Background\nR kicks trash with visualizing data. And the go-to package for creating professional-looking plots is ggplot2, one of the core packages of the tidyverse ecosystem. Base R also creates plots, but anybody who’s anybody is using ggplot2. (Okay, maybe I’m overstating this a bit, but just you wait and see if I’m lying.)"
  },
  {
    "objectID": "lessons/visualization.html#basics",
    "href": "lessons/visualization.html#basics",
    "title": "Visualization",
    "section": "Basics",
    "text": "Basics\nFirst, we need some data in a data frame (e.g., a tibble). Let’s use the Data_ptk.xlsx file in the LMS:\n\nlibrary(\"tidyverse\")\nlibrary(\"readxl\")\n\nptk &lt;- read_excel(\"../../../data_analysis/datasets/Data_ptk.xlsx\", sheet = \"data\")\nglimpse(ptk)\nhead(ptk)\n\nThe function ggplot() (without a 2, like in the package name) creates a plotting window or palette:\n\nggplot()\n\nThe first argument to ggplot() is a data frame, which can be piped in:\n\nggplot(ptk)\n\n# same as above, but with a pipe\nptk |&gt; ggplot()\n\nThe second argument to ggplot() is a call to the aes() function (short for “aesthetics”) which maps columns in the data frame to elements on the plot. The first argument to aes() is what will be plotted on the x-axis and the second argument, if needed, is what will be plotted on the y-axis. For example, let’s put the levels of LANG on the x-axis:\n\nptk |&gt; \n  ggplot(aes(x = LANG))\n\nOr we can put the prePhonBin on the y-axis:\n\nptk |&gt; \n  ggplot(aes(y = prePhonBin))\n\nYou’ll notice that the above plots don’t actually plot anything yet; they simply set up the plotting window with the right axis labels. We now need to add elements to the plot."
  },
  {
    "objectID": "lessons/visualization.html#one-categorical-variable",
    "href": "lessons/visualization.html#one-categorical-variable",
    "title": "Visualization",
    "section": "One categorical variable",
    "text": "One categorical variable\nPerhaps the most basic plot is of one categorical variable. In order to see the distribution of the levels (aka. values or groups) of a categorical variable, let’s create a popular option: the good ol’ fashioned barplot. Note: The several layers of the plot are separated by a plus sign + rather than the pipe operator, and the layers are often specified with a geom_...() function (short for geometric object). See the section about layers in the reference manual of ggplot2.\n\nptk |&gt; \n  ggplot(aes(x = LANG)) +\n  geom_bar()\n\nIt might be good to also get the exact number of tokens in each level with our ol’ friend:\n\nptk |&gt; \n  count(LANG)\n\n\nActivity\n\nTake a look at the variety of barplots that are available in ggplot.\nThen, using the Data_ptk.xlsx dataset, plot the distribution of several categorical variables, for example: LANG, GENRE, prePhonBin, folPhonBin, prevMention, cogStatus, lexClass, wdClass."
  },
  {
    "objectID": "lessons/visualization.html#two-categorical-variables",
    "href": "lessons/visualization.html#two-categorical-variables",
    "title": "Visualization",
    "section": "Two categorical variables",
    "text": "Two categorical variables\nThe following are popular plots to visualize the distribution of data points across two categorical variables.\n\nStacked barplot\nTo create a stacked (aka. filled) barplot, one categorical variable is given as x in aes() and the other is given as the fill argument in aes(), for example:\n\nptk |&gt; \n  ggplot(aes(x = LANG, fill = prePhonBin)) +\n  geom_bar()\n\nSee more examples of barplot in the docs.\nFollow-up question: How can we get the exact numbers of tokens in each of the subgroups displayed in the plot? After a good-faith effort, if you need help, take a look at Dr. Brown’s code below:\n\n\nCode\nptk |&gt; \n  count(LANG, prePhonBin)\n\n\n\n\nPercent barplot\nTo create a percent barplot, that is, a stacked barplot that sums to 1.0, let’s modify the previous barplot by adding the argument position = \"fill\" to geom_bar(), but outside of aes() (which isn’t used at all).\n\nptk |&gt; \n  ggplot(aes(x = LANG, fill=prePhonBin)) +\n  geom_bar(position = \"fill\")\n\nWe notice that the y-axis is labeled “count”, but those aren’t counts, they’re proportions. Let’s change the axis labels with the labs() element:\n\nptk |&gt; \n  ggplot(aes(x = LANG, fill=prePhonBin)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Language\", y = \"Proportion\")\n\n\n\nGrouped barplot\nTo create a grouped barplot, modify the previous barplot by changing the argument to position = \"dodge\". Simple enough.\n\nptk |&gt; \n  ggplot(aes(x = LANG, fill = prePhonBin)) +\n  geom_bar(position = \"dodge\")\n\n\n\nMosaic plot\nAnother useful plot for two categorical variables is a mosaic plot, which displays proportions as relative sizes of the various pieces of the mosaic. Let’s download an R package that can be used to create mosaic plots: vcd has a transparently named package name: Visualizing Categorical Data.\nLet’s plot prePhonBin on the x-axis and LANG on the y-axis:\n\nlibrary(\"vcd\")\n\ntemp &lt;- ptk |&gt; \n  count(LANG, prePhonBin) |&gt; \n  pivot_wider(names_from = prePhonBin, values_from = n)\nrow_names &lt;- temp |&gt; pull(1)\ntemp &lt;- temp |&gt; data.matrix()\ntemp &lt;- temp[,-1]\nrownames(temp) &lt;- row_names\nvcd::mosaic(temp, shade = TRUE, varnames = FALSE)\n\n\n\nActivity\n\nGive it a whirl! Using the Data_ptk.xlsx dataset, plot the distribution of sets of two categorical variables, e.g., LANG and GENRE, prePhonBin and folPhonBin, prevMention and cogStatus, lexClass and wdClass, and any combination of these variables."
  },
  {
    "objectID": "lessons/visualization.html#one-continuous-variable",
    "href": "lessons/visualization.html#one-continuous-variable",
    "title": "Visualization",
    "section": "One continuous variable",
    "text": "One continuous variable\nIt is useful to visualize the distribution of continuous variables in order to see whether the variable is distributed normally or has some skew or is bimodal.\nThe following plots are useful for this purpose. You only need to specify the x argument in aes() within ggplot() with the one variable.\n\nHistogram\nA histogram slices or bins of a continuous variable into N bins and then plots the count within each bin on the y-axis. It gives a barplot (i.e., it looks like a barplot), but instead of levels within a categorical variable, the bars are equidistant intervals across a continuous variable. Here’s an example with the continuous variable VOT in the ptk dataset:\n\nptk |&gt; \n  ggplot(aes(x = VOT)) +\n  geom_histogram()\n\nYou can specify a specific number of bins with the bins argument, for example:\n\nptk |&gt; \n  ggplot(aes(x = VOT)) +\n  geom_histogram(bins = 10)\n\nInstead of the number of bins (i.e., bars), you can specify the width of each bin on the scale of the variable that you’re plotting. This is useful because the bin width is often more meaningful than a simple number of bins. Let’s create a bin width of 0.01 seconds (aka. 10 milliseconds):\n\nptk |&gt; \n  ggplot(aes(x = VOT)) +\n  geom_histogram(binwidth = 0.01)\n\n\n\nDensity plot\nDensity plots are like histograms, but they use a curved line rather than bins:\n\nptk |&gt; \n  ggplot(aes(VOT)) +\n  geom_density()\n\nThe cool thing about density plots is that we can specify that the area below the curve sum to 1. That fact will be important when we look at p-values.\nLet’s plot VOT as a density plot:\n\nptk |&gt; \n  ggplot(aes(scale(VOT))) +\n  geom_density(fill = \"blue\")\n\n\n\nActivity\nGive it a go! Plot the distribution of several different continuous variables in the ptk dataset, and be ready to share with a neighbor and/or the class."
  },
  {
    "objectID": "lessons/visualization.html#two-continuous-variables",
    "href": "lessons/visualization.html#two-continuous-variables",
    "title": "Visualization",
    "section": "Two continuous variables",
    "text": "Two continuous variables\nEnter the mighty scatterplot!\nLet’s take a look at Dr. Joey Stanley’s vowels, literally (well, as literally as we can).\nMany thanks are expressed (this is a performative passive verb) to Joey for his data and expertise with plotting vowels. Check out this webpage on Data Visualization.\nFirst, let’s download a package to allow us to download R packages from github, and then download the package with his vowels:\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"joeystanley/joeysvowels\")\n\nLet’s load a dataset called coronals and and inspect it to see what data are included and how they are organized:\n\nlibrary(\"tidyverse\")\nmidpoints &lt;- joeysvowels::coronals\nglimpse(midpoints)\nhead(midpoints)\n\nLet’s only use the middle of each vowel and exclude diphthongs. Notice that we save the result of the pipeline back to the same variable name:\n\nmidpoints &lt;- midpoints|&gt; \n    filter(percent == 50)  |&gt; \n    select(-percent) |&gt; \n    filter(!vowel %in% c(\"PRICE\", \"MOUTH\", \"CHOICE\"))\n\nLet’s plot the first two formants of his vowels. In vowel plots, the norm is to plot F2 on the x-axis because it deals with vowel backness (i.e., depth in the mouth) and F1 on the y-axis because it deals with vowel height. Let’s add a geom_point() layer:\n\nmidpoints |&gt; \n  ggplot(aes(x = F2, y = F1)) +\n  geom_point()\n\nIn order to put the points in the same position that they are within the mouth, let’s reverse the scale of both the x-axis and y-axis:\n\nmidpoints |&gt; \n  ggplot(aes(x = F2, y = F1)) +\n  geom_point() +\n  scale_x_reverse() +\n  scale_y_reverse()\n\nThat’s nice and all, but… how about knowing which dot corresponds to which vowel. Amirite?! Let’s go! We can do so by using the color argument in the aes() call within the ggplot() function:\n\nmidpoints |&gt; \n  ggplot(aes(x = F2, y = F1, color = vowel)) +\n  geom_point() +\n  scale_x_reverse() +\n  scale_y_reverse()\n\nThe gray background of ggplot may not be as nice as white when the data points (aka. observations) are colored. Let’s fix it (you guys/y’all/youse know by now that you can customize to your heart’s delight in R, right?):\n\nmidpoints |&gt; \n  ggplot(aes(x = F2, y = F1, color = vowel)) +\n  geom_point() +\n  scale_x_reverse() +\n  scale_y_reverse() +\n  theme_bw()\n\nIf you plan to put this plot in a publication that doesn’t support color, you can use the shape argument instead:\n\nmidpoints |&gt; \n  ggplot(aes(x = F2, y = F1, shape = vowel)) +\n  geom_point() +\n  scale_x_reverse() +\n  scale_y_reverse() +\n  theme_bw()\n\nBut, there’s a problem: R only gives you six shapes by default, so… keep that in mind if you use shape instead of color. An obvious workaround is to filter only several vowels before piping the data frame into ggplot(), for example:\n\nmidpoints |&gt; \n  filter(vowel %in% c(\"LOT\", \"THOUGHT\")) |&gt; \n  ggplot(aes(F2, F1, color = vowel, shape = vowel)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse() +\n  theme_bw()\n\nWe can add more information to the plot with the diameter and/or opacity of the dots. It usually only make sense to vary the diameter or opacity of the dots with a continuous variable.\nLet’s make a continuous variable of duration of the vowel within our pipeline, before passing our data frame into ggplot(). and then specify with the size argument that the diameter of the dots should be proportional to the duration of the vowel that the dot represents:\n\nmidpoints |&gt; \n  mutate(duration = end - start) |&gt; \n  ggplot(aes(x = F2, y = F1, color = vowel, size = duration))+\n  geom_point()+\n  scale_x_reverse()+\n  scale_y_reverse()+\n  theme_bw()\n\nThe above plot is nice, but the overplotting (i.e., dots on top of each other) makes it a bit hard to tell where there are many dots.\nAnother way to put a fourth variable in the plot is with the level of transparency of the dots so that the clusters of dots that are on top of each other are easier to see. Rather than the size argument, let’s use the alpha argument:\n\nmidpoints |&gt; \n  mutate(duration = end - start) |&gt; \n  ggplot(aes(x = F2, y = F1, color = vowel, alpha = duration))+\n  geom_point()+\n  scale_x_reverse()+\n  scale_y_reverse()+\n  theme_bw()\n\nWe can also plot the duration of the vowel as the size of the dots:\n\nmidpoints |&gt; \n  mutate(duration = end - start) |&gt; \n  ggplot(aes(F2, F1, color = vowel, size = duration)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  theme_bw()\n\nIf the dots are too big, we can constrain the range of their sizes:\n\nmidpoints |&gt; \n  mutate(duration = end - start) |&gt; \n  ggplot(aes(F2, F1, color = vowel, size = duration)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_size_continuous(range = c(3, 0.25))+\n  theme_bw()\n\n\nRegression line\nIf you’d like to see how one continuous variable affects the other continuous variable, if at all, plotting a regression line (aka. trend line) is useful. This is a straight line that shows the overall trend through the points. This is done with another layer: geom_smooth(method = lm). The lm part of this line refer to a linear model.\nLet’s go back to our ptk dataset and put a regression line through the points off two continuous variables: VOT and COG.\n\nptk |&gt; \n  ggplot(aes(x = COG, y = VOT))+\n  geom_point()+ # plot individual data points\n  geom_smooth(method = lm)+ # regression line\n  theme_bw() # make it black and white\n\n\n\nLOESS line\nAnother option to show the trend through the data points of two continuous variables is a LOESS (locally estimated scatterplot smoothing) line. This is a locally based smoother line that shows the local trend of the dots closest to it. Let’s modify our plot above by removing the regression line and putting in a loess line (yeah, I’m gonna use lowercase from here on out):\n\nptk |&gt; \n  ggplot(aes(x = COG, y = VOT))+\n  geom_point()+\n  geom_smooth(method = loess)+ \n  theme_bw() \n\nWe can see that the loess line shows that there is a positive relationship between COG (center of gravity) of the friction and the VOT (voice onset time) of the voiceless stops, but that the effect weakens as COG increases (because the loess line becomes flatter).\nOf course, we can include both a regression line and a loess line in the same plot. And if we do that, it’s probably wise to change the color of one of them and perhaps remove the 95% standard error band around the loess line:\n\nptk |&gt; \n  ggplot(aes(x = COG, y = VOT))+\n  geom_point()+\n  geom_smooth(method = lm)+\n  geom_smooth(method = loess, se = FALSE, color = \"red\")+ \n  theme_bw()"
  },
  {
    "objectID": "lessons/visualization.html#one-continuous-variable-and-one-categorical-variable",
    "href": "lessons/visualization.html#one-continuous-variable-and-one-categorical-variable",
    "title": "Visualization",
    "section": "One continuous variable and one categorical variable",
    "text": "One continuous variable and one categorical variable\nAnother common situation is when we have one continuous variable and one categorical variable and we want to see how they affect each other, if at all.\n\nBoxplot\nThe go-to plot for this situation is the mighty boxplot (aka. box and whisker plot).\nLet’s go back to Joey’s vowels. But to make it more manageable for pedagogical purposes, let’s filter the vowels so that we only have front vowels, and also create a duration column while we’re at it:\n\nfront_durs &lt;- midpoints |&gt; \n    filter(vowel %in% c(\"FLEECE\", \"KIT\", \"FACE\", \"DRESS\", \"TRAP\")) |&gt; \n    mutate(duration = end - start)\n\nWith a boxplot, we need to specify the categorical variable on the x-axis and the continuous variable on the y-axis:\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot()+\n  theme_bw()\n\nBoxplots are useful (and better than barplots of the means of a continuous variable) because they show both central tendency (i.e., mean and median) and dispersion or spread around the central tendency.\nHere’s how to interpret a boxplot:\n\nThe box shows the middle 50% of the data points, that is, from the first quartile (aka. Q1), which is the 25th percentile, to the third quartile (aka. Q3), which is the 75th percentile;\nThe horizontal line within the box is the median (i.e., 50th percentile);\nThe lines that extend above and below the boxes are called whiskers, and they extend 1.5 times the interquartile range (aka. IQR), which is simply Q3 - Q1, from the box, that is, the upper whisker extend 1.5 IQR above Q3, while the lower whisker extends 1.5 IQR below Q1;\nOutliers above or below the whiskers are plotted individually as dots.\n\nBy default, geom_boxplot() doesn’t plot the mean, but it’s easy to add with another layer. See this tutorial for the possible shapes; here we’ll choose an “x” with shape=4:\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot()+\n  stat_summary(fun.y=mean, shape=4)+\n  theme_bw()\n\nAnother feature of boxplots in ggplot is that you can add notches to the box, which gives visual information about whether the levels might be statistically significantly different from each other (but a real statistical test that gives a p-value should also be run to assert statistical significance). If the notches of two boxes don’t overlap vertically, this is prima facie evidence that they are likely statistically significantly different from each other.\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot(notch = TRUE)+\n  stat_summary(fun.y=mean, shape=4)+\n  theme_bw()\n\nAs a good follow-up, let’s do an ANOVA (Analysis of Variance) test, and if the ANOVA returns a significant p-value, let’s run a post-doc Tukey Honest Significant Difference test to see if the p-values given for the pairwise comparisons coincides with the notches.\n\nresults1 &lt;- aov(duration ~ vowel, data = front_durs)\nsummary(results1)\n\nYep, something statistically significant is going on here. Next step is a post-hoc test to which pairwise comparisons are significant:\n\nTukeyHSD(results1)\n\nYep, the p-values indicate that the notches were rightly suggestive of statistical significance.\n\n\nViolin plot\nAnother plot for one continuous variable and one categorical variable is the violin plot. This plot is a combination of a boxplot and a density plot. As with the boxplot above, let’s add the mean as an “x”. Here we go:\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_violin()+\n  stat_summary(fun.y=mean, shape=4)+\n  theme_bw()\n\n\n\nAdding observations\nHopefully, you’re getting the idea that there are many possibilities when creating plots with R. In fact, we can mix and match, and add additional layers with additional info. For example, we can add individual observations (aka. tokens) to either the boxplot or the violin plot. Let’s take a look:\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_violin()+\n  geom_point()+\n  theme_bw()\n\nThe geom_point() function adds the data points in a straight, vertical line, which can make it difficult to see exactly how many dots there when they are clustered together. It might be better to either move horizontally a bit, or to make them slightly transparent. In order to offset them a little bit, instead of geom_point(), we can use the geom_jitter(). And, importantly, we probably want to make sure the dots don’t move vertically at all, and only move a little bit horizontally. We can specify limits for the directions with the width and height arguments in geom_jitter():\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_violin()+\n  geom_jitter(width = 0.1, height = 0)+\n  theme_bw()\n\nThe order of the layers is meaningful, that is, elements of the plot will change based on where they are in the pipeline. For example, what happens if we put the geom_jitter() higher up in the pipeline?\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_jitter(width = 0.1, height = 0)+\n  geom_violin()+\n  theme_bw()\n\nWe can also plot individual observations on a boxplot, but it can get confusing because the outliers in a boxplot are plotted individually by geom_boxplot(), but also plotted by geom_jitter() (a bit little to the left or right):\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot()+\n  geom_jitter(width = 0.1, height = 0)+\n  theme_bw()\n\nSo, to fix that, we make the outliers that geom_boxplot() plots be fully transparent:\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot(outlier.alpha = 0)+\n  geom_jitter(width = 0.1, height = 0)+\n  theme_bw()\n\nAnyone for a boxplot and a violin together? Questions: Which one should be overlaid on the other? Should the one in front be slightly transparent so that the one behind it can be seen?\n\n\nActivity\nTake some time to experiment with several options (e.g., violin behind boxplot, and vice versa) and several different levels of transparency (with the alpha argument). If you need some help, take a look at Dr. Brown’s code below:\n\n\nCode\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot()+\n  geom_violin(alpha = 0.3)+\n  stat_summary(fun=mean, shape=4)+\n  theme_bw()\n\n\n\n\nViolin-dot plot\nAnother possibility is a violin-dot plot that is available in the see package. First, let’s download that package to our harddrive:\n\ninstall.packages(\"see\", repos = \"http://cran.rstudio.com\")\n\nYou can play with the dots_size and binwidth arguments to get the dots just right.\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  see::geom_violindot(dots_size = 0.8, binwidth = 0.005)+\n  theme_bw()\n\n\n\nRug\nAnother way add individual observations to a plot with is the geom_rug() function. It puts the tokens on the edge of the plotting area, so it’s really only useful with you have exactly levels in the categorical variable. Let’s\n\nfront_durs |&gt; \n  filter(vowel %in% c(\"TRAP\", \"KIT\")) |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot()+\n  geom_rug(data = front_durs |&gt; filter(vowel == \"TRAP\"), sides = \"left\")+\n  geom_rug(data = front_durs |&gt; filter(vowel == \"KIT\"), sides = \"right\")+\n  theme_bw()\n\n\n\nActivity\nMake two plots to visualize each of the following:\n\nThe effect, if any, of the previous segment on the duration of vowels;\nThe effect, if any, of the following segment on the duration of vowels.\n\nDecide which type of plots you’d like (e.g., boxplot with or without notches, violin plot, boxplot and violin plot, with or without individual observations, etc.).\nAfter a good-faith effort, if you need some help take a look at Dr. Brown’s code below:\n\n\nCode\nfront_durs |&gt; \n  ggplot(aes(x = pre, y = duration))+\n  geom_boxplot(notch = TRUE)+\n  geom_violin(alpha = 0.5)+\n  stat_summary(fun=mean, shape=4)+\n  theme_bw()\n\nfront_durs |&gt; \n  ggplot(aes(x = fol, y = duration))+\n  geom_boxplot(notch = TRUE)+\n  geom_violin(alpha = 0.5)+\n  stat_summary(fun=mean, shape=4)+\n  theme_bw()"
  },
  {
    "objectID": "lessons/visualization.html#facetting",
    "href": "lessons/visualization.html#facetting",
    "title": "Visualization",
    "section": "Facetting",
    "text": "Facetting\nWe can one or two variables to our plot by making an individual plot for each the levels of a categorical variable or two. Let’s go back to our histograms above. When we plotted the histogram of all vowels, we didn’t know how the various vowels differ with respect to the distribution of their durations. There are two main facetting layers (that Dr. Brown will show here): facet_wrap() and facet_grid().\nLet’s use the midpoints dataset from above and create a new column with the duration of the vowel, and save it to a new data frame named midpoints_durs:\n\nmidpoints_durs &lt;- midpoints |&gt; \n  mutate(duration = end - start)\n\n\nfacet_wrap()\nNow, let’s create a single plot with subplots with a histogram, one histogram per vowel:\n\nmidpoints_durs |&gt; \n  ggplot(aes(x = duration))+\n  geom_histogram(binwidth = 0.01)+\n  facet_wrap(~vowel)+\n  theme_bw()\n\nYou may notice the scale of the x-axis and the scale of the y-axis are the same for all subplots. That’s useful when you want to directly compare the subplots, which is usually the case. However, if you need to the scales to be independent of each other, you guess it, that can be specified:\n\nmidpoints_durs |&gt; \n  ggplot(aes(x = duration))+\n  geom_histogram(binwidth = 0.01)+\n  facet_wrap(~vowel, scales = \"free\")+\n  theme_bw()\n\nRather than have independent scales for both the x-axis and the y-axis, you can specify only one if you’d like with scale = \"free_x\" or scale = \"free_y\". Give it a try!\nAlso, we can control the number of rows or columns that the subplots are placed in with the nrow or ncol arguments:\n\nmidpoints_durs |&gt; \n  ggplot(aes(x = duration))+\n  geom_histogram(binwidth = 0.01)+\n  facet_wrap(~vowel, scales = \"free\", nrow = 2)+\n  theme_bw()\n\nmidpoints_durs |&gt; \n  ggplot(aes(x = duration))+\n  geom_histogram(binwidth = 0.01)+\n  facet_wrap(~vowel, scales = \"free\", ncol = 3)+\n  theme_bw()\n\n\n\nfacet_grid()\nWhile facet_wrap() takes one variable, that is, column in the input data frame, facet_grid() takes two columns and creates a grid layout. For example, let’s look at the distribution of vowel duration by vowel and by following segment:\n\nmidpoints_durs |&gt; \n  ggplot(aes(x = duration))+\n  geom_histogram(binwidth = 0.025)+\n  facet_grid(fol~vowel)+\n  theme_bw()\n\nThis layout doesn’t allow you to specify the number of rows nor of columns, nor allow the scales to vary independently.\n\n\nActivity\nYour turn! Make a plot with the distribution of durations by vowel and by preceding segment. Feel free to create histograms or density plots.\nAfter a good-faith effort, if you need help, see Dr. Brown’s code below:\n\n\nCode\nmidpoints_durs |&gt; \n  ggplot(aes(x = duration))+\n  geom_histogram()+\n  facet_grid(pre~vowel)"
  },
  {
    "objectID": "lessons/visualization.html#color",
    "href": "lessons/visualization.html#color",
    "title": "Visualization",
    "section": "Color",
    "text": "Color\nWe can adjust the color of the lines and the fill of elements of a plot with a myriad of possibilities. First, let’s take a look at the possibilities:\n\ncolors()\n\nThe RStudio IDE displays the highlights the name of the color right in the source file (aka. script). Let’s change the color of the lines with the color argument and the color of the fill of the boxes of a boxplot with the fill argument:\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration)) + \n  geom_boxplot(color = \"navy\", fill = \"lightblue\")+\n  theme_bw()\n\nNote that when you explicitly specify colors in the call to a layer, it overrides any previous color specification. For example, if I have ggplot() to plot a color, but then give geom_boxplot() specific colors, only the colors given to the later layer are plotted.\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration, color = vowel)) + \n  geom_boxplot(color = \"navy\", fill = \"lightblue\")+\n  theme_bw()\n\nHowever, if a particular layer doesn’t overwrite a color specification inherited from ggplot(), for example geom_jitter() below, then the later layer use the color from ggplot():\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration, color = vowel)) + \n  geom_boxplot(color = \"navy\", fill = \"lightblue\")+\n  geom_jitter(width = 0.1, height = 0)+\n  theme_bw()\n\nSo, the aes() in ggplot() passes its color down to the subsequent layers, but aes() with a specific layer only affects that layer.\n\nActivity\nCompare and contrast the following three blocks of code their resulting plots. Identify how the plots differ and how the placement of color = vowel affects which elements of the plot are colored.\n\nfront_durs |&gt; \n  ggplot(aes(vowel, duration)) + \n  geom_boxplot(aes(color = vowel)) + \n  geom_jitter(width = 0.1, height = 0) +\n  theme_bw()\n\nfront_durs |&gt; \n  ggplot(aes(vowel, duration)) + \n  geom_boxplot() + \n  geom_jitter(aes(color = vowel), width = 0.1, height = 0) +\n  theme_bw()\n\nfront_durs |&gt; \n  ggplot(aes(vowel, duration, color = vowel)) + \n  geom_boxplot() + \n  geom_jitter(width = 0.1, height = 0) +\n  theme_bw()\n\nCheck out the docs for scale_color_distiller() for other color palettes.\nIn the code above, we only map color to a categorical variable. But, we can also map it to a continuous variable. Let’s use the midpoints data frame from above:\n\nmidpoints_durs |&gt; \n  ggplot(aes(F2, F1, color = duration)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse()+\n  theme_bw()\n\nBy default, ggplot() uses the blue-to-black palette, but we can change it manually if we’d like:\n\nmidpoints_durs |&gt; \n  ggplot(aes(F2, F1, color = duration)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse()+\n  scale_color_gradient(low = \"gold\", high = \"forestgreen\")+\n  theme_bw()"
  },
  {
    "objectID": "lessons/visualization.html#reorder-levels-of-a-categorical-variable",
    "href": "lessons/visualization.html#reorder-levels-of-a-categorical-variable",
    "title": "Visualization",
    "section": "Reorder levels of a categorical variable",
    "text": "Reorder levels of a categorical variable\nLet’s take a look at our a boxplot with jittered observations:\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration)) + \n  geom_boxplot(outlier.alpha = 0) + \n  geom_jitter(width = 0.1, height = 0)+\n  theme_bw()\n\nWe can also adjust levels of a categorical variable so that they appear in a different order. This can be done in one of two place: with the dplyr part of the pipeline, or within the ggplot part of the pipeline. First, let’s reorder the levels with our ol’ friend fct_relevel():\n\nfront_durs %&gt;%\n  mutate(vowel = fct_relevel(vowel, c(\"FLEECE\", \"KIT\", \"FACE\", \"DRESS\", \"TRAP\"))) |&gt; \n  ggplot(aes(vowel, duration)) + \n  geom_boxplot(outlier.size = 0) + \n  geom_jitter(width = 0.1, height = 0)+\n  theme_bw()\n\nOr, we could ask use a ggplot layer to reorder the levels of this categorical variable:\n\nfront_durs %&gt;%\n  ggplot(aes(vowel, duration)) + \n  geom_boxplot(outlier.size = 0) + \n  geom_jitter(width = 0.1, height = 0)+\n  scale_x_discrete(limits = c(\"FLEECE\", \"KIT\", \"FACE\", \"DRESS\", \"TRAP\"))+ \n  theme_bw()\n\nIf that’s confusing to have two ways to do the same thing, just choose one and forget the other. If you need help choosing, Dr. Brown’s recommends the fct_relevel() way.\n\nActivity\nCreate a boxplot of the five vowels in the front_durs data frame and order the boxes by median duration. Hint: First, use some of our ol’ friends in dplyr to create a vector of the five vowels ordered by median, then use that vector with in fct_relevel() or scale_x_discrete().\nAfter a good-faith effort, if you need help, take a look at Dr. Brown’s code:\n\n\nCode\nordered_by_median &lt;- front_durs |&gt; \n  group_by(vowel) |&gt; \n  summarize(median_dur = median(duration)) |&gt; \n  arrange(median_dur) |&gt; \n  pull(vowel) |&gt; \n  as.character()\nfront_durs |&gt; \n  mutate(vowel = fct_relevel(vowel, ordered_by_median)) |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot(outlier.size = 0) + \n  geom_jitter(width = 0.1, height = 0)+\n  theme_bw()\n\n\nWe can also order bar of a barplot as well. Let’s use the midpoints data frame to order the vowel in descending order by frequency:\n\nmidpoints |&gt; \n  mutate(vowel = fct_infreq(vowel)) |&gt; \n  ggplot(aes(vowel)) + \n  geom_bar()+\n  theme_bw()"
  },
  {
    "objectID": "lessons/visualization.html#add-ellipses-by-level-in-scatterplot",
    "href": "lessons/visualization.html#add-ellipses-by-level-in-scatterplot",
    "title": "Visualization",
    "section": "Add ellipses by level in scatterplot",
    "text": "Add ellipses by level in scatterplot\nLet’s go back to our scatterplot with F1 and F2 of vowels:\n\nmidpoints |&gt; \n  ggplot(aes(F2, F1, color = vowel)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse()+\n  theme_bw()\n\n…And now add ellipses around two-thirds of the data points per vowel:\n\nmidpoints |&gt; \n  ggplot(aes(F2, F1, color = vowel)) + \n  geom_point(alpha = 0.5) + \n  stat_ellipse(level = 0.67) + \n  scale_x_reverse() + \n  scale_y_reverse()+\n  theme_bw()\n\nWith so many vowels and some of the colors so close to each other, it’d be nice to add labels right on the plot. You guessed it! That can be done. First, let’s create a small data frame with the mean F1 and mean F2 by vowel:\n\nF_means &lt;- midpoints %&gt;%\n  group_by(vowel) %&gt;%\n  summarize(mean_F1 = mean(F1), mean_F2 = mean(F2))\n\nNow let’s use those means in order to place the labels in the right spots on the plot:\n\nmidpoints |&gt; \n  ggplot(aes(F2, F1, color = vowel)) + \n  geom_point(alpha = 0.5) + \n  stat_ellipse(level = 0.67) + \n  geom_text(data = F_means, aes(mean_F2, mean_F1, label = vowel), size = 5)+\n  scale_x_reverse() + \n  scale_y_reverse()+\n  theme_bw()\n\nGiven the fact that the labels are difficult to read because of the dots behind him, it might be best to use black labels:\n\nmidpoints |&gt; \n  ggplot(aes(F2, F1, color = vowel)) + \n  geom_point(alpha = 0.5) + \n  stat_ellipse(level = 0.67) + \n  geom_text(data = F_means, aes(mean_F2, mean_F1, label = vowel), size = 5, color = \"black\")+\n  scale_x_reverse() + \n  scale_y_reverse()+\n  theme_bw()"
  },
  {
    "objectID": "lessons/visualization.html#small-adjustments",
    "href": "lessons/visualization.html#small-adjustments",
    "title": "Visualization",
    "section": "Small adjustments",
    "text": "Small adjustments\n\nBackground color\nThe default color background of ggplot is gray, for example:\n\np &lt;- midpoints |&gt; \n  ggplot(aes(F2, F1, color = vowel)) + \n  geom_point(alpha = 0.5) + \n  stat_ellipse(level = 0.67) + \n  geom_text(data = F_means, aes(mean_F2, mean_F1, label = vowel), size = 5, color = \"black\")+\n  scale_x_reverse() + \n  scale_y_reverse()\nprint(p)\n\nAs you may have noticed by now, Dr. Brown prefers the black-n-white theme that theme_bw() gives.\n\np + theme_bw()\n\nAnother nice theme is theme_minimal(), which is basically the theme_bw() but without the border:\n\np + theme_minimal()\n\n\n\nActivity\nExploration time: Take a look at the docs for the themes by entering ?theme_bw at the console, and then try some of the other themes. Be ready to report on which theme you like the most, and which you like the least.\n\n\nLabels\nWe can add custom labels to our plots with the labs() layer.\n\np + \n  theme_classic()+\n  labs(title = \"F1 and F2 by Vowel\", subtitle = \"of Joey Stanley's speech\", x = \"Second formant (F2)\", y = \"First formant (F1)\")\n\nWe can also adjust the font, angle, horizontal and vertical adjustments of text on the plot, for example:\n\np +\n  theme_bw()+\n  theme(text = element_text(family = \"Times New Roman\"),)\n\nOr tilt the angle of the labels:\n\nmidpoints |&gt; \n  mutate(vowel = fct_infreq(vowel)) |&gt; \n  ggplot(aes(vowel)) + \n  geom_bar()+\n  theme_bw()+\n  theme(text = element_text(family = \"Times New Roman\"),\n        axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))\n\nHow about something as nasty as this?\n\np +\n  labs(title = \"Joey's vowels\", subtitle = \"Notice that this is left-aligned to the plotting area\") + \n    theme(plot.title = element_text(hjust = 0.2, vjust = 0.6, angle = 2, family = \"Avenir\", size = 18, color = \"forestgreen\", face = \"italic\", lineheight = 1.2, margin = margin(t = 1, r = 0.2, b = 1.4, l = -4, unit = \"cm\"), debug = TRUE))"
  },
  {
    "objectID": "lessons/bootstrapping.html",
    "href": "lessons/bootstrapping.html",
    "title": "Bootstrapping",
    "section": "",
    "text": "Students will perform bootstrapping of a statistical test. (They might even strap their own boots, to boot!)"
  },
  {
    "objectID": "lessons/bootstrapping.html#objective",
    "href": "lessons/bootstrapping.html#objective",
    "title": "Bootstrapping",
    "section": "",
    "text": "Students will perform bootstrapping of a statistical test. (They might even strap their own boots, to boot!)"
  },
  {
    "objectID": "lessons/bootstrapping.html#assumptions",
    "href": "lessons/bootstrapping.html#assumptions",
    "title": "Bootstrapping",
    "section": "Assumptions",
    "text": "Assumptions\nThose rascally assumptions of statistical tests sometimes/ofttimes are not met. First, try some transformations of the continuous and/or recoding and combining levels in categorical variables. If that doesn’t do the trick, what do you do?\nEnter bootstrapping."
  },
  {
    "objectID": "lessons/bootstrapping.html#bootstrapping",
    "href": "lessons/bootstrapping.html#bootstrapping",
    "title": "Bootstrapping",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nLevshina (2015, p. 167) wrote:\n“When one or more assumptions of linear regression have been violated, one can use regression based on bootstrapping, which was introduced in the previous subsection. Its advantage is that it does not require all those assumptions to be met in order to return results that can be trusted.”\nEgbert & Plonsky (2020, p. 938) mention that bootstrapping is “recommended for small samples and samples with unknown or non-normal distributions.” They describe the technique as: “Bootstrapping is a statistical technique that relies on randomly resampling with replacement from a set of observed values in order to estimate the accuracy of statistical parameters” (p. 942).\nBrown (2023) used a 10,000-iteration bootstrapping procedure in his study of the duration of words in the Buckeye Corpus of English.\nMORE ABOUT BOOTSTRAPPING TO COME IN THE FUTURE"
  },
  {
    "objectID": "lessons/chi-square.html",
    "href": "lessons/chi-square.html",
    "title": "Chi-square test",
    "section": "",
    "text": "Students will perform a chi-square (𝜒2) test of counts."
  },
  {
    "objectID": "lessons/chi-square.html#objective",
    "href": "lessons/chi-square.html#objective",
    "title": "Chi-square test",
    "section": "",
    "text": "Students will perform a chi-square (𝜒2) test of counts."
  },
  {
    "objectID": "lessons/chi-square.html#count-em-up",
    "href": "lessons/chi-square.html#count-em-up",
    "title": "Chi-square test",
    "section": "Count ’em up!",
    "text": "Count ’em up!\nThe chi-square test is the go-to test for counts of levels in two categorical variables. For example:\n\nbathroom v. washroom in American English v. Canadian English\ngive it to me v. give me it in academic v. spoken English\nLatin v. Greek borrowing in legal v. medical jargon"
  },
  {
    "objectID": "lessons/chi-square.html#contingency-tables",
    "href": "lessons/chi-square.html#contingency-tables",
    "title": "Chi-square test",
    "section": "Contingency tables",
    "text": "Contingency tables\nWhen we have two categorical variables, we can (and really should) create a contingency table (aka. cross tab). Let’s create some fictitious data for the first example above:\n\n\n\n\nAmerican Eng.\nCanadian Eng.\n\n\nbathroom\n23\n10\n\n\nwashroom\n34\n45"
  },
  {
    "objectID": "lessons/chi-square.html#margins",
    "href": "lessons/chi-square.html#margins",
    "title": "Chi-square test",
    "section": "Margins",
    "text": "Margins\nThe margins in the context of contingency tables are the row and columns totals:\n\n\n\n\nAmerican Eng.\nCanadian Eng.\nTotal\n\n\nbathroom\n23\n10\n33\n\n\nwashroom\n34\n45\n79\n\n\nTotal\n57\n55\n112"
  },
  {
    "objectID": "lessons/chi-square.html#observed-v.-expected",
    "href": "lessons/chi-square.html#observed-v.-expected",
    "title": "Chi-square test",
    "section": "Observed v. expected",
    "text": "Observed v. expected\nAnother super important step in performing the chi-square test is calculating the expected values. Let’s take bathroom in American English for a moment. We see that our (fictitious) count is 23, which is our observed value. For each cell in the contingency table, we need to calculate the expected value, and that is accomplished by taking the row total and multiplying it by the column total and then dividing that product by the total N: \\(33\\ *\\ 57\\ /\\ 112\\ =\\ 16.79464\\)\nWhen we calculate the expected values for each cell in the contingency table, we end up with the following expected values:\n\n\n\n\nAmerican Eng.\nCanadian Eng.\n\n\nbathroom\n16.79464\n16.20536\n\n\nwashroom\n40.20536\n38.79464\n\n\n\nLet’s run a chi-square test:\n\npotty &lt;- matrix(data = c(23, 10, 34, 45), nrow = 2, byrow = TRUE)\nprint(potty)\n\n     [,1] [,2]\n[1,]   23   10\n[2,]   34   45\n\nresult = chisq.test(potty)\nprint(result)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  potty\nX-squared = 5.5955, df = 1, p-value = 0.01801\n\n\nSo yeah, we reject the null hypothesis that there is no association between term for where to go potty and the country where English is spoken.\nIf we want to see the expected values of the contingency table, we can ask for it from the result of the chisq.test() call:\n\nresult$expected\n\n         [,1]     [,2]\n[1,] 16.79464 16.20536\n[2,] 40.20536 38.79464\n\n\nIf we can to see which cell contributes (the most) to a chi-square test being significant, we can look for the largest absolute residual value:\n\nresult$residuals\n\n           [,1]       [,2]\n[1,]  1.5141936 -1.5414785\n[2,] -0.9786442  0.9962788\n\n\nCanadians say bathroom too infrequently.\n\nAssumption\nIf any of the expected values is 5 or smaller, we can’t use the chi-square. Instead, we’d have to use a Fisher Exact test (this isn’t the case with our fictitious dataset, but let’s run a Fisher Exact test for fun):\n\nfisher.test(potty)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  potty\np-value = 0.01288\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 1.191312 8.099109\nsample estimates:\nodds ratio \n  3.013478"
  },
  {
    "objectID": "lessons/chi-square.html#activity",
    "href": "lessons/chi-square.html#activity",
    "title": "Chi-square test",
    "section": "Activity",
    "text": "Activity\nYour turn! Using a dataset of your choice, create a contingency table of two categorical variables of your choice. Then, calculate the a chi-square test or a Fisher Exact test (depending on if any of the expected values is 5 or smaller)."
  },
  {
    "objectID": "lessons/interaction.html",
    "href": "lessons/interaction.html",
    "title": "Interaction between explanatory variables",
    "section": "",
    "text": "Students will run and interpret the interaction between two explanatory variables."
  },
  {
    "objectID": "lessons/interaction.html#objective",
    "href": "lessons/interaction.html#objective",
    "title": "Interaction between explanatory variables",
    "section": "",
    "text": "Students will run and interpret the interaction between two explanatory variables."
  },
  {
    "objectID": "lessons/interaction.html#interactions",
    "href": "lessons/interaction.html#interactions",
    "title": "Interaction between explanatory variables",
    "section": "Interactions",
    "text": "Interactions\nLet’s talk about interactions between explanatory variables by looking at an example. In the LMS, there’s a fictitious dataset named “regionalisms.csv” with the number of regionalisms (e.g., potato bug for rolly polly, coke for any soda, scallions for green onions, etc.) normalized to N per hour of speech, as spoken by 200 speakers grouped by age and sex. Download the CSV file to your harddrive and load it into R.\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nregion &lt;- read_csv(\"/Users/ekb5/Documents/LING_440/regionalisms.csv\")\n\nRows: 200 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): age, sex\ndbl (1): n\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "lessons/interaction.html#plot-age",
    "href": "lessons/interaction.html#plot-age",
    "title": "Interaction between explanatory variables",
    "section": "Plot age",
    "text": "Plot age\nWhat is the effect of age group on number of regionalisms spoken per hour? Let’s draw a boxplot and see.\n\np1 &lt;- region %&gt;% \n  ggplot(aes(x = age, y = n))+\n  geom_boxplot(notch = TRUE)+\n  stat_summary(fun = mean)+\n  theme_bw()\np1\n\nWarning: Removed 2 rows containing missing values (`geom_segment()`).\n\n\n\n\n\nWow! Looks like we have a serious effect from age group on number of regionalisms, with older speakers using more regionalisms."
  },
  {
    "objectID": "lessons/interaction.html#plot-sex",
    "href": "lessons/interaction.html#plot-sex",
    "title": "Interaction between explanatory variables",
    "section": "Plot sex",
    "text": "Plot sex\nNow let’s see the effect of biological sex on the number of regionalisms spoken per hour.\n\np2 &lt;- region %&gt;% \n  ggplot(aes(x = sex, y = n))+\n  geom_boxplot(notch = TRUE)+\n  stat_summary(fun = mean)+\n  theme_bw()\np2\n\nWarning: Removed 2 rows containing missing values (`geom_segment()`).\n\n\n\n\n\nAnother boxplot that seems to suggest that there is a big effect from sex on number of regionalisms per hour of speech, with men using more regionalisms than women."
  },
  {
    "objectID": "lessons/interaction.html#plot-age-and-sex-together",
    "href": "lessons/interaction.html#plot-age-and-sex-together",
    "title": "Interaction between explanatory variables",
    "section": "Plot age and sex together",
    "text": "Plot age and sex together\nWait! Hold up! Could it be more complicated than the above boxplots suggest? Could there be an interaction between age and sex, such that the effect of one variable is affected by the other variable? Let’s take a look at a faceted boxplot of age group and sex.\n\np3 &lt;- region %&gt;% \n  ggplot(aes(x = sex, y = n))+\n  geom_boxplot(notch = TRUE)+\n  stat_summary(fun = mean)+\n  facet_wrap(~age)+\n  theme_bw()\np3\n\nWarning: Removed 2 rows containing missing values (`geom_segment()`).\nRemoved 2 rows containing missing values (`geom_segment()`).\n\n\n\n\n\nOh! Now we see what’s going on. It’s not that older speakers in general produce more regionalisms than younger speaker in general, nor that men in general produce more regionalisms than women in general. We see that the effect is really only from older men who produce lots of regionalisms, while all other speakers use many fewer regionalisms. The takehome message: We have an interaction between age group and sex because the effect of one variable is affected by the other variable."
  },
  {
    "objectID": "lessons/interaction.html#only-main-effects",
    "href": "lessons/interaction.html#only-main-effects",
    "title": "Interaction between explanatory variables",
    "section": "Only main effects",
    "text": "Only main effects\nLet’s fit a linear regression with only the two main effects (i.e., age group and sex). Look at the adjusted \\(R^{2}\\) value and the AIC (Akaike information criterion).\n\nm1 &lt;- lm(n ~ age + sex, data = region)\nsummary(m1)\n\n\nCall:\nlm(formula = n ~ age + sex, data = region)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.505 -2.505 -0.145  2.855  7.945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  22.0550     0.4739  46.541  &lt; 2e-16 ***\nageyoung     -4.9100     0.5472  -8.973 2.29e-16 ***\nsexwomen     -4.5500     0.5472  -8.315 1.49e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.869 on 197 degrees of freedom\nMultiple R-squared:  0.4317,    Adjusted R-squared:  0.4259 \nF-statistic: 74.83 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\nAIC(m1)\n\n[1] 1113.78"
  },
  {
    "objectID": "lessons/interaction.html#with-interaction-term",
    "href": "lessons/interaction.html#with-interaction-term",
    "title": "Interaction between explanatory variables",
    "section": "With interaction term",
    "text": "With interaction term\nNow, let’s fit a linear regression with the two main effects (i.e., age group and sex) as well as a interaction term between the two main effect. Again, pay attention to the adjusted \\(R^{2}\\) value (the bigger the better with \\(R^{2}\\)) and the AIC value (the smaller the better with AIC), and compare them to the previous linear regression model.\n\nm2 &lt;- lm(n ~ age + sex + age:sex, data = region)\nsummary(m2)\n\n\nCall:\nlm(formula = n ~ age + sex + age:sex, data = region)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -5.38  -3.02   0.28   2.62   5.82 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        24.1800     0.4569  52.917   &lt;2e-16 ***\nageyoung           -9.1600     0.6462 -14.175   &lt;2e-16 ***\nsexwomen           -8.8000     0.6462 -13.618   &lt;2e-16 ***\nageyoung:sexwomen   8.5000     0.9139   9.301   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.231 on 196 degrees of freedom\nMultiple R-squared:  0.6057,    Adjusted R-squared:  0.5997 \nF-statistic: 100.4 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\nAIC(m2)\n\n[1] 1042.662\n\n\nOkay, now we’re getting somewhere! The \\(R^{2}\\) has increased and the AIC has decreased, both of which are good. That is, the model with an interaction term explains more of the variability in the response variable, and the estimator of the prediction error has gone down (i.e., AIC)."
  },
  {
    "objectID": "lessons/interaction.html#no-interaction",
    "href": "lessons/interaction.html#no-interaction",
    "title": "Interaction between explanatory variables",
    "section": "No interaction",
    "text": "No interaction\nFor comparison, if there were an monotonic effect from age group and another monotonic effect from sex, with no interaction effect between these two variables, the boxplot would look something like the following one.\n\nsize_per_group &lt;- 50\nold_men &lt;- tibble(\n  n_regionalisms = sample(seq(20, 30), size = size_per_group, replace = T),\n  age = sample(seq(35, 55), size = size_per_group, replace = T),\n  sex = rep(\"men\", size_per_group)\n)\nold_women &lt;- tibble(\n  n_regionalisms = sample(seq(15, 25), size = size_per_group, replace = T),\n  age = sample(seq(35, 55), size = size_per_group, replace = T),\n  sex = rep(\"women\", size_per_group)\n)\nyoung_men &lt;- tibble(\n  n_regionalisms = sample(seq(10, 20), size = size_per_group, replace = T),\n  age = sample(seq(18, 34), size = size_per_group, replace = T),\n  sex = rep(\"men\", size_per_group)\n)\nyoung_women &lt;- tibble(\n  n_regionalisms = sample(seq(5, 15), size = size_per_group, replace = T),\n  age = sample(seq(18, 34), size = size_per_group, replace = T),\n  sex = rep(\"women\", size_per_group)\n)\n  \nfake_data &lt;- tibble() %&gt;% \n  bind_rows(old_men) %&gt;% \n  bind_rows(old_women) %&gt;% \n  bind_rows(young_men) %&gt;% \n  bind_rows(young_women)\n\nfake_data &lt;- fake_data %&gt;% \n  mutate(age_group = ifelse(age &lt;= 34, \"young\", \"old\"))\n\np4 &lt;- fake_data %&gt;% \n  ggplot(aes(x = sex, y = n_regionalisms))+\n  geom_boxplot(notch = TRUE)+\n  stat_summary(fun = mean)+\n  facet_wrap(~age_group)+\n  theme_bw()\np4\n\nWarning: Removed 2 rows containing missing values (`geom_segment()`).\nRemoved 2 rows containing missing values (`geom_segment()`).\n\n\n\n\n\n\nm3 &lt;- lm(n_regionalisms ~ age_group + sex + age_group:sex, data = fake_data)\nsummary(m3)\n\n\nCall:\nlm(formula = n_regionalisms ~ age_group + sex + age_group:sex, \n    data = fake_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -5.14  -2.61  -0.14   2.36   5.42 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              24.6000     0.4456  55.209  &lt; 2e-16 ***\nage_groupyoung           -9.4600     0.6301 -15.012  &lt; 2e-16 ***\nsexwomen                 -4.9600     0.6301  -7.871 2.33e-13 ***\nage_groupyoung:sexwomen  -0.6000     0.8912  -0.673    0.502    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.151 on 196 degrees of freedom\nMultiple R-squared:  0.7597,    Adjusted R-squared:  0.756 \nF-statistic: 206.5 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\nAIC(m3)\n\n[1] 1032.589\n\n\nThe interaction term was not selected as making a significant contribution to the prediction of the number of regionalisms in this second set of fictitious data."
  },
  {
    "objectID": "lessons/create_freq.html",
    "href": "lessons/create_freq.html",
    "title": "Creating frequency lists",
    "section": "",
    "text": "Students will create frequency lists from files on their harddrive."
  },
  {
    "objectID": "lessons/create_freq.html#objective",
    "href": "lessons/create_freq.html#objective",
    "title": "Creating frequency lists",
    "section": "",
    "text": "Students will create frequency lists from files on their harddrive."
  },
  {
    "objectID": "lessons/create_freq.html#frequency-in-language",
    "href": "lessons/create_freq.html#frequency-in-language",
    "title": "Creating frequency lists",
    "section": "Frequency in language",
    "text": "Frequency in language\nFrequency is an important construct in many areas of language, and more generally, in human cognition (see our amazing pattern recognition abilities). Frequency affects which words and phrases are learned first, in both L1s and L2s. More frequent grammatical constructions are learned before (and better for L2 speakers) than less frequent ones (e.g., active voice vs. passive voice in English). More frequency words experience phonetically-driven sound changes (e.g., lenition) first. More frequent words resist analogical leveling (keep -&gt; *keeped, but leap -&gt; leapt -&gt; leaped).\nIn summary, frequency is super important, and being able to calculate frequencies of language features, especially words, is an important skill for a language-oriented data analyst."
  },
  {
    "objectID": "lessons/create_freq.html#getting-frequencies-of-words-in-files",
    "href": "lessons/create_freq.html#getting-frequencies-of-words-in-files",
    "title": "Creating frequency lists",
    "section": "Getting frequencies of words in files",
    "text": "Getting frequencies of words in files\nThe logic to calculate frequencies of files on a harddrive in R is simple (when holding all words in RAM; see an alternative below):\n\nParse the files so that all words in all files are in a single vector with N elements (N being the total number of words [specifically, word tokens] across all files);\nAsk R to count up the number of word tokens per word type in the vector.\n\nThere are three ways (Dr. Brown will show) to count up word tokens per word type:\n\nWith the base R table() function;\nConvert the vector into a one-column data frame and then use count() in tidyverse.\nUsing a list data structure, count of frequencies of words in one file at a time."
  },
  {
    "objectID": "lessons/create_freq.html#the-table-function",
    "href": "lessons/create_freq.html#the-table-function",
    "title": "Creating frequency lists",
    "section": "The table() function",
    "text": "The table() function\nLet’s create a toy example:\n\nlibrary(\"tidyverse\")\n\n# create a sentence\nsentence &lt;- \"I like linguistics, and I like my students, but I love my wife and chidren. Sorry students. Maybe next time.\"\n\n# uppercase (or lowercase) the string, so that uppercase and lowercase words (e.g., \"The\" and \"the\" and \"THE\") are treated as the same word\nsentence &lt;- str_to_upper(sentence)\n\n# tokenize the string into words\nwords &lt;- str_extract_all(sentence, \"[-'’A-Z]+\")\n\n# unlist the list so that we're left with a vector\nwords &lt;- unlist(words)\n\n# throw the vector at table() and watch the magic happen!\nfreqs &lt;- table(words)\nprint(freqs)\n\nThe output of the table() function is a named one-dimensional array (like a vector), of class table. The values are the integers (i.e., the frequencies), and each integer has a name (i.e., a word). In order to extract only the names, you can use the names() function:\n\nprint(names(freqs))\n\nWe probably want to sort the frequencies in descending order:\n\nfreqs &lt;- sort(freqs, decreasing = TRUE)\nprint(freqs)\n\nThe frequencies and their names (i.e., the words) can be put into a data frame and then exported out as a CSV file.\n\nActivity\nDo just that, that is, export to a CSV file the words and frequencies using the result of the above toy example. One tip: You’ll need to coerce the data type of the numbers to integer with as.integer(freqs) when assigning the array to a column in the data frame.\nAfter giving it a good-faith effort, if you need help, take a look at Dr. Brown’s code below:\n\n\nCode\ndf &lt;- tibble(wd = names(freqs), freq = as.integer(freqs))\nwrite_csv(df, file = \"freqs.csv\")\n\n\nLet’s ramp it up:\nCreate a frequency list of words in many text files of your choice (e.g., from Project Gutenberg or the Saints files in the LMS).\nAfter a good-faith effort, if you need help, take a look at Dr. Brown’s start to the code below:\n\n\nCode\n# get filenames\nfilenames &lt;- dir(path = \"/pathway/to/Saints/txt/\", pattern = \"\\\\.txt$\", full.names = TRUE, recursive = TRUE)\n\n# create a collector string to collect the text of all files\nall_str &lt;- \"\"\n\n# loop over the filenames\nfor (filename in filenames) {\n  \n  # get the text from the current file\n  txt &lt;- read_file(filename)\n  \n  # add the text of the current file to the collector string\n  all_str &lt;- str_c(all_str, txt, sep = \" \")\n  \n}  # next filename\n\n\nAfter the for loop, the variable all_str is one big string with all text from all files. You should now be able to modify the code in the toy example above to get the frequencies of the words in this single big string. Go for it! You got this! Let’s go! You’re a super star! Etc.!"
  },
  {
    "objectID": "lessons/create_freq.html#the-count-function",
    "href": "lessons/create_freq.html#the-count-function",
    "title": "Creating frequency lists",
    "section": "The count() function",
    "text": "The count() function\nA second way (among other ways) is to take the vector with words (not the big single string, but the vector with each word as a separate element) and create a one-column data frame, and then use count() (within tidyverse). Let’s go!\nUsing the words vector from the toy example above:\n\ndf &lt;- tibble(wd = words)\nfreqs &lt;- df %&gt;% count(wd)\nprint(freqs)\n\nQuick little aside: The pipe operator %&gt;% passes the value on the left-hand side of the operator into the function on the right-hand side, as the first argument to that function.\n\nActivity\nThat’s right, it’s time to step up and use the count() function to calculate frequencies and then write them out to a CSV file. Ready… set… go!\nAfter a good-faith effort, if you need some help, take a look at Dr. Brown’s code below:\n\n\nCode\n# using the toy example above:\n# (enjoy all the pipe operators!)\ntibble(wd = words) %&gt;% \n  count(wd) %&gt;% \n  write_csv(file = \"freqs.csv\")"
  },
  {
    "objectID": "lessons/create_freq.html#populate-a-list",
    "href": "lessons/create_freq.html#populate-a-list",
    "title": "Creating frequency lists",
    "section": "Populate a list",
    "text": "Populate a list\nA third way to calculate frequencies of words in files is to create an R list and populate the list with words, one file at a time. This is a good way if the corpus that you want frequencies from is too big to fit in the RAM of your computer, or too big to fit comfortably because it makes your computer work slowly. You can have your script bring into the RAM only one file at a time and populate an R list. This approach to getting frequencies is slower than the previous two, so unless you have a good reason for using this approach, it’s probably best to one of the previous two approaches (fun fact: Dr. Brown likes the count() approach best).\nLet’s take a look at an example using the Saints corpus:\n\n# get filenames\nfilenames &lt;- dir(path = \"/pathway/to/Saints/txt/\", pattern = \"\\\\.txt$\", full.names = TRUE, recursive = TRUE)\n\n# create a collector string to collect the text of all files\nfreqs &lt;- list()\n\n# loop over the filenames\nfor (filename in filenames) {\n  \n  # get the text from the current file and uppercase it\n  txt &lt;- read_file(filename) %&gt;% str_to_upper()\n  \n  # get words in current file and unlist the list into a vector\n  wds &lt;- str_extract_all(txt, \"[-'’A-Z]+\") %&gt;% unlist()\n  \n  # loop over the vector of words\n  for (wd in wds) {\n    \n    # test whether the current word is already in the list\n    if (wd %in% names(freqs)) {\n      \n      # if so, increment the counter for that word by one\n      freqs[[wd]] &lt;- freqs[[wd]] + 1\n      \n    } else {\n      # if not, add an entry for the word and give it a value of one\n      freqs[[wd]] &lt;- 1\n      \n    }  # end if\n  }  # next word  \n}  # next filename\n\n# print out the words and their frequencies to the console\nfor (wd in names(freqs)) {\n  cat(wd, \": \", freqs[[wd]], \"\\n\", sep=\"\")\n}\n\nThe next step would be put the words and their frequencies into a data frame in order to (more easily) sort them and to (more easily) write them out to a CSV file."
  },
  {
    "objectID": "lessons/programming_basics.html",
    "href": "lessons/programming_basics.html",
    "title": "Programming basics",
    "section": "",
    "text": "Students will learn to code the basic building blocks of programming in R."
  },
  {
    "objectID": "lessons/programming_basics.html#objective",
    "href": "lessons/programming_basics.html#objective",
    "title": "Programming basics",
    "section": "",
    "text": "Students will learn to code the basic building blocks of programming in R."
  },
  {
    "objectID": "lessons/programming_basics.html#primitive-data-types-in-r",
    "href": "lessons/programming_basics.html#primitive-data-types-in-r",
    "title": "Programming basics",
    "section": "Primitive data types in R",
    "text": "Primitive data types in R\n\ninteger\n\nThis a whole number, i.e., there is no decimal component, for example “5”.\nTo specify an integer in R, type an uppercase “L” immediately to the right of the number (i.e., no space between the number and the “L”).\n\nE.g., 5L\n\n\nnumeric\n\nThis is a number with a decimal component, for example, “3.14”. Note: Unless an “L” is placed to the right of a whole number, R treats it as a numeric.\nTo specific a numeric in R, just the type the good ol’ fashioned number.\n\nE.g., 3.14\n\nNote: Dr. Brown may refer to this data type as a “float” because of language transfer from Python and Julia.\n\ncharacter\n\nE.g., \"hello world\" and c(\"hello\", \"hola\", \"hej\")\n\nNote: Dr. Brown will likely refer to this data type as “string” because of language transfer from Python and Julia.\nYou can extract part of a string with the str_sub() function (doc here).\n\n\nlogical\n\nThis data type has one of two values, either TRUE or FALSE (or T or F for shorthand).\nDr. Brown may refer to this data type as a “Boolean” because of language transfer from Python and Julia.\n\nThe class() function returns the data type of a variable or value.\n\nEg. class(5L) returns integer, while class(\"hello world\") returns character.\n\n\nActivity\n\nStudents use the class() function to become familiar with the data type of the values that they type."
  },
  {
    "objectID": "lessons/programming_basics.html#operators-in-r",
    "href": "lessons/programming_basics.html#operators-in-r",
    "title": "Programming basics",
    "section": "Operators in R",
    "text": "Operators in R\n\nAssignment operator: There are two assignment operators in R. The most common is &lt;- but = also works. For example:\n\nfruit &lt;- \"apple\"\nfruits &lt;- c(\"apple\", \"banana\", \"orange\", \"mango\")\n\nKeyboard shortcut in RStudio: ALT/OPT + -\n\npets = c(\"dog\", \"cat\", \"fish\", \"Madagascar hissing cockroach\")\nage = 46 (note: this creates a numeric rather than an integer; if an integer is wanted: age = 46L)\n\nInclusion operator: %in% tests for inclusion of a value in a collection of values (e.g., a vector), for example:\n\n\"apple\" %in% c(\"banana\", \"apple\", \"mango\") returns TRUE\n\"kiwi\" %in% c(\"banana\", \"apple\", \"mango\") returns FALSE\n\nEqual operator: == (i.e., two equal signs together with no space between them) tests whether the left-hand value and the right-hand value are identical, for example:\n\n\"mango\" == \"mango\" returns TRUE\n\"apple\" == \"manzana\" returns FALSE\n\"Hannah\" == \"HANNAH\" returns FALSE\n\nSuper important note: Computers treat lowercase and uppercase letters differently.\n\nThe equal operator can be used with a string on the left-hand side an a vector of strings on the right-hand side, for example:\n\n\"apple\" == c(\"banana\", \"apple\", \"mango\") returns FALSE TRUE FALSE\nQuick discussion: Speculate with a neighbor about the reason the above expression returns FALSE TRUE FALSE.\n\n\n\nActivity\n\nStudents use these three operators to create variables and vectors, and test for inclusion of a string in a vector of strings."
  },
  {
    "objectID": "lessons/programming_basics.html#comments",
    "href": "lessons/programming_basics.html#comments",
    "title": "Programming basics",
    "section": "Comments",
    "text": "Comments\n\nComments within computer code helps the human readers, whether other humans or your later self, to quickly understand what the various parts of a computer script do.\nComments in R are specified with a hashtag, for example:\n\n\n# assign a value to a variable\ndog &lt;- \"fido\"\n\n# create a vector of multiple elements\nkids &lt;- c(\"Bobby\", \"Luisa\", \"José\")"
  },
  {
    "objectID": "lessons/programming_basics.html#if-else-in-r",
    "href": "lessons/programming_basics.html#if-else-in-r",
    "title": "Programming basics",
    "section": "if else in R",
    "text": "if else in R\n\nThe logic is simple: Ask a question, and if the answer is TRUE, then do this thing, but if the answer is FALSE, then do that thing.\n\n\n\nTwo approaches to if else in R:\n\nThe most common approach is to use a code block. See an example in this Stack Overflow answer.\nA less common approach, but super useful for simple if else cases, is to use a function:\n\nbase R ifelse() function;\ndpylr (part of the tidyverse ecosystem) if_else() function.\n\n\n\nActivity\n\nStudents create a string with a single word, and then use if else (either a code block of a function) to print to the user whether the word begins with a vowel or a consonant.\n\nHint 1: The print() and cat() can be used to print to the console.\nHint 2: The str_sub() function can be used to extract a sub part of a string.\nHint 3: The %in% operator tests whether the left-hand value is within the right-hand collection."
  },
  {
    "objectID": "lessons/programming_basics.html#loops-in-r",
    "href": "lessons/programming_basics.html#loops-in-r",
    "title": "Programming basics",
    "section": "Loops in R",
    "text": "Loops in R\n\nThe mighty and super useful for loop iterates over all elements of a collection (e.g., a vector), for example see below (and see another example):\n\n\n# create a vector\nfruits &lt;- c(\"apple\", \"mango\", \"banana\", \"orange\")\n\n# loop over the elements of the vector\nfor (fruit in fruits) {\n  print(fruit)  # print the current element to the console\n}\n\n\nThe less-common-but-still-useful while loop tests the conditional statement at the beginning of each iteration and runs the body of the loop if the statement evaluates to TRUE. See an example.\nUseful keywords for both for loops and while loops:\n\nThe next keyword skips the rest of the current iteration and continues to the next iteration. This is very much like continue in Python and Julia.\nThe break keyword stops the loop completely, regardless of which iteration it was in, and no further iteration are executed.\n\n\nActivity\n\nStudents create a for loop to iterate from 1 to 10, skipping even numbers and printing out odd numbers.\n\nHint: The modulus operator %% will be helpful."
  },
  {
    "objectID": "lessons/programming_basics.html#defining-functions-in-r",
    "href": "lessons/programming_basics.html#defining-functions-in-r",
    "title": "Programming basics",
    "section": "Defining functions in R",
    "text": "Defining functions in R\n\nA very useful ability in R (and all programming languages) is for a user to define their own custom function.\nThe function() function does the trick.\n\nSee a tutorial.\n\n\nActivity\n\nStudents define a function that takes as input a word and returns as output a logical value (aka. Boolean value) indicating whether the word begins with one of the five orthographic vowels (i.e., a, e, i, o, u).\n\nHint: The %in% keyword will be helpful here.\n\nNow for a little fun and to put these basic programming skills together: Students define a function (likely with smaller helper functions) that translates a sentence from English into Pig Latin. A little refresher on Pig Latin: If a word begins with a vowel, the word yay is added to the end of it; if a word begins with a consonant or consonant cluster (e.g., ch, gr), that consonant or consonant cluster is moved to the end of the word followed by ay. For example How are you? &gt; Owhay areyay ouyay?\n\nHint: the stringr package (part of the tidyverse) will be useful here, especially the str_c(), str_sub(), str_split() (and unlist()) functions, as will the letters and LETTERS built-in constants.\nHint: After a good-faith effort, if you need help, see the script written by Dr. Brown by clicking on “▶ Code” below.\n\n\n\n\nCode\nsuppressPackageStartupMessages(library(\"tidyverse\"))\n\n# helper function 1, for vowel words\ntrans_v &lt;-  function(wd, vowels) {\n  return(str_c(wd, \"yay\"))\n}\n\n# helper function 2, for consonant words\ntrans_c &lt;- function(wd, vowels, first_let) {\n  second_let &lt;- str_sub(wd, 2, 2)\n  if (!str_to_lower(second_let) %in% vowels) {\n    first_two &lt;- str_sub(wd, 1, 2)\n    rest_wd &lt;- str_sub(wd, 3, str_length(wd))\n    return(str_c(rest_wd, first_two, \"ay\"))\n  } else {\n    rest_wd &lt;- str_sub(wd, 2, str_length(wd))\n    return(str_c(rest_wd, first_let, \"ay\"))\n  }\n}\n\n# the main function\ntrans_pig &lt;- function(sentence, vowels) {\n  wds &lt;- unlist(str_split(sentence, \"\\\\s+\"))\n  trans_sent &lt;- \"\"\n  for (wd in wds) {\n    first_let &lt;- str_sub(wd, 1, 1)\n    if (str_to_lower(first_let) %in% vowels) {\n      # this is a vowel word\n      trans_sent &lt;- str_c(trans_sent, trans_v(wd, vowels), \" \")\n    } else {\n      # the current word is a consonant word\n      trans_sent &lt;- str_c(trans_sent, trans_c(wd, vowels, first_let), \" \")\n    }\n  }\n  return(str_trim(trans_sent))\n}\n\n### test the function\nsentence &lt;- \"I do not like green eggs and ham.\"\nvowels &lt;- c(\"a\", \"e\", \"i\", \"o\", \"u\")\nprint(trans_pig(sentence, vowels))"
  },
  {
    "objectID": "lessons/quanteda.html",
    "href": "lessons/quanteda.html",
    "title": "quanteda",
    "section": "",
    "text": "Students will analyze textual data (aka. texts) with the quanteda R package."
  },
  {
    "objectID": "lessons/quanteda.html#objective",
    "href": "lessons/quanteda.html#objective",
    "title": "quanteda",
    "section": "",
    "text": "Students will analyze textual data (aka. texts) with the quanteda R package."
  },
  {
    "objectID": "lessons/quanteda.html#using-r-for-textual-analysis",
    "href": "lessons/quanteda.html#using-r-for-textual-analysis",
    "title": "quanteda",
    "section": "Using R for textual analysis",
    "text": "Using R for textual analysis\nThe quanteda R package is a super useful package for analyzing texts in R. Some of the techniques are corpus linguistic techniques through and through: tokenizing into words or sentences, keyword-in-context, removing stopwords. Other techniques might be better considered computational linguistic techniques: sentiment analysis, document feature matrix. Regardless, we need to get to know this package, as I can’t call myself a good professor of Linguistics Data Analysis with R without looking at this package."
  },
  {
    "objectID": "lessons/quanteda.html#install-the-package-and-its-friends",
    "href": "lessons/quanteda.html#install-the-package-and-its-friends",
    "title": "quanteda",
    "section": "Install the package and its friends",
    "text": "Install the package and its friends\nStep #1: Install the quanteda package and the other related package that the quanteda creators recommend:\n\ninstall.packages(c(\"quanteda\", \"quanteda.textmodels\", \"quanteda.textstats\", \"quanteda.textplots\", \"readtext\", \"spacyr\", \"remotes\"), repos = \"http://cran.rstudio.com\")\n\nThen, we need to install two packages on a Github:\n\nremotes::install_github(\"quanteda/quanteda.corpora\")\nremotes::install_github(\"kbenoit/quanteda.dictionaries\")"
  },
  {
    "objectID": "lessons/quanteda.html#create-a-corpus",
    "href": "lessons/quanteda.html#create-a-corpus",
    "title": "quanteda",
    "section": "Create a corpus",
    "text": "Create a corpus\nLet’s create a corpus in quanteda! First, let’s get their example code here working on our computers.\nLet’s read in the three currently published volumes of Saints. First, download the zipped file Saints.zip from the LMS. Then, unzip (aka. decompress or extract) the zipped file so that you end up with a directory with several subdirectories organized by volume number.\nLet’s read in the corpus such that we add a column with metadata about which volume each file is from. There are three ways (that Dr. Brown can think of) to do that.\n\nOption 1: Each volume individually (most verbose)\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# load each volume as a separate corpus\nsaints01 &lt;- readtext(\"Volume01/*.txt\") %&gt;% corpus()  # same as: saints01 &lt;- corpus(readtext(\"Volume01/*.txt\"))\ndocvars(saints01, \"Volume\") &lt;- \"01\"\n\nsaints02 &lt;- readtext(\"Volume02/*.txt\") %&gt;% corpus()\ndocvars(saints02, \"Volume\") &lt;- \"02\"\n\nsaints03 &lt;- readtext(\"Volume03/*.txt\") %&gt;% corpus()\ndocvars(saints03, \"Volume\") &lt;- \"03\"\n\n# combine the three corpora into one, with the docvar column identifying which volume each file comes from\nsaints &lt;- saints01 + saints02 + saints03\n\nprint(summary(saints))\n\n\n\nOption 2: Automate the previous approach (less verbose)\nWe can use the eval() and parse() functions in base R to automate the previous approach. Shall we?\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# our ol' friend the for loop \nfor (i in 1:3) {\n  to_str &lt;- str_glue(\"saints0{i} &lt;- readtext('Volume0{i}/*.txt') %&gt;% corpus(); docvars(saints0{i}, 'Volume') &lt;- '0{i}'\")\n  eval(parse(text = to_str))\n}\n\n# combine the three corpora into a new fourth one\nsaints &lt;- saints01 + saints02 + saints03\n\nprint(summary(saints))\n\n\n\nOption 3: Use the docvarfrom, dvsep, and docvarnames arguments (least verbose)\nThe readtext() function has a handful of arguments. The docvarfrom, dvsep, and the docvarnames arguments can create a new docvar (i.e., metadata about the corpus) from the filenames and/or the filepaths. Take a look at the documentation here.\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# one pipeline will do the trick!\nsaints &lt;- readtext(file = \"*/*.txt\", docvarsfrom = \"filepaths\", dvsep = \"/\", docvarnames = c(\"Volume\", \"filename\")) %&gt;% corpus()\n\nprint(summary(saints))\n\n\n\nActivity\nYou guessed it! It’s your turn. Create a corpus of your choice with files of your choice (e.g., perhaps from Project Gutenberg). Add a docvar of your choice, which may mean you need to preprocess the files a bit by specifying filenames or directory structure in a certain way."
  },
  {
    "objectID": "lessons/quanteda.html#creating-subcorpora",
    "href": "lessons/quanteda.html#creating-subcorpora",
    "title": "quanteda",
    "section": "Creating subcorpora",
    "text": "Creating subcorpora\nIt is super simple to create a subcorpus using a docvar (i.e., metadata about the files in the corpus). For example, let’s say we have our one Saints corpus with a docvar of Volume with values of “01”, “02”, and “03”. In order to create a subcorpus of just the files in Volume 1, we could run the following code. See the docs here.\n\nvol01 &lt;- corpus_subset(saints, Volume == \"01\") \n\n\nActivity\nGive it a try! Create a subcorpus from a larger corpus of your choice using the docvar of your choice."
  },
  {
    "objectID": "lessons/quanteda.html#keyword-in-context-kwic",
    "href": "lessons/quanteda.html#keyword-in-context-kwic",
    "title": "quanteda",
    "section": "Keyword-in-context (KWIC)",
    "text": "Keyword-in-context (KWIC)\nLet’s perform into a corpus linguistic technique: the keyword-in-context display or concordance lines or concordances. It is super easy to do so with the tokens() and kwic() functions. See example here.\nLet’s start out slowly with a simple word search using our Saints corpus:\n\n# Assuming packages are loaded and corpus has been created\n\n# tokenize and then get concordances\ntoks &lt;- tokens(saints)\nconcordances &lt;- kwic(toks, pattern = \"tree\")\nprint(concordances)\n\nLet’s ramp it up with some regular expression pizzazz. Question: What does the regular expression find?\n\ntoks &lt;- tokens(saints)\nconcordances &lt;- kwic(toks, pattern = \"\\\\w+(\\\\w{2,})\\\\W+\\\\w+\\\\1\\\\b\", valuetype = \"regex\")\nprint(concordances)\n\nThe main tokenizer function tokens() has a handful of arguments that are worth inspecting. Take a gander here. Also, there many other tokenizer functions that can be useful. Take a look here.\n\nActivity\nGet to know the tokenizing functions and the kwic() function (example here and docs here) with the corpus of your choice."
  },
  {
    "objectID": "lessons/quanteda.html#searching-with-a-dictionary",
    "href": "lessons/quanteda.html#searching-with-a-dictionary",
    "title": "quanteda",
    "section": "Searching with a dictionary",
    "text": "Searching with a dictionary\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# one pipeline will do the trick!\nsaints &lt;- readtext(file = \"*/*.txt\", docvarsfrom = \"filepaths\", dvsep = \"/\", docvarnames = c(\"Volume\", \"filename\")) %&gt;% corpus()\n\n# create a dictionary with named vectors\ndict &lt;- dictionary(list(temple = c(\"temple\", \"sealing\", \"spire\", \"open house\"),\n                missionary = c(\"missionary\", \"mission\", \"labor\\\\b\")))\n\n# search with the dictionary\nkwic(tokens(saints), pattern = dict, valuetype = \"regex\") %&gt;% print()"
  },
  {
    "objectID": "lessons/quanteda.html#collocations",
    "href": "lessons/quanteda.html#collocations",
    "title": "quanteda",
    "section": "Collocations",
    "text": "Collocations\nYou can retrieve collocations with the textstat_collocations() function here. At the time of making this lesson plan, it only calculates the lambda metric proposed by Blaheta and Johnson (2001):\nBlaheta, D. & Johnson, M. (2001). Unsupervised learning of multi-word verbs. Presented at the ACLEACL Workshop on the Computational Extraction, Analysis and Exploitation of Collocations.\nHowever, the doc page says that there are plans to add more measures. Hopefully log-dice is in the works, as it’s a great word association metric.\nLet’s retrieve the collocations in the first three volumes of Saints:\n\nlibrary(quanteda)\nlibrary(readtext)\nlibrary(tidyverse)\nlibrary(quanteda.textstats)\n\n# change directories into the Saints directories\nsetwd(\"/pathway/to/Saints/txt\")\n\n# load the Saints corpus, creating a docvar with the Volume that each file belongs to\nsaints &lt;- readtext(file = \"*/*.txt\", docvarsfrom = \"filepaths\", dvsep = \"/\", docvarnames = c(\"Volume\", \"filename\")) %&gt;%  corpus() \n# retrieve collocations\nsaints %&gt;% \n  tokens() %&gt;% \n  textstat_collocations()  %&gt;% \n  arrange(desc(lambda)) %&gt;%  # arrange them in descending order by lambda\n  head(50)  # show only first 50 collocations\n\nThat’s okay, but there are lots of proper nouns and low-frequency collocations. We can remove those with the min_count argument: textstat_collocations(min_count = 25).\nIf we want, we can also remove function words, which have little semantic value, with a stopword list:\n\nlibrary(\"stopwords\")\nsaints %&gt;% \n  tokens() %&gt;% \n  tokens_remove(stopwords(\"en\")) %&gt;% \n  textstat_collocations(min_count = 25) %&gt;% \n  arrange(desc(lambda)) %&gt;% \n  head(50)\n\n\nActivity\nGo for it! That is, retrieve collocations in a corpus of your choice. Look at and possibly use some of the other arguments in the textstat_collocations() function. Reminder: In order to pull up the doc page of a function within the RStudio IDE, you can type a question mark and then the name of the function (no space between them) in the console, e.g., ?textstat_collocations. And here’s that same doc page on the internet."
  },
  {
    "objectID": "lessons/quanteda.html#document-feature-matrix",
    "href": "lessons/quanteda.html#document-feature-matrix",
    "title": "quanteda",
    "section": "Document-feature matrix",
    "text": "Document-feature matrix\nWithin quanteda, a document-feature matrix is very similar to a document-term matrix (if you know what that is). It’s a tabular dataset (hence the name “matrix”) and has the name of the files (aka. documents) as rows, and the features (i.e., words and punctuation) are the columns. Here’s an example. Let’s take a look with the Saints corpus:\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# one pipeline will do the trick!\nsaints &lt;- readtext(file = \"*/*.txt\", docvarsfrom = \"filepaths\", dvsep = \"/\", docvarnames = c(\"Volume\", \"filename\")) %&gt;% corpus()\n\nsaints %&gt;% \n  tokens() %&gt;% \n  dfm()\n\nWe can use a dictionary of patterns to limit the number of features (i.e., columns) that are returned in the matrix:\n\ndict &lt;- dictionary(list(\n  temple = c(\"temple\", \"sealing\", \"spire\", \"open house\"),\n  missionary = c(\"missionary\", \"mission\", \"labor\"))\n  )\n\nsaints %&gt;% \n  tokens() %&gt;% \n  tokens_lookup(dictionary = dict) %&gt;% \n  dfm() %&gt;% \n  as_tibble() %&gt;%  # cast to tibble in order to use arrange below\n  arrange(desc(temple))"
  },
  {
    "objectID": "lessons/quanteda.html#similarities-between-texts",
    "href": "lessons/quanteda.html#similarities-between-texts",
    "title": "quanteda",
    "section": "Similarities between texts",
    "text": "Similarities between texts\nYou can plot similarities between texts in a corpus. Let’s run the example code in the Quick Guide here.\nNow, let’s make this work with our Saints corpus."
  },
  {
    "objectID": "lessons/quanteda.html#keyness-analysis",
    "href": "lessons/quanteda.html#keyness-analysis",
    "title": "quanteda",
    "section": "Keyness analysis",
    "text": "Keyness analysis\nWe can use quanteda to perform keyness analysis, that is, to identify the keywords that typify a section of the corpus from the rest of the corpus.\nLet’s get the creators’ example working in the doc page (in console ?textstat_keyness) and here.\n\nActivity\nTake some time to look over the other analysis abilities of quanteda and try to get one or two working with a corpus of your choice."
  },
  {
    "objectID": "lessons/multicollinearity.html#objective",
    "href": "lessons/multicollinearity.html#objective",
    "title": "Multicollinearity",
    "section": "",
    "text": "Students measure the amount of multicollinearity among explanatory variables."
  },
  {
    "objectID": "lessons/File_IO.html#arrow-aka.-feather-files",
    "href": "lessons/File_IO.html#arrow-aka.-feather-files",
    "title": "File I/O in R",
    "section": "Arrow (aka. feather) files",
    "text": "Arrow (aka. feather) files\n\nNote: The Arrow ICP file format was formerly known as Feather file format.\nThis file format is quickly read and written, which are good for big data files.\nReading in (input)\n\nThe arrow::read_feather() function does it.\n\nWriting out (output)\n\nThe arrow::write_feather() function does it.\n\nActivity\n\nDownload the “freqs_Saints.arrow” file from the Dataset module in Canvas.\nRead it into R and display it with the view() function."
  },
  {
    "objectID": "lessons/webscrape_freq.html#multiple-html-tables",
    "href": "lessons/webscrape_freq.html#multiple-html-tables",
    "title": "Webscrape frequencies",
    "section": "Multiple HTML Tables",
    "text": "Multiple HTML Tables\nIf the webpage that you want to harvest an HTML Table from has multiple tables, you can use an index to get the particular table that you need. Let’s take an example from the chess world. The Wikipedia article World Chess Championship has several HTML Tables, one of which is titled “FIDE (reunified) world champions (2006–present)”. We’ll need to use the html_elements() function (mind the plural marker on the function name) rather than html_element() that we used above, and then use an integer index to get the table we want. We can figure out what the integer index is for the table we need by trial and error.\nHere’s some code to get that particular HTML Table:\n\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\"https://en.wikipedia.org/wiki/World_Chess_Championship\" %&gt;%  # put URL in a string\n  read_html() %&gt;%  # request page\n  html_elements(\"table\") %&gt;%  # find all \"table\" HTML elements; mind the plural \"html_elements\"\n  .[6] %&gt;% # get the sixth HTML Table on the webpage\n  html_table(header = TRUE) %&gt;%  # convert HTML table to data frame (i.e., tibble)\n  print()\n\n\nActivity\nInstructions: Find a webpage of your choice with two or more HTML Tables. If you’re struggling to find such a webpage, you can use the Wikipedia article about Denmark. You should inspect the HTML code behind the webpage in order to ensure that the tables are HTML Tables, that is, that they have &lt;table&gt; HTML tags. Pull out the table of your choice by modifying the code given above."
  }
]