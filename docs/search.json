[
  {
    "objectID": "lessons/File_IO.html",
    "href": "lessons/File_IO.html",
    "title": "File I/O in R",
    "section": "",
    "text": "Students will read data from files and writing data to files."
  },
  {
    "objectID": "lessons/File_IO.html#objective",
    "href": "lessons/File_IO.html#objective",
    "title": "File I/O in R",
    "section": "",
    "text": "Students will read data from files and writing data to files."
  },
  {
    "objectID": "lessons/File_IO.html#directory-operations",
    "href": "lessons/File_IO.html#directory-operations",
    "title": "File I/O in R",
    "section": "Directory operations",
    "text": "Directory operations\n\nComputers are structured in a hierarchal filesystem (e.g., “/Users/Fernando/Documents/my_novel.docx”).\nOften, R scripts need to change directories (aka. folders) in order to access specific files.\nR has several functions to move around a filesystem. Here are a few of them:\n\ngetwd() (here) prints to the console the current working directory (i.e., the directory at the script currently has access to).\nsetwd() (here) sets a directory so that the script has access to it.\ndir() (here, an alias for list.files()) lists the contents of a directory, either only in the one directory or also in all sub-directories (with the recursive argument)."
  },
  {
    "objectID": "lessons/File_IO.html#text-files-.txt",
    "href": "lessons/File_IO.html#text-files-.txt",
    "title": "File I/O in R",
    "section": "Text files (.txt)",
    "text": "Text files (.txt)\n\nReading in data (aka. input) from text files can be accomplished in two ways:\n\nWay 1: Slurp all text in the file at once and hold it in the working memory of the computer:\n\nscan() (here) returns a vector (the default) or list.\nreadLines() (here) in base R returns a vector with each line (i.e., hard return) in the input file as an element. This is a bare-bones version of scan().\nread_lines() (here) in the readr package (part of tidyverse) is a quicker version of readLines().\nread_file() (here) in the readr package slurps all text into a single string.\n\nWay 2: Read data line-by-line:\n\nThis is useful when the text file is massive and would be difficult to hold in memory at once. This approach holds only one line at a time in memory.\nSteps:\n\nCreate a connection to the file with the file() function (here).\nUse the readLines(n = 1) function in a while loop.\nSee an example here.\n\n\n\nWriting out data (aka. output) to a text files can be accomplished with several functions:\n\ncat() (here) in base R can write out to a text file when the file argument gives a pathway to a file.\nwriteLines() (here) is the output version of readLines() mentioned above.\nwrite_lines() (here) function is the output version of read_lines() mentioned above.\nwrite_file() (here) output-equivalent of read_file() above."
  },
  {
    "objectID": "lessons/File_IO.html#csv-files-.csv",
    "href": "lessons/File_IO.html#csv-files-.csv",
    "title": "File I/O in R",
    "section": "CSV files (.csv)",
    "text": "CSV files (.csv)\n\nReading in tabular datasets from CSV files can be accomplished with several functions:\n\nread.table() (here) is a versatile function with many arguments, that returns a data frame.\nread_csv() (here) in the readr package (part of tidyverse) reads CSV files that have a comma as the separator between columns, and returns a tibble.\nread_tsv() (here) in the readr package reads TSV files that have a tab as the separator between columns, and returns a tibble.\n\nNote: Some files with the extension .csv or .txt are actually .tsv files, that is, the column separator is a tab.\n\n\nWriting out a data frame to a CSV files is a cinch:\n\nwrite.table() (here) in base R is the output equivalent of read.table() mentioned above.\nwrite_csv() (here) in the readr package is the output equivalent of the read_csv() mentioned above.\n\n\n\nActivity\n\nInstall the tidyverse suite of packages with the command install.packages(\"tidyverse\") in the console.\nDownload some TXT files of your choice (perhaps from Project Gutenberg or Saints.zip from the Canvas Module “Datasets”).\nCreate a script that reads in all TXT files in a directory (and perhaps any subdirectories) and simply print the text to the console.\n\nInclude library(\"tidyverse\") at the top of your script (i.e., .r file.)\n\nRamp it up by breaking up the text into words and printing those to the console.\nNow for some fun, as a class let’s count the number of words in each text file, and print the name of the file and the number of words to the console.\nAs a final step, let’s write out a CSV file with two columns: column A = the name of the file, column B = the number of words in that file."
  },
  {
    "objectID": "lessons/File_IO.html#excel-files",
    "href": "lessons/File_IO.html#excel-files",
    "title": "File I/O in R",
    "section": "Excel files",
    "text": "Excel files\n\nReading in (input) an Excel is easy with readxl::read_excel().\n\nYou can specify which worksheet to read (with the sheet argument), or even a specific set of cells within a specific worksheet (with the range argument).\n\nWriting out (output)\n\nxlsx::write.xlsx() function does the trick.\n\n\n\nActivity\n\nDownload an Excel (.xlsx) file of your choice (perhaps from the Module “Datasets” in Canvas) or use one that’s already on your harddrive.\nOpen the Excel file (in Excel) and inspect the worksheet(s) to figure out where the data is (i.e., sheet name, cell range).\nRead in the appropriate worksheet and display it within RStudio with the view() function."
  },
  {
    "objectID": "lessons/File_IO.html#spss-.sav-stata-.dta-and-sas-.sas-files",
    "href": "lessons/File_IO.html#spss-.sav-stata-.dta-and-sas-.sas-files",
    "title": "File I/O in R",
    "section": "SPSS (.sav), Stata (.dta), and SAS (.sas) files",
    "text": "SPSS (.sav), Stata (.dta), and SAS (.sas) files\n\nReading in (input)\n\nThe haven R package does the trick.\n\nWriting out (output)\n\nWho cares? You should output it as something more cross-platform-friendly like CSV.\n\n\n\nActivity\n\nDownload the SPSS (.sav) file in the CMS.\nRead in the SPSS file and display it with view()."
  },
  {
    "objectID": "lessons/File_IO.html#feather-.feather-files",
    "href": "lessons/File_IO.html#feather-.feather-files",
    "title": "File I/O in R",
    "section": "Feather (.feather) files",
    "text": "Feather (.feather) files\n\nThis file format is quickly read and written, which are good for big data files.\nReading in (input)\n\nThe feather::read_feather() function does it.\n\nWriting out (output)\n\nThe feather::write_feather() function does it."
  },
  {
    "objectID": "lessons/data_structures.html",
    "href": "lessons/data_structures.html",
    "title": "Data structures in R",
    "section": "",
    "text": "Students will become familiar with common data structures in R."
  },
  {
    "objectID": "lessons/data_structures.html#objective",
    "href": "lessons/data_structures.html#objective",
    "title": "Data structures in R",
    "section": "",
    "text": "Students will become familiar with common data structures in R."
  },
  {
    "objectID": "lessons/data_structures.html#common-data-structures-in-r",
    "href": "lessons/data_structures.html#common-data-structures-in-r",
    "title": "Data structures in R",
    "section": "Common data structures in R",
    "text": "Common data structures in R\n\nvector: A single dimension collection of values of the same data type (e.g., all numeric or all character). Kinda like a list in Python or an array in Julia.\n\nNote: Values of different data types are coerced to the more complex data type, for example, if a numeric (aka. float) and an integer are put into the same vector, both values will have a numeric (aka. float) data type.\nA vector (and list, see below) can be created with the c() function (doc here).\n\ndata.frame: A tabular data structure with columns and rows, much like a table in a spreadsheet like Excel or Google Sheets. See doc here.\ntibble: A slightly modified, and better, data.frame in the tibble (doc here) package within the tidyverse metapackage or ecosystem (doc here). See doc here.\ndata.table: Another tabular data structure, written in C under the hood (so it’s fast) from the data.table package here. See a tutorial here.\nmatrix: Another tabular data structure in base R. Less common than data.frame and tibble and data.table. See doc here.\nlist: A flexible data structure that can hold whatever, including other data structures. See doc here and tutorial here.\n\nActivity\n\nStudents create a few vectors of the same length (i.e., the same number of elements).\nStudents create a single data frame with the several vectors created above. The doc here may be helpful.\nStudents create several data frames and put them in a list.\nStudent iterate over the list with a for loop and print each data frame to the console.\nStudents iterate over the list and then iterate over the row of each data frame, and print each row."
  },
  {
    "objectID": "lessons/create_freq.html",
    "href": "lessons/create_freq.html",
    "title": "Creating frequency lists",
    "section": "",
    "text": "Students will create frequency lists from files on their harddrive."
  },
  {
    "objectID": "lessons/create_freq.html#objective",
    "href": "lessons/create_freq.html#objective",
    "title": "Creating frequency lists",
    "section": "",
    "text": "Students will create frequency lists from files on their harddrive."
  },
  {
    "objectID": "lessons/create_freq.html#frequency-in-language",
    "href": "lessons/create_freq.html#frequency-in-language",
    "title": "Creating frequency lists",
    "section": "Frequency in language",
    "text": "Frequency in language\nFrequency is an important construct in many areas of language, and more generally, in human cognition (see our amazing pattern recognition abilities). Frequency affects which words and phrases are learned first, in both L1s and L2s. More frequent grammatical constructions are learned before (and better for L2 speakers) than less frequent ones (e.g., active voice vs. passive voice in English). More frequency words experience phonetically-driven sound changes (e.g., lenition) first. More frequent words resist analogical leveling (keep -&gt; *keeped, but leap -&gt; leapt -&gt; leaped).\nIn summary, frequency is super important, and being able to calculate frequencies of language features, especially words, is an important skill for a language-oriented data analyst."
  },
  {
    "objectID": "lessons/create_freq.html#getting-frequencies-of-words-in-files",
    "href": "lessons/create_freq.html#getting-frequencies-of-words-in-files",
    "title": "Creating frequency lists",
    "section": "Getting frequencies of words in files",
    "text": "Getting frequencies of words in files\nThe logic to calculate frequencies of files on a harddrive in R is simple (when holding all words in RAM; see an alternative below):\n\nParse the files so that all words in all files are in a single vector with N elements (N being the total number of words across all files);\nAsk R to count up the number of word tokens per word type in the vector.\n\nThere are two ways (Dr. Brown will show) to count up word tokens per word type:\n\nWith the base R table() function;\nConvert the vector into a one-column data frame and then use count() in tidyverse."
  },
  {
    "objectID": "lessons/create_freq.html#the-table-function",
    "href": "lessons/create_freq.html#the-table-function",
    "title": "Creating frequency lists",
    "section": "The table() function",
    "text": "The table() function\nLet’s create a toy example:\n\nlibrary(\"tidyverse\")\n\n# create a sentence\nsentence &lt;- \"I like linguistics, and I like my students, but I love my wife and chidren. Sorry students. Maybe next time.\"\n\n# uppercase (or lowercase) the string, so that uppercase and lowercase words (e.g., \"The\" and \"the\" and \"THE\") are treated as the same word\nsentence &lt;- str_to_upper(sentence)\n\n# tokenize the string into words\nwords &lt;- str_extract_all(sentence, \"[-'’A-Z]+\")\n\n# unlist the list so that we're left with a vector\nwords &lt;- unlist(words)\n\n# throw the vector at table() and watch the magic happen!\nfreqs &lt;- table(words)\nprint(freqs)\n\nThe output of the table() function is a named one-dimensional array (like a vector), of class table. The values are the integers (i.e., the frequencies), and each integer has a name (i.e., a word). In order to extract only the names, you can use the names() function:\n\nprint(names(freqs))\n\nWe probably want to sort the frequencies in descending order:\n\nfreqs &lt;- sort(freqs, decreasing = TRUE)\nprint(freqs)\n\nThe frequencies and their names (i.e., the words) can be put into a data frame and then exported out as a CSV file.\n\nActivity\nDo just that, that is, export to a CSV file the words and frequencies using the result of the above toy example. One tip: You’ll need to coerce the data type of the numbers to integer with as.integer(freqs) when assigning the array to a column in the data frame.\nAfter giving it a good-faith effort, if you need help, take a look at Dr. Brown’s code below:\n\n\nCode\ndf &lt;- tibble(wd = names(freqs), freq = as.integer(freqs))\nwrite_csv(df, file = \"freqs.csv\")\n\n\nLet’s ramp it up:\nCreate a frequency list of words in many text files of your choice (e.g., from Project Gutenberg or the Saints files in the LMS).\nAfter a good-faith effort, if you need help, take a look at Dr. Brown’s start to the code below:\n\n\nCode\n# get filenames\nfilenames &lt;- dir(path = \"/pathway/to/Saints/txt/\", pattern = \"\\\\.txt$\", full.names = TRUE, recursive = TRUE)\n\n# create a collector string to collect the text of all files\nall_str &lt;- \"\"\n\n# loop over the filenames\nfor (filename in filenames) {\n  \n  # get the text from the current file\n  txt &lt;- read_file(filename)\n  \n  # add the text of the current file to the collector string\n  all_str &lt;- str_c(all_str, txt, sep = \" \")\n  \n}  # next filename\n\n\nAfter the for loop, the variable all_str is one big string with all text from all files. You should now be able to modify the code in the toy example above to get the frequencies of the words in this single big string. Go for it! You got this! Let’s go! You’re a super star! Etc.!"
  },
  {
    "objectID": "lessons/create_freq.html#the-count-function",
    "href": "lessons/create_freq.html#the-count-function",
    "title": "Creating frequency lists",
    "section": "The count() function",
    "text": "The count() function\nA second way (among other ways) is to take the vector with words (not the big single string, but the vector with each word as a separate element) and create a one-column data frame, and then use count() (within tidyverse). Let’s go!\nUsing the words vector from the toy example above:\n\ndf &lt;- tibble(wd = words)\nfreqs &lt;- df %&gt;% count(wd)\nprint(freqs)\n\nQuick little aside: The pipe operator %&gt;% passes the value on the left-hand side of the operator into the function on the right-hand side, as the first argument to that function.\n\nActivity\nThat’s right, it’s time to step up and use the count() function to calculate frequencies and then write them out to a CSV file. Ready… set… go!\nAfter a good-faith effort, if you need some help, take a look at Dr. Brown’s code below:\n\n\nCode\n# using the toy example above:\n# (enjoy all the pipe operators!)\ntibble(wd = words) %&gt;% \n  count(wd) %&gt;% \n  write_csv(file = \"freqs.csv\")"
  },
  {
    "objectID": "lessons/create_freq.html#populate-a-list",
    "href": "lessons/create_freq.html#populate-a-list",
    "title": "Creating frequency lists",
    "section": "Populate a list",
    "text": "Populate a list\nA third way to calculate frequencies of words in files is to create an R list and populate the list with words, one file at a time. This is a good way if the corpus that you want frequencies from is too big to fit in the RAM of your computer, or too big to fit comfortably because it makes your computer work slowly. You can have your script bring into the RAM only one file at a time and populate an R list. This approach to getting frequencies is slower than the previous two, so unless you have a good reason for using this approach, it’s probably best to one of the previous two approaches (fun fact: Dr. Brown likes the count() approach best).\nLet’s take a look at an example using the Saints corpus:\n\n# get filenames\nfilenames &lt;- dir(path = \"/pathway/to/Saints/txt/\", pattern = \"\\\\.txt$\", full.names = TRUE, recursive = TRUE)\n\n# create a collector string to collect the text of all files\nfreqs &lt;- list()\n\n# loop over the filenames\nfor (filename in filenames) {\n  \n  # get the text from the current file and uppercase it\n  txt &lt;- read_file(filename) %&gt;% str_to_upper()\n  \n  # get words in current file and unlist the list into a vector\n  wds &lt;- str_extract_all(txt, \"[-'’A-Z]+\") %&gt;% unlist()\n  \n  # loop over the vector of words\n  for (wd in wds) {\n    \n    # test whether the current word is already in the list\n    if (wd %in% names(freqs)) {\n      \n      # if so, increment the counter for that word by one\n      freqs[[wd]] &lt;- freqs[[wd]] + 1\n      \n    } else {\n      # if not, add an entry for the word and give it a value of one\n      freqs[[wd]] &lt;- 1\n      \n    }  # end if\n  }  # next word  \n}  # next filename\n\n# print out the words and their frequencies to the console\nfor (wd in names(freqs)) {\n  cat(wd, \": \", freqs[[wd]], \"\\n\", sep=\"\")\n}\n\nThe next step would be put the words and their frequencies into a data frame in order to (more easily) sort them and to (more easily) write them out to a CSV file."
  },
  {
    "objectID": "lessons/chi-square.html",
    "href": "lessons/chi-square.html",
    "title": "Chi-square test",
    "section": "",
    "text": "Students will perform a chi-square (𝜒2) test of counts."
  },
  {
    "objectID": "lessons/chi-square.html#objective",
    "href": "lessons/chi-square.html#objective",
    "title": "Chi-square test",
    "section": "",
    "text": "Students will perform a chi-square (𝜒2) test of counts."
  },
  {
    "objectID": "lessons/chi-square.html#count-em-up",
    "href": "lessons/chi-square.html#count-em-up",
    "title": "Chi-square test",
    "section": "Count ’em up!",
    "text": "Count ’em up!\nThe chi-square test is the go-to test for counts of levels in two categorical variables. For example:\n\nbathroom v. washroom in American English v. Canadian English\ngive it to me v. give me it in academic v. spoken English\nLatin v. Greek borrowing in legal v. medical jargon"
  },
  {
    "objectID": "lessons/chi-square.html#contingency-tables",
    "href": "lessons/chi-square.html#contingency-tables",
    "title": "Chi-square test",
    "section": "Contingency tables",
    "text": "Contingency tables\nWhen we have two categorical variables, we can (and really should) create a contingency table (aka. cross tab). Let’s create some fictitious data for the first example above:\n\n\n\n\nAmerican Eng.\nCanadian Eng.\n\n\nbathroom\n23\n10\n\n\nwashroom\n34\n45"
  },
  {
    "objectID": "lessons/chi-square.html#margins",
    "href": "lessons/chi-square.html#margins",
    "title": "Chi-square test",
    "section": "Margins",
    "text": "Margins\nThe margins in the context of contingency tables are the row and columns totals:\n\n\n\n\nAmerican Eng.\nCanadian Eng.\nTotal\n\n\nbathroom\n23\n10\n33\n\n\nwashroom\n34\n45\n79\n\n\nTotal\n57\n55\n112"
  },
  {
    "objectID": "lessons/chi-square.html#observed-v.-expected",
    "href": "lessons/chi-square.html#observed-v.-expected",
    "title": "Chi-square test",
    "section": "Observed v. expected",
    "text": "Observed v. expected\nAnother super important step in performing the chi-square test is calculating the expected values. Let’s take bathroom in American English for a moment. We see that our (fictitious) count is 23, which is our observed value. For each cell in the contingency table, we need to calculate the expected value, and that is accomplished by taking the row total and multiplying it by the column total and then dividing that product by the total N: \\(33\\ *\\ 57\\ /\\ 112\\ =\\ 16.79464\\)\nWhen we calculate the expected values for each cell in the contingency table, we end up with the following expected values:\n\n\n\n\nAmerican Eng.\nCanadian Eng.\n\n\nbathroom\n16.79464\n16.20536\n\n\nwashroom\n40.20536\n38.79464\n\n\n\nLet’s run a chi-square test:\n\npotty &lt;- matrix(data = c(23, 10, 34, 45), nrow = 2, byrow = TRUE)\nprint(potty)\n\n     [,1] [,2]\n[1,]   23   10\n[2,]   34   45\n\nresult = chisq.test(potty)\nprint(result)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  potty\nX-squared = 5.5955, df = 1, p-value = 0.01801\n\n\nSo yeah, we reject the null hypothesis that there is no association between term for where to go potty and the country where English is spoken.\nIf we want to see the expected values of the contingency table, we can ask for it from the result of the chisq.test() call:\n\nresult$expected\n\n         [,1]     [,2]\n[1,] 16.79464 16.20536\n[2,] 40.20536 38.79464\n\n\nIf we can to see which cell contributes (the most) to a chi-square test being significant, we can look for the largest absolute residual value:\n\nresult$residuals\n\n           [,1]       [,2]\n[1,]  1.5141936 -1.5414785\n[2,] -0.9786442  0.9962788\n\n\nCanadians say bathroom too infrequently.\n\nAssumption\nIf any of the expected values is 5 or smaller, we can’t use the chi-square. Instead, we’d have to use a Fisher Exact test (this isn’t the case with our fictitious dataset, but let’s run a Fisher Exact test for fun):\n\nfisher.test(potty)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  potty\np-value = 0.01288\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 1.191312 8.099109\nsample estimates:\nodds ratio \n  3.013478"
  },
  {
    "objectID": "lessons/chi-square.html#activity",
    "href": "lessons/chi-square.html#activity",
    "title": "Chi-square test",
    "section": "Activity",
    "text": "Activity\nYour turn! Using a dataset of your choice, create a contingency table of two categorical variables of your choice. Then, calculate the a chi-square test or a Fisher Exact test (depending on if any of the expected values is 5 or smaller)."
  },
  {
    "objectID": "lessons/visualization.html",
    "href": "lessons/visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "Students visualize data with easy-to-interpret plots."
  },
  {
    "objectID": "lessons/visualization.html#objective",
    "href": "lessons/visualization.html#objective",
    "title": "Visualization",
    "section": "",
    "text": "Students visualize data with easy-to-interpret plots."
  },
  {
    "objectID": "lessons/visualization.html#background",
    "href": "lessons/visualization.html#background",
    "title": "Visualization",
    "section": "Background",
    "text": "Background\nR kicks trash with visualizing data. And the go-to package for creating professional-looking plots is ggplot2, one of the core packages of the tidyverse ecosystem. Base R also creates plots, but anybody who’s anybody is using ggplot2. (Okay, maybe I’m overstating this a bit, but just you wait and see if I’m lying.)"
  },
  {
    "objectID": "lessons/visualization.html#basics",
    "href": "lessons/visualization.html#basics",
    "title": "Visualization",
    "section": "Basics",
    "text": "Basics\nFirst, we need some data in a data frame (e.g., a tibble). Let’s use the Data_ptk.xlsx file in the LMS:\n\nlibrary(\"tidyverse\")\nlibrary(\"readxl\")\n\nptk &lt;- read_excel(\"../../../data_analysis/datasets/Data_ptk.xlsx\", sheet = \"data\")\nglimpse(ptk)\nhead(ptk)\n\nThe function ggplot() (without a 2, like in the package name) creates a plotting window or palette:\n\nggplot()\n\nThe first argument to ggplot() is a data frame, which can be piped in:\n\nggplot(ptk)\n\n# same as above, but with a pipe\nptk |&gt; ggplot()\n\nThe second argument to ggplot() is a call to the aes() function (short for “aesthetics”) which maps columns in the data frame to elements on the plot. The first argument to aes() is what will be plotted on the x-axis and the second argument, if needed, is what will be plotted on the y-axis. For example, let’s put the levels of LANG on the x-axis:\n\nptk |&gt; \n  ggplot(aes(x = LANG))\n\nOr we can put the prePhonBin on the y-axis:\n\nptk |&gt; \n  ggplot(aes(y = prePhonBin))\n\nYou’ll notice that the above plots don’t actually plot anything yet; they simply set up the plotting window with the right axis labels. We now need to add elements to the plot."
  },
  {
    "objectID": "lessons/visualization.html#one-categorical-variable",
    "href": "lessons/visualization.html#one-categorical-variable",
    "title": "Visualization",
    "section": "One categorical variable",
    "text": "One categorical variable\nPerhaps the most basic plot is of one categorical variable. In order to see the distribution of the levels (aka. values or groups) of a categorical variable, let’s create a popular option: the good ol’ fashioned barplot. Note: The several layers of the plot are separated by a plus sign + rather than the pipe operator, and the layers are often specified with a geom_...() function (short for geometric object). See the section about layers in the reference manual of ggplot2.\n\nptk |&gt; \n  ggplot(aes(x = LANG)) +\n  geom_bar()\n\nIt might be good to also get the exact number of tokens in each level with our ol’ friend:\n\nptk |&gt; \n  count(LANG)\n\n\nActivity\n\nTake a look at the variety of barplots that are available in ggplot.\nThen, using the Data_ptk.xlsx dataset, plot the distribution of several categorical variables, for example: LANG, GENRE, prePhonBin, folPhonBin, prevMention, cogStatus, lexClass, wdClass."
  },
  {
    "objectID": "lessons/visualization.html#two-categorical-variables",
    "href": "lessons/visualization.html#two-categorical-variables",
    "title": "Visualization",
    "section": "Two categorical variables",
    "text": "Two categorical variables\nThe following are popular plots to visualize the distribution of data points across two categorical variables.\n\nStacked barplot\nTo create a stacked (aka. filled) barplot, one categorical variable is given as x in aes() and the other is given as the fill argument in aes(), for example:\n\nptk |&gt; \n  ggplot(aes(x = LANG, fill = prePhonBin)) +\n  geom_bar()\n\nSee more examples of barplot in the docs.\nFollow-up question: How can we get the exact numbers of tokens in each of the subgroups displayed in the plot? After a good-faith effort, if you need help, take a look at Dr. Brown’s code below:\n\n\nCode\nptk |&gt; \n  count(LANG, prePhonBin)\n\n\n\n\nPercent barplot\nTo create a percent barplot, that is, a stacked barplot that sums to 1.0, let’s modify the previous barplot by adding the argument position = \"fill\" to geom_bar(), but outside of aes() (which isn’t used at all).\n\nptk |&gt; \n  ggplot(aes(x = LANG, fill=prePhonBin)) +\n  geom_bar(position = \"fill\")\n\nWe notice that the y-axis is labeled “count”, but those aren’t counts, they’re proportions. Let’s change the axis labels with the labs() element:\n\nptk |&gt; \n  ggplot(aes(x = LANG, fill=prePhonBin)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Language\", y = \"Proportion\")\n\n\n\nGrouped barplot\nTo create a grouped barplot, modify the previous barplot by changing the argument to position = \"dodge\". Simple enough.\n\nptk |&gt; \n  ggplot(aes(x = LANG, fill = prePhonBin)) +\n  geom_bar(position = \"dodge\")\n\n\n\nMosaic plot\nAnother useful plot for two categorical variables is a mosaic plot, which displays proportions as relative sizes of the various pieces of the mosaic. Let’s download an R package that can be used to create mosaic plots: vcd has a transparently named package name: Visualizing Categorical Data.\nLet’s plot prePhonBin on the x-axis and LANG on the y-axis:\n\nlibrary(\"vcd\")\n\ntemp &lt;- ptk |&gt; \n  count(LANG, prePhonBin) |&gt; \n  pivot_wider(names_from = prePhonBin, values_from = n)\nrow_names &lt;- temp |&gt; pull(1)\ntemp &lt;- temp |&gt; data.matrix()\ntemp &lt;- temp[,-1]\nrownames(temp) &lt;- row_names\nvcd::mosaic(temp, shade = TRUE, varnames = FALSE)\n\n\n\nActivity\n\nGive it a whirl! Using the Data_ptk.xlsx dataset, plot the distribution of sets of two categorical variables, e.g., LANG and GENRE, prePhonBin and folPhonBin, prevMention and cogStatus, lexClass and wdClass, and any combination of these variables."
  },
  {
    "objectID": "lessons/visualization.html#one-continuous-variable",
    "href": "lessons/visualization.html#one-continuous-variable",
    "title": "Visualization",
    "section": "One continuous variable",
    "text": "One continuous variable\nIt is useful to visualize the distribution of continuous variables in order to see whether the variable is distributed normally or has some skew or is bimodal.\nThe following plots are useful for this purpose. You only need to specify the x argument in aes() within ggplot() with the one variable.\n\nHistogram\nA histogram slices or bins of a continuous variable into N bins and then plots the count within each bin on the y-axis. It gives a barplot (i.e., it looks like a barplot), but instead of levels within a categorical variable, the bars are equidistant intervals across a continuous variable. Here’s an example with the continuous variable VOT in the ptk dataset:\n\nptk |&gt; \n  ggplot(aes(x = VOT)) +\n  geom_histogram()\n\nYou can specify a specific number of bins with the bins argument, for example:\n\nptk |&gt; \n  ggplot(aes(x = VOT)) +\n  geom_histogram(bins = 10)\n\nInstead of the number of bins (i.e., bars), you can specify the width of each bin on the scale of the variable that you’re plotting. This is useful because the bin width is often more meaningful than a simple number of bins. Let’s create a bin width of 0.01 seconds (aka. 10 milliseconds):\n\nptk |&gt; \n  ggplot(aes(x = VOT)) +\n  geom_histogram(binwidth = 0.01)\n\n\n\nDensity plot\nDensity plots are like histograms, but they use a curved line rather than bins:\n\nptk |&gt; \n  ggplot(aes(VOT)) +\n  geom_density()\n\nThe cool thing about density plots is that we can specify that the area below the curve sum to 1. That fact will be important when we look at p-values.\nLet’s plot VOT as a density plot:\n\nptk |&gt; \n  ggplot(aes(scale(VOT))) +\n  geom_density(fill = \"blue\")\n\n\n\nActivity\nGive it a go! Plot the distribution of several different continuous variables in the ptk dataset, and be ready to share with a neighbor and/or the class."
  },
  {
    "objectID": "lessons/visualization.html#two-continuous-variables",
    "href": "lessons/visualization.html#two-continuous-variables",
    "title": "Visualization",
    "section": "Two continuous variables",
    "text": "Two continuous variables\nEnter the mighty scatterplot!\nLet’s take a look at Dr. Joey Stanley’s vowels, literally (well, as literally as we can).\nMany thanks are expressed (this is a performative passive verb) to Joey for his data and expertise with plotting vowels. Check out this webpage on Data Visualization.\nFirst, let’s download a package to allow us to download R packages from github, and then download the package with his vowels:\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"joeystanley/joeysvowels\")\n\nLet’s load a dataset called coronals and and inspect it to see what data are included and how they are organized:\n\nlibrary(\"tidyverse\")\nmidpoints &lt;- joeysvowels::coronals\nglimpse(midpoints)\nhead(midpoints)\n\nLet’s only use the middle of each vowel and exclude diphthongs. Notice that we save the result of the pipeline back to the same variable name:\n\nmidpoints &lt;- midpoints|&gt; \n    filter(percent == 50)  |&gt; \n    select(-percent) |&gt; \n    filter(!vowel %in% c(\"PRICE\", \"MOUTH\", \"CHOICE\"))\n\nLet’s plot the first two formants of his vowels. In vowel plots, the norm is to plot F2 on the x-axis because it deals with vowel backness (i.e., depth in the mouth) and F1 on the y-axis because it deals with vowel height. Let’s add a geom_point() layer:\n\nmidpoints |&gt; \n  ggplot(aes(x = F2, y = F1)) +\n  geom_point()\n\nIn order to put the points in the same position that they are within the mouth, let’s reverse the scale of both the x-axis and y-axis:\n\nmidpoints |&gt; \n  ggplot(aes(x = F2, y = F1)) +\n  geom_point() +\n  scale_x_reverse() +\n  scale_y_reverse()\n\nThat’s nice and all, but… how about knowing which dot corresponds to which vowel. Amirite?! Let’s go! We can do so by using the color argument in the aes() call within the ggplot() function:\n\nmidpoints |&gt; \n  ggplot(aes(x = F2, y = F1, color = vowel)) +\n  geom_point() +\n  scale_x_reverse() +\n  scale_y_reverse()\n\nThe gray background of ggplot may not be as nice as white when the data points (aka. observations) are colored. Let’s fix it (you guys/y’all/youse know by now that you can customize to your heart’s delight in R, right?):\n\nmidpoints |&gt; \n  ggplot(aes(x = F2, y = F1, color = vowel)) +\n  geom_point() +\n  scale_x_reverse() +\n  scale_y_reverse() +\n  theme_bw()\n\nIf you plan to put this plot in a publication that doesn’t support color, you can use the shape argument instead:\n\nmidpoints |&gt; \n  ggplot(aes(x = F2, y = F1, shape = vowel)) +\n  geom_point() +\n  scale_x_reverse() +\n  scale_y_reverse() +\n  theme_bw()\n\nBut, there’s a problem: R only gives you six shapes by default, so… keep that in mind if you use shape instead of color. An obvious workaround is to filter only several vowels before piping the data frame into ggplot(), for example:\n\nmidpoints |&gt; \n  filter(vowel %in% c(\"LOT\", \"THOUGHT\")) |&gt; \n  ggplot(aes(F2, F1, color = vowel, shape = vowel)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse() +\n  theme_bw()\n\nWe can add more information to the plot with the diameter and/or opacity of the dots. It usually only make sense to vary the diameter or opacity of the dots with a continuous variable.\nLet’s make a continuous variable of duration of the vowel within our pipeline, before passing our data frame into ggplot(). and then specify with the size argument that the diameter of the dots should be proportional to the duration of the vowel that the dot represents:\n\nmidpoints |&gt; \n  mutate(duration = end - start) |&gt; \n  ggplot(aes(x = F2, y = F1, color = vowel, size = duration))+\n  geom_point()+\n  scale_x_reverse()+\n  scale_y_reverse()+\n  theme_bw()\n\nThe above plot is nice, but the overplotting (i.e., dots on top of each other) makes it a bit hard to tell where there are many dots.\nAnother way to put a fourth variable in the plot is with the level of transparency fo the dots so that cluster of dots that are on top of each other are easier to see. Rather than the size argument, let’s use the alpha argument:\n\nmidpoints |&gt; \n  mutate(duration = end - start) |&gt; \n  ggplot(aes(x = F2, y = F1, color = vowel, alpha = duration))+\n  geom_point()+\n  scale_x_reverse()+\n  scale_y_reverse()+\n  theme_bw()\n\nWe can also plot the duration of the vowel as the size of the dots:\n\nmidpoints |&gt; \n  mutate(duration = end - start) |&gt; \n  ggplot(aes(F2, F1, color = vowel, size = duration)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  theme_bw()\n\nIf the dots are too big, we can constrain the range of their sizes:\n\nmidpoints |&gt; \n  mutate(duration = end - start) |&gt; \n  ggplot(aes(F2, F1, color = vowel, size = duration)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_size_continuous(range = c(3, 0.25))+\n  theme_bw()\n\n\nRegression line\nIf you’d like to see how one continuous variable affects the other continuous variable, if at all, plotting a regression line (aka. trend line) is useful. This is a straight line that shows the overall trend through the points. This is done with another layer: geom_smooth(method = lm). The lm part of this line refer to a linear model.\nLet’s go back to our ptk dataset and put a regression line through the points off two continuous variables: VOT and COG.\n\nptk |&gt; \n  ggplot(aes(x = COG, y = VOT))+\n  geom_point()+ # plot individual data points\n  geom_smooth(method = lm)+ # regression line\n  theme_bw() # make it black and white\n\n\n\nLOESS line\nAnother option to show the trend through the data points of two continuous variables is a LOESS (locally estimated scatterplot smoothing) line. This is a locally based smoother line that shows the local trend of the dots closest to it. Let’s modify our plot above by removing the regression line and putting in a loess line (yeah, I’m gonna use lowercase from here on out):\n\nptk |&gt; \n  ggplot(aes(x = COG, y = VOT))+\n  geom_point()+\n  geom_smooth(method = loess)+ \n  theme_bw() \n\nWe can see that the loess line shows that there is a positive relationship between COG (center of gravity) of the friction and the VOT (voice onset time) of the voiceless stops, but that the effect weakens as COG increases (because the loess line becomes flatter).\nOf course, we can include both a regression line and a loess line in the same plot. And if we do that, it’s probably wise to change the color of one of them and perhaps remove the 95% standard error band around the loess line:\n\nptk |&gt; \n  ggplot(aes(x = COG, y = VOT))+\n  geom_point()+\n  geom_smooth(method = lm)+\n  geom_smooth(method = loess, se = FALSE, color = \"red\")+ \n  theme_bw()"
  },
  {
    "objectID": "lessons/visualization.html#one-continuous-variable-and-one-categorical-variable",
    "href": "lessons/visualization.html#one-continuous-variable-and-one-categorical-variable",
    "title": "Visualization",
    "section": "One continuous variable and one categorical variable",
    "text": "One continuous variable and one categorical variable\nAnother common situation is when we have one continuous variable and one categorical variable and we want to see how they affect each other, if at all.\n\nBoxplot\nThe go-to plot for this situation is the mighty boxplot (aka. box and whisker plot).\nLet’s go back to Joey’s vowels. But to make it more manageable for pedagogical purposes, let’s filter the vowels so that we only have front vowels, and also create a duration column while we’re at it:\n\nfront_durs &lt;- midpoints |&gt; \n    filter(vowel %in% c(\"FLEECE\", \"KIT\", \"FACE\", \"DRESS\", \"TRAP\")) |&gt; \n    mutate(duration = end - start)\n\nWith a boxplot, we need to specify the categorical variable on the x-axis and the continuous variable on the y-axis:\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot()+\n  theme_bw()\n\nBoxplots are useful (and better than barplots of the means of a continuous variable) because they show both central tendency (i.e., mean and median) and dispersion or spread around the central tendency.\nHere’s how to interpret a boxplot:\n\nThe box shows the middle 50% of the data points, that is, from the first quartile (aka. Q1), which is the 25th percentile, to the third quartile (aka. Q3), which is the 75th percentile;\nThe horizontal line within the box is the median (i.e., 50th percentile);\nThe lines that extend above and below the boxes are called whiskers, and they extend 1.5 times the interquartile range (aka. IQR), which is simply Q3 - Q1, from the box, that is, the upper whisker extend 1.5 IQR above Q3, while the lower whisker extends 1.5 IQR below Q1;\nOutliers above or below the whiskers are plotted individually as dots.\n\nBy default, geom_boxplot() doesn’t plot the mean, but it’s easy to add with another layer. See this tutorial for the possible shapes; here we’ll choose an “x” with shape=4:\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot()+\n  stat_summary(fun.y=mean, shape=4)+\n  theme_bw()\n\nAnother feature of boxplots in ggplot is that you can add notches to the box, which gives visual information about whether the levels might be statistically significantly different from each other (but a real statistical test that gives a p-value should also be run to assert statistical significance). If the notches of two boxes don’t overlap vertically, this is prima facie evidence that they are likely statistically significantly different from each other.\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot(notch = TRUE)+\n  stat_summary(fun.y=mean, shape=4)+\n  theme_bw()\n\nAs a good follow-up, let’s do an ANOVA (Analysis of Variance) test, and if the ANOVA returns a significant p-value, let’s run a post-doc Tukey Honest Significant Difference test to see if the p-values given for the pairwise comparisons coincides with the notches.\n\nresults1 &lt;- aov(duration ~ vowel, data = front_durs)\nsummary(results1)\n\nYep, something statistically significant is going on here. Next step is a post-hoc test to which pairwise comparisons are significant:\n\nTukeyHSD(results1)\n\nYep, the p-values indicate that the notches were rightly suggestive of statistical significance.\n\n\nViolin plot\nAnother plot for one continuous variable and one categorical variable is the violin plot. This plot is a combination of a boxplot and a density plot. As with the boxplot above, let’s add the mean as an “x”. Here we go:\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_violin()+\n  stat_summary(fun.y=mean, shape=4)+\n  theme_bw()\n\n\n\nAdding observations\nHopefully, you’re getting the idea that there are many possibilities when creating plots with R. In fact, we can mix and match, and add additional layers with additional info. For example, we can add individual observations (aka. tokens) to either the boxplot or the violin plot. Let’s take a look:\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_violin()+\n  geom_point()+\n  theme_bw()\n\nThe geom_point() function adds the data points in a straight, vertical line, which can make it difficult to see exactly how many dots there when they are clustered together. It might be better to either move horizontally a bit, or to make them slightly transparent. In order to offset them a little bit, instead of geom_point(), we can use the geom_jitter(). And, importantly, we probably want to make sure the dots don’t move vertically at all, and only move a little bit horizontally. We can specify limits for the directions with the width and height arguments in geom_jitter():\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_violin()+\n  geom_jitter(width = 0.1, height = 0)+\n  theme_bw()\n\nThe order of the layers is meaningful, that is, elements of the plot will change based on where they are in the pipeline. For example, what happens if we put the geom_jitter() higher up in the pipeline?\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_jitter(width = 0.1, height = 0)+\n  geom_violin()+\n  theme_bw()\n\nWe can also plot individual observations on a boxplot, but it can get confusing because the outliers in a boxplot are plotted individually by geom_boxplot(), but also plotted by geom_jitter() (a bit little to the left or right):\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot()+\n  geom_jitter(width = 0.1, height = 0)+\n  theme_bw()\n\nSo, to fix that, we make the outliers that geom_boxplot() plots be fully transparent:\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot(outlier.alpha = 0)+\n  geom_jitter(width = 0.1, height = 0)+\n  theme_bw()\n\nAnyone for a boxplot and a violin together? Questions: Which one should be overlaid on the other? Should the one in front be slightly transparent so that the one behind it can be seen?\n\n\nActivity\nTake some time to experiment with several options (e.g., violin behind boxplot, and vice versa) and several different levels of transparency (with the alpha argument). If you need some help, take a look at Dr. Brown’s code below:\n\n\nCode\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot()+\n  geom_violin(alpha = 0.3)+\n  stat_summary(fun.y=mean, shape=4)+\n  theme_bw()\n\n\n\n\nViolin-dot plot\nAnother possibility is a violin-dot plot that is available in the see package. First, let’s download that package to our harddrive:\n\ninstall.packages(\"see\", repos = \"http://cran.rstudio.com\")\n\nYou can play with the dots_size and binwidth arguments to get the dots just right.\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  see::geom_violindot(dots_size = 0.8, binwidth = 0.005)+\n  theme_bw()\n\n\n\nRug\nAnother way add individual observations to a plot with is the geom_rug() function. It puts the tokens on the edge of the plotting area, so it’s really only useful with you have exactly levels in the categorical variable. Let’s\n\nfront_durs |&gt; \n  filter(vowel %in% c(\"TRAP\", \"KIT\")) |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot()+\n  geom_rug(data = front_durs |&gt; filter(vowel == \"TRAP\"), sides = \"left\")+\n  geom_rug(data = front_durs |&gt; filter(vowel == \"KIT\"), sides = \"right\")+\n  theme_bw()\n\n\n\nActivity\nMake two plots to visualize each of the following:\n\nThe effect, if any, of the previous segment on the duration of vowels;\nThe effect, if any, of the following segment on the duration of vowels.\n\nDecide which type of plots you’d like (e.g., boxplot with or without notches, violin plot, boxplot and violin plot, with or without individual observations, etc.).\nAfter a good-faith effort, if you need some help take a look at Dr. Brown’s code below:\n\n\nCode\nfront_durs |&gt; \n  ggplot(aes(x = pre, y = duration))+\n  geom_boxplot(notch = TRUE)+\n  geom_violin(alpha = 0.5)+\n  stat_summary(fun.y=mean, shape=4)+\n  theme_bw()\n\nfront_durs |&gt; \n  ggplot(aes(x = fol, y = duration))+\n  geom_boxplot(notch = TRUE)+\n  geom_violin(alpha = 0.5)+\n  stat_summary(fun.y=mean, shape=4)+\n  theme_bw()"
  },
  {
    "objectID": "lessons/visualization.html#facetting",
    "href": "lessons/visualization.html#facetting",
    "title": "Visualization",
    "section": "Facetting",
    "text": "Facetting\nWe can one or two variables to our plot by making an individual plot for each the levels of a categorical variable or two. Let’s go back to our histograms above. When we plotted the histogram of all vowels, we didn’t know how the various vowels differ with respect to the distribution of their durations. There are two main facetting layers (that Dr. Brown will show here): facet_wrap() and facet_grid().\nLet’s use the midpoints dataset from above and create a new column with the duration of the vowel, and save it to a new data frame named midpoints_durs:\n\nmidpoints_durs &lt;- midpoints |&gt; \n  mutate(duration = end - start)\n\n\nfacet_wrap()\nNow, let’s create a single plot with subplots with a histogram, one histogram per vowel:\n\nmidpoints_durs |&gt; \n  ggplot(aes(x = duration))+\n  geom_histogram(binwidth = 0.01)+\n  facet_wrap(~vowel)+\n  theme_bw()\n\nYou may notice the scale of the x-axis and the scale of the y-axis are the same for all subplots. That’s useful when you want to directly compare the subplots, which is usually the case. However, if you need to the scales to be independent of each other, you guess it, that can be specified:\n\nmidpoints_durs |&gt; \n  ggplot(aes(x = duration))+\n  geom_histogram(binwidth = 0.01)+\n  facet_wrap(~vowel, scales = \"free\")+\n  theme_bw()\n\nRather than have independent scales for both the x-axis and the y-axis, you can specify only one if you’d like with scale = \"free_x\" or scale = \"free_y\". Give it a try!\nAlso, we can control the number of rows or columns that the subplots are placed in with the nrow or ncol arguments:\n\nmidpoints_durs |&gt; \n  ggplot(aes(x = duration))+\n  geom_histogram(binwidth = 0.01)+\n  facet_wrap(~vowel, scales = \"free\", nrow = 2)+\n  theme_bw()\n\nmidpoints_durs |&gt; \n  ggplot(aes(x = duration))+\n  geom_histogram(binwidth = 0.01)+\n  facet_wrap(~vowel, scales = \"free\", ncol = 3)+\n  theme_bw()\n\n\n\nfacet_grid()\nWhile facet_wrap() takes one variable, that is, column in the input data frame, facet_grid() takes two columns and creates a grid layout. For example, let’s look at the distribution of vowel duration by vowel and by following segment:\n\nmidpoints_durs |&gt; \n  ggplot(aes(x = duration))+\n  geom_histogram(binwidth = 0.025)+\n  facet_grid(fol~vowel)+\n  theme_bw()\n\nThis layout doesn’t allow you to specify the number of rows nor of columns, nor allow the scales to vary independently.\n\n\nActivity\nYour turn! Make a plot with the distribution of durations by vowel and by preceding segment. Feel free to create histograms or density plots.\nAfter a good-faith effort, if you need help, see Dr. Brown’s code below:\n\n\nCode\nmidpoints_durs |&gt; \n  ggplot(aes(x = duration))+\n  geom_histogram()+\n  facet_grid(pre~vowel)"
  },
  {
    "objectID": "lessons/visualization.html#color",
    "href": "lessons/visualization.html#color",
    "title": "Visualization",
    "section": "Color",
    "text": "Color\nWe can adjust the color of the lines and the fill of elements of a plot with a myriad of possibilities. First, let’s take a look at the possibilities:\n\ncolors()\n\nThe RStudio IDE displays the highlights the name of the color right in the source file (aka. script). Let’s change the color of the lines with the color argument and the color of the fill of the boxes of a boxplot with the fill argument:\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration)) + \n  geom_boxplot(color = \"navy\", fill = \"lightblue\")+\n  theme_bw()\n\nNote that when you explicitly specify colors in the call to a layer, it overrides any previous color specification. For example, if I have ggplot() to plot a color, but then give geom_boxplot() specific colors, only the colors given to the later layer are plotted.\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration, color = vowel)) + \n  geom_boxplot(color = \"navy\", fill = \"lightblue\")+\n  theme_bw()\n\nHowever, if a particular layer doesn’t overwrite a color specification inherited from ggplot(), for example geom_jitter() below, then the later layer use the color from ggplot():\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration, color = vowel)) + \n  geom_boxplot(color = \"navy\", fill = \"lightblue\")+\n  geom_jitter(width = 0.1, height = 0)+\n  theme_bw()\n\nSo, the aes() in ggplot() passes its color down to the subsequent layers, but aes() with a specific layer only affects that layer.\n\nActivity\nCompare and contrast the following three blocks of code their resulting plots. Identify how the plots differ and how the placement of color = vowel affects which elements of the plot are colored.\n\nfront_durs |&gt; \n  ggplot(aes(vowel, duration)) + \n  geom_boxplot(aes(color = vowel)) + \n  geom_jitter(width = 0.1, height = 0) +\n  theme_bw()\n\nfront_durs |&gt; \n  ggplot(aes(vowel, duration)) + \n  geom_boxplot() + \n  geom_jitter(aes(color = vowel), width = 0.1, height = 0) +\n  theme_bw()\n\nfront_durs |&gt; \n  ggplot(aes(vowel, duration, color = vowel)) + \n  geom_boxplot() + \n  geom_jitter(width = 0.1, height = 0) +\n  theme_bw()\n\nCheck out the docs for scale_color_distiller() for other color palettes.\nIn the code above, we only map color to a categorical variable. But, we can also map it to a continuous variable. Let’s use the midpoints data frame from above:\n\nmidpoints_durs |&gt; \n  ggplot(aes(F2, F1, color = duration)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse()+\n  theme_bw()\n\nBy default, ggplot() uses the blue-to-black palette, but we can change it manually if we’d like:\n\nmidpoints_durs |&gt; \n  ggplot(aes(F2, F1, color = duration)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse()+\n  scale_color_gradient(low = \"gold\", high = \"forestgreen\")+\n  theme_bw()"
  },
  {
    "objectID": "lessons/visualization.html#reorder-levels-of-a-categorical-variable",
    "href": "lessons/visualization.html#reorder-levels-of-a-categorical-variable",
    "title": "Visualization",
    "section": "Reorder levels of a categorical variable",
    "text": "Reorder levels of a categorical variable\nLet’s take a look at our a boxplot with jittered observations:\n\nfront_durs |&gt; \n  ggplot(aes(x = vowel, y = duration)) + \n  geom_boxplot(outlier.alpha = 0) + \n  geom_jitter(width = 0.1, height = 0)+\n  theme_bw()\n\nWe can also adjust levels of a categorical variable so that they appear in a different order. This can be done in one of two place: with the dplyr part of the pipeline, or within the ggplot part of the pipeline. First, let’s reorder the levels with our ol’ friend fct_relevel():\n\nfront_durs %&gt;%\n  mutate(vowel = fct_relevel(vowel, c(\"FLEECE\", \"KIT\", \"FACE\", \"DRESS\", \"TRAP\"))) |&gt; \n  ggplot(aes(vowel, duration)) + \n  geom_boxplot(outlier.size = 0) + \n  geom_jitter(width = 0.1, height = 0)+\n  theme_bw()\n\nOr, we could ask use a ggplot layer to reorder the levels of this categorical variable:\n\nfront_durs %&gt;%\n  ggplot(aes(vowel, duration)) + \n  geom_boxplot(outlier.size = 0) + \n  geom_jitter(width = 0.1, height = 0)+\n  scale_x_discrete(limits = c(\"FLEECE\", \"KIT\", \"FACE\", \"DRESS\", \"TRAP\"))+ \n  theme_bw()\n\nIf that’s confusing to have two ways to do the same thing, just choose one and forget the other. If you need help choosing, Dr. Brown’s recommends the fct_relevel() way.\n\nActivity\nCreate a boxplot of the five vowels in the front_durs data frame and order the boxes by median duration. Hint: First, use some of our ol’ friends in dplyr to create a vector of the five vowels ordered by median, then use that vector with in fct_relevel() or scale_x_discrete().\nAfter a good-faith effort, if you need help, take a look at Dr. Brown’s code:\n\n\nCode\nordered_by_median &lt;- front_durs |&gt; \n  group_by(vowel) |&gt; \n  summarize(median_dur = median(duration)) |&gt; \n  arrange(median_dur) |&gt; \n  pull(vowel) |&gt; \n  as.character()\nfront_durs |&gt; \n  mutate(vowel = fct_relevel(vowel, ordered_by_median)) |&gt; \n  ggplot(aes(x = vowel, y = duration))+\n  geom_boxplot(outlier.size = 0) + \n  geom_jitter(width = 0.1, height = 0)+\n  theme_bw()\n\n\nWe can also order bar of a barplot as well. Let’s use the midpoints data frame to order the vowel in descending order by frequency:\n\nmidpoints |&gt; \n  mutate(vowel = fct_infreq(vowel)) |&gt; \n  ggplot(aes(vowel)) + \n  geom_bar()+\n  theme_bw()"
  },
  {
    "objectID": "lessons/visualization.html#add-ellipses-by-level-in-scatterplot",
    "href": "lessons/visualization.html#add-ellipses-by-level-in-scatterplot",
    "title": "Visualization",
    "section": "Add ellipses by level in scatterplot",
    "text": "Add ellipses by level in scatterplot\nLet’s go back to our scatterplot with F1 and F2 of vowels:\n\nmidpoints |&gt; \n  ggplot(aes(F2, F1, color = vowel)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse()+\n  theme_bw()\n\n…And now add ellipses around two-thirds of the data points per vowel:\n\nmidpoints |&gt; \n  ggplot(aes(F2, F1, color = vowel)) + \n  geom_point(alpha = 0.5) + \n  stat_ellipse(level = 0.67) + \n  scale_x_reverse() + \n  scale_y_reverse()+\n  theme_bw()\n\nWith so many vowels and some of the colors so close to each other, it’d be nice to add labels right on the plot. You guessed it! That can be done. First, let’s create a small data frame with the mean F1 and mean F2 by vowel:\n\nF_means &lt;- midpoints %&gt;%\n  group_by(vowel) %&gt;%\n  summarize(mean_F1 = mean(F1), mean_F2 = mean(F2))\n\nNow let’s use those means in order to place the labels in the right spots on the plot:\n\nmidpoints |&gt; \n  ggplot(aes(F2, F1, color = vowel)) + \n  geom_point(alpha = 0.5) + \n  stat_ellipse(level = 0.67) + \n  geom_text(data = F_means, aes(mean_F2, mean_F1, label = vowel), size = 5)+\n  scale_x_reverse() + \n  scale_y_reverse()+\n  theme_bw()\n\nGiven the fact that the labels are difficult to read because of the dots behind him, it might be best to use black labels:\n\nmidpoints |&gt; \n  ggplot(aes(F2, F1, color = vowel)) + \n  geom_point(alpha = 0.5) + \n  stat_ellipse(level = 0.67) + \n  geom_text(data = F_means, aes(mean_F2, mean_F1, label = vowel), size = 5, color = \"black\")+\n  scale_x_reverse() + \n  scale_y_reverse()+\n  theme_bw()"
  },
  {
    "objectID": "lessons/visualization.html#small-adjustments",
    "href": "lessons/visualization.html#small-adjustments",
    "title": "Visualization",
    "section": "Small adjustments",
    "text": "Small adjustments\n\nBackground color\nThe default color background of ggplot is gray, for example:\n\np &lt;- midpoints |&gt; \n  ggplot(aes(F2, F1, color = vowel)) + \n  geom_point(alpha = 0.5) + \n  stat_ellipse(level = 0.67) + \n  geom_text(data = F_means, aes(mean_F2, mean_F1, label = vowel), size = 5, color = \"black\")+\n  scale_x_reverse() + \n  scale_y_reverse()\nprint(p)\n\nAs you may have noticed by now, Dr. Brown prefers the black-n-white theme that theme_bw() gives.\n\np + theme_bw()\n\nAnother nice theme is theme_minimal(), which is basically the theme_bw() but without the border:\n\np + theme_minimal()\n\n\n\nActivity\nExploration time: Take a look at the docs for the themes by entering ?theme_bw at the console, and then try some of the other themes. Be ready to report on which theme you like the most, and which you like the least.\n\n\nLabels\nWe can add custom labels to our plots with the labs() layer.\n\np + \n  theme_classic()+\n  labs(title = \"F1 and F2 by Vowel\", subtitle = \"of Joey Stanley's speech\", x = \"Second formant (F2)\", y = \"First formant (F1)\")\n\nWe can also adjust the font, angle, horizontal and vertical adjustments of text on the plot, for example:\n\np +\n  theme_bw()+\n  theme(text = element_text(family = \"Times New Roman\"),)\n\nOr tilt the angle of the labels:\n\nmidpoints |&gt; \n  mutate(vowel = fct_infreq(vowel)) |&gt; \n  ggplot(aes(vowel)) + \n  geom_bar()+\n  theme_bw()+\n  theme(text = element_text(family = \"Times New Roman\"),\n        axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))\n\nHow about something as nasty as this?\n\np +\n  labs(title = \"Joey's vowels\", subtitle = \"Notice that this is left-aligned to the plotting area\") + \n    theme(plot.title = element_text(hjust = 0.2, vjust = 0.6, angle = 2, family = \"Avenir\", size = 18, color = \"forestgreen\", face = \"italic\", lineheight = 1.2, margin = margin(t = 1, r = 0.2, b = 1.4, l = -4, unit = \"cm\"), debug = TRUE))"
  },
  {
    "objectID": "lessons/outliers.html",
    "href": "lessons/outliers.html",
    "title": "Detecting outliers",
    "section": "",
    "text": "Students will detect and remove outliers of continuous variables."
  },
  {
    "objectID": "lessons/outliers.html#objective",
    "href": "lessons/outliers.html#objective",
    "title": "Detecting outliers",
    "section": "",
    "text": "Students will detect and remove outliers of continuous variables."
  },
  {
    "objectID": "lessons/outliers.html#extreme-values-vs.-outliers",
    "href": "lessons/outliers.html#extreme-values-vs.-outliers",
    "title": "Detecting outliers",
    "section": "Extreme values vs. outliers",
    "text": "Extreme values vs. outliers\n(Thanks to Joey Stanley for some of this material!)\nOutliers of continuous or quantitative variables have extreme values, either extremely low or extremely high. Let’s consider this vector of (human) babies heights in inches: c(21, 19, 20, 18, 20, 73, 22, 18, 19). Wait, what? A baby that is 73 inches tall?! That must be an adult. That height is an outlier because it doesn’t come from the same population (i.e., babies).\nHow about this vector of (modern, human) ages of eight siblings: c(39, 38, 37, 356, 34, 33, 33, 31, 25). A modern human being with an age of 365 years?! This is probably a simple data entry mistake (probably the 5 and 6 keys pressed together). The best thing to do here would be to track down the right age, and if that’s not possible, consider this extreme age as an outlier and remove it.\nLet’s consider a third vector of number, this time of the number of children that each of the previously mentioned siblings have: c(0, 2, 8, 1, 0, 2, 2, 2, 0). Wait, someone has eight kids?! Oh yeah, nevermind, that’s not uncommon in certain religious groups. So, in this case the eight is an extreme value but not an outlier, and it should be kept in the dataset.\nMoral of the story: It’s a (really) good idea to investigate extreme values in order to make sure they actually are outliers that should be excluded, rather than simply extreme values that should still be kept in."
  },
  {
    "objectID": "lessons/outliers.html#how-to-remove-outliers",
    "href": "lessons/outliers.html#how-to-remove-outliers",
    "title": "Detecting outliers",
    "section": "How to remove outliers",
    "text": "How to remove outliers\nThere are a couple ways to identify (possible) outliers. One is based on the mean and the standard deviation of the observations. Another way is based on the interquartile range of the observations.\n\nMean and standard deviation\nA common way in linguistics to remove outliers is to remove data points that lay outside of the first two standard deviations above or below the mean value of a given continuous variable. Here’s the mathematical formula to find the threshold below which or above which possible outliers are detected:\n\\[\nthreshold = μ ± 2 * σ\n\\]\n…where \\(μ\\) is the mean of all observations, and \\(σ\\) is the standard deviation of all observations.\nLet’s go back to our baby heights example above and calculate the mean and standard deviation:\n\nlibrary(\"tidyverse\")\nbaby_heights &lt;- c(21, 19, 20, 18, 20, 73, 22, 18, 19)\ntibble(baby_heights) |&gt; \n  summarize(mean(baby_heights), sd(baby_heights))\n\nSo, the mean is \\(25.6\\) and the standard deviation is \\(17.8\\). So, values below the mean minus two times the standard deviation (i.e., \\(25.6-2*17.8=-10\\)), and values above the mean plus two times the standard deviation (i.e., \\(25.6+2*17.8=61.2\\)), might safely be considered outliers and can be removed.\nOf course, we don’t have to do this math by hand:\n\ntibble(baby_heights) |&gt; \n  filter(\n    baby_heights &gt; mean(baby_heights) - 2 * sd(baby_heights) & \n    baby_heights &lt; mean(baby_heights) + 2 *sd(baby_heights)\n  )\n\nPerhaps an easier way to do the previous filtering is to first create a new column with z-score values of the original ages, and then simply filter on \\(±2\\):\n\ntibble(baby_heights) |&gt; \n  mutate(z_height = as.vector(scale(baby_heights))) |&gt;\n  filter(z_height &gt; -2 & z_height &lt; 2)\n\nWhat is a z-score, you ask? It is a centered and scaled (aka. normalized) set of values. It is calculated by taking the mean and the standard deviation of a continuous variable, and then taking each value and subtracting the mean, and then dividing that difference by the standard deviation. Here’s the mathematical formula to do that:\n\\[\nz=\\frac{x-μ}{σ}\n\\]\n…where \\(x\\) is each observation, \\(μ\\) is the mean of all observations, and \\(σ\\) is the standard deviation of all observations.\n\n\nInterquartile range\nAnother way to identify (possible) outliers is by finding the first quartile (i.e., the 25th percentile, aka. Q1) and the third quartile (i.e., the 75th percentile, aka. Q3), and then calculating the difference between those two quartiles. The difference between Q3 and Q1 is called the interquartile range (aka. IQR). Then, outliers are identified as observations above 1.5 times IQR above Q3 or 1.5 times IQR below Q1. Here’s the mathematical notation:\n\\[\nQ_3+1.5*IQR\\quad\\text{or}\\quad Q_1-1.5* IQR\n\\]\nLet’s try this in R. First, let’s get Q1, Q3, and IQR. Then, we’ll use these three values in filter() within a good ol’ fashioned dplyr pipeline:\n\nquartiles &lt;- tibble(baby_heights) |&gt; \n  pull(baby_heights) |&gt; \n  quantile(probs = c(0.25, 0.75))\nq1 &lt;- quartiles[1]\nq3 &lt;- quartiles[2]\niqr &lt;- q3 - q1\n\n# here we go with the actual filtering\ntibble(baby_heights) |&gt; \n  filter(\n    baby_heights &gt; q1 - 1.5 * iqr & \n    baby_heights &lt; q3 + 1.5 * iqr)"
  },
  {
    "objectID": "lessons/outliers.html#activity",
    "href": "lessons/outliers.html#activity",
    "title": "Detecting outliers",
    "section": "Activity",
    "text": "Activity\nLoad up a dataset of your choice, for example, from the Dataset module in Canvas, and test several different continuous variables to see if any of them have outliers, and if so, how many outliers. Choose either method to identify (possible) outliers.\nAfter a good-faith effort, if you need help take a look at Dr. Brown’s code below:\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"readxl\")\nptk &lt;- read_excel(\"/Users/ekb5/Documents/data_analysis/datasets/Data_ptk.xlsx\", sheet = \"data\")\n\n# get the number of rows and columns\nptk |&gt; dim() # rows, columns\n\n# filter outliers below or above two standard deviations from the mean and then get the number of rows and columns again\nptk |&gt; \n  mutate(VOT_z = as.vector(scale(VOT))) |&gt; \n  filter(VOT_z &gt; -2 & VOT_z &lt; 2) |&gt; \n  dim()"
  },
  {
    "objectID": "lessons/outliers.html#two-continuous-variables",
    "href": "lessons/outliers.html#two-continuous-variables",
    "title": "Detecting outliers",
    "section": "Two continuous variables",
    "text": "Two continuous variables\nWhen you’re looking for outliers across two variables, visual inspection is probaby the easiest way. Here’s a toy example:\n\nlibrary(\"tidyverse\")\nx &lt;- runif(n = 100, min = 23, max = 67)\ny &lt;- jitter(x * 3.1415926, amount = 20)\ndf &lt;- tibble(x, y) |&gt; \n  bind_rows(tibble(x = 59, y = 100))\ndf |&gt; \n  ggplot(aes(x = x, y = y))+\n  geom_point()\n\nAs seen, there is an outlier below the majority of the data points (aka. observations). It would be good to filter the data frame in order to only see that particular observation. For example, we could filter so that only tokens with an x value greater than, say, 55 and a y value less than, say, 125 are kept:\n\ndf |&gt; \n  filter(x &gt; 55, y &lt; 125)\n\nIt would be wise to go look at this particular token to determine if it’s simply an extreme value and should be kept, or if it’s an outlier that should be removed."
  },
  {
    "objectID": "lessons/python.html",
    "href": "lessons/python.html",
    "title": "Other languages",
    "section": "",
    "text": "Students will write R code that calls other languages."
  },
  {
    "objectID": "lessons/python.html#objective",
    "href": "lessons/python.html#objective",
    "title": "Other languages",
    "section": "",
    "text": "Students will write R code that calls other languages."
  },
  {
    "objectID": "lessons/python.html#multilingual-programming-world",
    "href": "lessons/python.html#multilingual-programming-world",
    "title": "Other languages",
    "section": "Multilingual programming world",
    "text": "Multilingual programming world\nJust like human languages and all their variety and beauty, the variety of programming languages gives beauty to the computer world. More specifically, some programming languages are better suited than others for specific tasks. So, rather than having to be monolingual within only one programming language, we can use several languages as needed."
  },
  {
    "objectID": "lessons/python.html#python",
    "href": "lessons/python.html#python",
    "title": "Other languages",
    "section": "Python",
    "text": "Python\nIt is Dr. Brown’s opinion that Python is the best language for text processing (and is an all-around great language for many tasks). Thus, when an R user has a need to do text processing, it might be better to call on Python for that specific task.\n\nThe reticulate R package\nThe reticulate R package allows R users to call Python from within R. Here’s a simple example:\n\n# R code here\nlibrary(\"reticulate\")\nuse_python(\"/usr/local/bin/python3\")\n\ntxt &lt;- \"The quick brown fox jumped over the lazy dog.\"\n\nnltk &lt;- import(\"nltk\")\ntokens &lt;- nltk$word_tokenize(txt)\nprint(tokens)\ntagged &lt;- nltk$pos_tag(tokens)\nprint(tagged)\n\nIf you know Python (and NLTK), you might remember that the output of this call in Python is a Python list of two-item tuples. Differently, here we get an R list (which is different from a Python list). We could continue to process this R list as needed, for example, looping over it in a for loop to find particular part-of-speech tags:\n\n# R code here\nfor (tag in tagged) {\n  if (tag[2] == \"NN\") {\n    print(tag[1])\n  }\n}\n\n\n\nWithin Quarto documents\nWe can also write Python code within code blocks in a Quarto document. When working in a Quarto document, you can type a forward slash and get a drop-down menu of possible things to insert into your document, one of which is a Python code block.\nThe following code blocks are written in a Quarto document (as Dr. Brown uses Quarto document for his lesson plans). The code is pure Python, as if he were writing Python code in a .py file. Here’s a first simple example:\n\n# Python code here\nprint(\"hello Python!\")\n\nhello Python!\n\n\nA more meaningful example might be to use the Natural Language Toolkit (NLTK) to tokenize word and perform part-of-speech tagging, like with did above within R using reticulate:. Differently, here, we don’t need reticulate because we calling Python directly on the computer, rather than through R.\n\n# more Python code here\nimport nltk\ntxt = \"The quick brown fox jumped over the lazy dog.\"\ntokens = nltk.word_tokenize(txt)\nprint(tokens)\n\n['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.']\n\n\nThe above code tokenizes the famous sentence in linguistics into a Python list of words. Now, let’s ask NLTK to tag for part-of-speech:\n\n# Python code here\ntagged = nltk.pos_tag(tokens)\nprint(tagged)\n\n[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n\n\nThose with experience with Python may notice that the output of the above code is a Python list with two-item tuples. We’re back in the comfort of the Python world! (The R world is comfortable too, though.)\n\n\nActivity\nYour turn! Write some (simple) R code that calls Python using the reticulate package. If you’re not sure what to write, just try to get the examlpes on the reticulate docs working."
  },
  {
    "objectID": "lessons/python.html#other-languages",
    "href": "lessons/python.html#other-languages",
    "title": "Other languages",
    "section": "Other languages",
    "text": "Other languages\nMany other languages can be called from within R. Some of the most likely to be called from with R are C++ and Julia.\n\nC++\nThe Rcpp R package is an awesome way to have R call C++. This is especially useful for bottlenecks that can be sped up with C++ (which runs much, much faster than R). First, let’s install Rcpp:\n\ninstall.packages(\"Rcpp\", repos = \"http://cran.rstudio.com/\")\n\nNext, let’s\n\nlibrary(\"Rcpp\")\ncppFunction('int add(int x, int y, int z) {\n  int sum = x + y + z;\n  return sum;\n}')\n# add works like a regular R function\nadd(1, 2, 3)\n\nIn addition to defining C++ function within an R script (i.e., an R file) with cppFunction(), we can write the C++ code with a .cpp file and call that file into an R script with the sourceCpp() function. Take a look at the example in Hadley Wickham’s book, in chapter 25 “Rewriting R code in C++”.\n\n\nJulia\nThe Julia programming language is a relatively new language oriented towards data science and statistics (very much like R). When you’re writing a Quarto document, you can insert a Julia code block by typing a forward slash “/” and selecting the Julia code block option. First, we need to install the JuliaCall R package:\n\ninstall.packages(\"JuliaCall\", repos = \"http://cran.rstudio.com/\")\n\n\n# Julia code here\n#| eval: false\nprintln(\"hello Julia!\")\n\nhello Julia!\n\n\nUsing JuliaCall, you can also call Julia directly from within R code. See some examples on their docs.\nOther programming languages can be called from within R, but Python, C++ and Julia are probably the most common (in Dr. Brown’s uninformed estimation)."
  },
  {
    "objectID": "lessons/linear_regression.html",
    "href": "lessons/linear_regression.html",
    "title": "Linear regression",
    "section": "",
    "text": "Students will build a linear regression model."
  },
  {
    "objectID": "lessons/linear_regression.html#objective",
    "href": "lessons/linear_regression.html#objective",
    "title": "Linear regression",
    "section": "",
    "text": "Students will build a linear regression model."
  },
  {
    "objectID": "lessons/linear_regression.html#ordinary-least-squares-regression",
    "href": "lessons/linear_regression.html#ordinary-least-squares-regression",
    "title": "Linear regression",
    "section": "Ordinary Least Squares Regression",
    "text": "Ordinary Least Squares Regression\nA very common form of regression is Ordinary Least Squares (OLS) linear regression. If no modifiers are put before the term “linear regression” then Ordinary Least Squares linear regression is being referred to.\nThe basic idea is to try to fit a straight line (aka. regression line) through data points such that you end up with the lowest possible sum of squared residuals."
  },
  {
    "objectID": "lessons/linear_regression.html#residuals",
    "href": "lessons/linear_regression.html#residuals",
    "title": "Linear regression",
    "section": "Residuals",
    "text": "Residuals\nA residual in regression is the distance between an observed data point (aka. observation) and the regression line. While there is a mathematical formula to find where exactly that line should go, that is, to find its slope and the value of \\(y\\) when the line crosses \\(x=0\\), for pedagogical purposes, let’s do a bit of trial-n-error."
  },
  {
    "objectID": "lessons/linear_regression.html#toy-example",
    "href": "lessons/linear_regression.html#toy-example",
    "title": "Linear regression",
    "section": "Toy example",
    "text": "Toy example\nLet’s create a simple data frame:\n\nsuppressPackageStartupMessages(library(\"tidyverse\"))\nmorenos &lt;- tibble(\n  person = c(\"Sam\", \"Eloisa\", \"Lola\", \"Heidi\", \"Ellen\"),\n  height = c(72, 70, 69, 63, 62),\n  age = c(18, 17, 14, 13, 11)\n)\nprint(morenos)\n\n# A tibble: 5 × 3\n  person height   age\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Sam        72    18\n2 Eloisa     70    17\n3 Lola       69    14\n4 Heidi      63    13\n5 Ellen      62    11\n\n\nLet’s plot height by age of the kids in the above data frame:\n\np1 &lt;- morenos %&gt;% \n  ggplot(aes(x = age, y = height))+\n  geom_point(size = 3)+\n  theme_minimal()\nplot(p1)\n\n\n\n\nIt looks like there is a relationship between height and age of these kids, such that as the kids get older, their heights increase (unsurprisingly)."
  },
  {
    "objectID": "lessons/linear_regression.html#trial-n-error",
    "href": "lessons/linear_regression.html#trial-n-error",
    "title": "Linear regression",
    "section": "Trial-n-error",
    "text": "Trial-n-error\nLet’s plot some lines and calculate the sum of the squares of the residuals for each line. In the plots below, the black points are observed values while the red points are predicted values. The vertical lines connecting the observed values (black points) and the predicted values (red points) are the residuals (aka. errors).\n\n# define a helper function\nplot_dots_and_line &lt;- function(df, input_slope, input_intercept) {\n  temp &lt;- df %&gt;% \n    mutate(hypothetical = age * input_slope + input_intercept)\n  p1 &lt;- temp %&gt;%\n    ggplot(aes(x = age, y = height))+\n    \n    # draw a straight line with the slope and y-intercept\n    geom_abline(slope = input_slope, intercept = input_intercept, color = \"blue\", linetype=3)+\n    \n    geom_point(aes(y = hypothetical), color = \"red\", size = 3)+\n    geom_segment(aes(xend = age, yend = hypothetical), alpha = 0.5)+\n    geom_point(size = 3)+\n    ggtitle(str_interp(\"Slope = ${input_slope}; y-intercept = ${input_intercept}; Sum of squares of residuals = ${sum((temp$height - temp$hypothetical)^2)}\"))+\n    theme_minimal()\n  return(p1)\n}\n\n# Make a list of guesses of slopes and y-intercepts\nguesses &lt;- list(\n  list(g_slope = 0, g_intercept = 65),\n  list(g_slope = 0.5, g_intercept = 65),\n  list(g_slope = 0.5, g_intercept = 60),\n  list(g_slope = 1, g_intercept = 50),\n  list(g_slope = 1, g_intercept = 52)\n)\n\n# Loop over the guesses\nfor (guess in guesses) {\n  p1 &lt;- plot_dots_and_line(morenos, guess$g_slope, guess$g_intercept)\n  plot(p1)\n}"
  },
  {
    "objectID": "lessons/linear_regression.html#the-formula",
    "href": "lessons/linear_regression.html#the-formula",
    "title": "Linear regression",
    "section": "The formula",
    "text": "The formula\nOkay, enough horsin’ around. Let’s use the formula (actually formulas, or formulae if you must) to get the best-fit regression line, that is, the line with the lowest sum of squares of residuals.\nHere’s the formula to get the predicted y-value for a given x-value.\n\\[\n\\hat{y}=slope*x+intercept\n\\]\nThe \\(\\hat{y}\\) in the formula is referred to as “y-hat” and is the predicted (aka. fitted) y-value for a given x value. And how do you get the slope and intercept, you ask? Math (don’t freak out, it’s not that bad):\n\\[\nslope = \\frac{N * \\sum{(x * y)} - \\sum{x} * \\sum{y}}{N * \\sum{(x^{2})} - (\\sum{x})^{2}}\n\\]\nWhere:\n\\(N\\) = number of observations in dataset\n\\(*\\) = Good ol’ fashioned multiplication\n\\(\\sum\\) = The sum of the numbers to the right\n\\(x\\) = The explanatory (aka. independent or predictor) variable (here age)\n\\(y\\) = The response (aka. dependent) variable (here height)\nLet’s look at each piece of this formula with color:\n\\[\nslope = \\frac{N * \\color{red} \\sum{(x * y)} \\color{black} - \\color{orange} \\sum{x} \\color{black} * \\color{green} \\sum{y}}{N * \\color{blue} \\sum{(x^{2})} \\color{black} - \\color{purple} (\\sum{x})^{2}}\n\\]\nSo:\n\\(N=5\\) \\(\\color{red} \\sum{(x*y)} \\color{black} =(18*72+17*70+14*69+13*63+11*62)=(1296+1190+966+819+682) = 4953\\)\n\\(\\color{orange} \\sum{x} \\color{black} =(18+17+14+13+11) = 73\\)\n\\(\\color{green} \\sum{y} \\color{black} =(72+70+69+63+62) = 336\\)\n\\(\\color{blue} \\sum{(x^{2})} \\color{black} =(18^{2}+17^{2}+14^{2}+13^{2}+11^{2})=(324+289+196+169+121) = 1099\\)\n\\(\\color{purple} (\\sum{x})^{2} \\color{black} =(18+17+14+13+11)^{2} = 73^{2} = 5329\\)\nNow, let’s put the numbers into the formula and finally get that rascally slope:\n\n\\[slope=\\frac{5* \\color{red} 4953 \\color{black} - \\color{orange} 73 \\color{black} * \\color{green} 336}{5* \\color{blue} 1099 \\color{black} - \\color{purple} 5329}=\\frac{24765-24528}{5495-5329}=\\frac{237}{166}= 1.427711\n\\]\nLovely, the slope of the least squares line (aka. line of best fit) is 1.427711.\nNow, here’s the formula for the y-intercept (which uses the slope that we just calculated):\n\n\\[intercept = \\frac{\\color{green} \\sum{y} \\color{black} - slope * \\color{orange} \\sum{x}}{N}\n\\]\nWe already calculated above all the numbers in this formula, so let’s get crackin’!:\n\\[intercept = \\frac{\\color{green} 336 \\color{black} - 1.427711 * \\color{orange} 73}{5}=\\frac{336-104.2229}{5}=\\frac{231.7771}{5}=46.35542\n\\]\nTerrific, the y-intercept of the least squares line is 46.35542.\nNow, let’s use the slope and the y-intercept we just calculated to draw a line through our data points and calculate the sum of the squares of residuals.\n\nplot_dots_and_line(morenos, 1.427711, 46.35542)"
  },
  {
    "objectID": "lessons/linear_regression.html#fit-a-linear-regression",
    "href": "lessons/linear_regression.html#fit-a-linear-regression",
    "title": "Linear regression",
    "section": "Fit a linear regression",
    "text": "Fit a linear regression\nAs I’m sure you have likely guessed, there’s a function to do all the math we just did by hand: lm(). Because the dependent variable (aka. response variable) is continuous, we will fit a linear regression (rather than a logistic regression). When we have only one explanatory variable, the term “simple linear regression” is often used. Differently, when we have two or more explanatory variables, the term “multiple linear regression” is used. We see multiple linear regression below.\n\nfitted_model &lt;- morenos %&gt;% \n  lm(height ~ age, data = .)\nprint(fitted_model)\n\n\nCall:\nlm(formula = height ~ age, data = .)\n\nCoefficients:\n(Intercept)          age  \n     46.355        1.428  \n\n\nThe summary() function gives you the above info as well as more info:\n\nsummary(fitted_model)\n\n\nCall:\nlm(formula = height ~ age, data = .)\n\nResiduals:\n       1        2        3        4        5 \n-0.05422 -0.62651  2.65663 -1.91566 -0.06024 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  46.3554     4.9552   9.355  0.00259 **\nage           1.4277     0.3342   4.272  0.02355 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.926 on 3 degrees of freedom\nMultiple R-squared:  0.8588,    Adjusted R-squared:  0.8117 \nF-statistic: 18.25 on 1 and 3 DF,  p-value: 0.02355\n\n\nThe Call section and the Residuals sections are transparently named. With small datasets, the residuals are given here in the summary, but when there are many data points (and therefore an equal number of many residuals), only summary information is given about the residuals, specifically the min and max, and the first and third quartiles, and the median. Ideally, the residuals should be normally distributed and centered on zero. If the summary info given isn’t enough to determine those assumptions, you can retrieve all the residuals with the residuals() function:\n\nresiduals(fitted_model)\n\n          1           2           3           4           5 \n-0.05421687 -0.62650602  2.65662651 -1.91566265 -0.06024096 \n\n\nThe Coefficients section gives a lot of numbers. The Estimate column gives the intercept, which again, is the predicted or fitted value of \\(y\\) when \\(x\\) is zero (i.e., the predicted value of the response variable when the explanatory variable has a value of zero). The slope of age is given in the age row, under the Estimate column. What this means, is that for each unit increase of the the explanatory (i.e., age), there is on average a 1.4277 unit increase in the response variable. So, in this toy example, this means that for each year older, a kid is 1.4277 inches taller. And, this general trend is statistically significant because the p-value (i.e., 0.02355) in the far right column (i.e., Pr(&gt;|t|)) is smaller than the alpha level of 0.05 (is we consider that alpha level to be our threshold below this statistical significance occurs)."
  },
  {
    "objectID": "lessons/linear_regression.html#predicted-values",
    "href": "lessons/linear_regression.html#predicted-values",
    "title": "Linear regression",
    "section": "Predicted values",
    "text": "Predicted values\nWe can get the predicted (aka. fitted) values (i.e., \\(\\hat{y}\\)) from the model and add them to the original dataframe.\n\nmorenos &lt;- morenos %&gt;% \n  mutate(predicted = predict(fitted_model))\nprint(morenos)\n\n# A tibble: 5 × 4\n  person height   age predicted\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Sam        72    18      72.1\n2 Eloisa     70    17      70.6\n3 Lola       69    14      66.3\n4 Heidi      63    13      64.9\n5 Ellen      62    11      62.1"
  },
  {
    "objectID": "lessons/linear_regression.html#residuals-1",
    "href": "lessons/linear_regression.html#residuals-1",
    "title": "Linear regression",
    "section": "Residuals",
    "text": "Residuals\nWe can get the residuals from the model and add them to the original dataframe. The residuals are the distances between the observed values and their corresponding predicted values.\n\nmorenos &lt;- morenos %&gt;% \n  mutate(residual = residuals(fitted_model)) \nprint(morenos)\n\n# A tibble: 5 × 5\n  person height   age predicted residual\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 Sam        72    18      72.1  -0.0542\n2 Eloisa     70    17      70.6  -0.627 \n3 Lola       69    14      66.3   2.66  \n4 Heidi      63    13      64.9  -1.92  \n5 Ellen      62    11      62.1  -0.0602\n\n\nLet’s now (more easily) plot the observed data points \\(y\\), the predicted values (i.e., \\(\\hat{y}\\)), and the residuals.\n\nmorenos %&gt;% \n  ggplot(aes(x = age, y = height))+\n  geom_smooth(method = lm, se = FALSE, linetype = 3)+\n  geom_point(aes(y = predicted), color = \"red\")+\n  geom_point()+\n  theme_minimal()+\n  geom_segment(aes(xend = age, yend = predicted), alpha = 0.5) \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/linear_regression.html#assumptions-of-linear-regression",
    "href": "lessons/linear_regression.html#assumptions-of-linear-regression",
    "title": "Linear regression",
    "section": "Assumptions of linear regression",
    "text": "Assumptions of linear regression\nLinear regression (like most inferential statistical tests) has a number of assumptions that should be met in order to obtain reliable results. I’m borrowing liberally from Levshina (2015, p. 155) here.\nAssumptions:\n\nThe observations are independent from one another. If the observations are related to each other (e.g., some/many tokens come from the same person or from the same word), then mixed-effect regression should be used with person and/or word specified as what’s called a random effect.\nThe response variable is continuous. If ordinal, then ordinal regression should be used instead. If categorical with two levels in the response variable, then logistic regression should be used. If categorical with three or more levels in the response variable, then multinomial regression should be used.\nThe relationship between the response and explanatory variable is linear. If not, then a transformation of the response and/or explanatory variables should be performed (e.g., taking the logarithm of word frequency).\nThe residuals of the model vary constantly. This is called homoscedasticity of variance. In other words, the variability of the residuals does not increase or decrease with the response variable nor with the explanatory variable(s). Fitting a scatterplot of the residuals by the predicted (aka. fitted) values of the linear model is a good way to detect if there is heteroscedasticity of variance. If the model was produced by lm(), you can simly call plot(variable_hold_model, which = 1). If p. 157 of Levshina (2015). If there is heteroscedasticity, a Box-Cox transformation of the response variable can help. If still problematic, a boostrapping procedure is the next step.\nThere is no multicollinearity of the explanatory variables, that is, they are not (overly) correlated with each other. Value Inflation Factors (VIF) scores below 5 or 10 indicate not much multicollinearity going on. The car package has a vif() function (and there are other packages with a similar functions.) See pp. 159-161 of Levshina (2015).\nThe residuals are not autocorrelated. A p-value below 0.05 in the durbinWatsonTest() function in the car package indicates autocorrelation. This is rarely a problem.\nThe residuals should be normally distributed, with a mean of zero. This assumption becomes less important as sample size increases. You can plot the residuals of a model with as a histogram or a density plot, after pulling the residuals out with residuals(). Also, you can use a Shapiro-Wilk test with shapiro.test() (p-values above [!] 0.05 suggest a normally distributed distribution), if the dataset has fewer than 5,000 observations.\n\nDetailed explanations of these assumptions and how to test for are offered by Levshina (2015, pp. 155 - 162)."
  },
  {
    "objectID": "lessons/linear_regression.html#activity",
    "href": "lessons/linear_regression.html#activity",
    "title": "Linear regression",
    "section": "Activity",
    "text": "Activity\nFrom Chapter 4 “Linear Regression 1” in Regression Modeling for Linguistic Data v1.1 by Sonderegger et al. (2022).\n\nUsing the languageR::english dataset, create a linear regression model with RTlexdec as the response variable and WrittenFrequency as the only explanatory variable.\nThen, figure out what the predicted RTlexdec for each of the following written frequencies. First, calculate the predicted RTlexdec values manually using the y-intercept and the slope given by the linear regression model (and probably using R as a basic calculator, e.g., \\(y\\text{-}intercept + slope * x\\)). Second, calculate the predicted RTlexdec values by using the predict.lm() function.\n\nWhat is the predicted RTlexdec value when WrittenFrequency is 5?\nHow about when WrittenFrequency is 10?\n\n\nAfter a good-faith effort, if you need help take a look at Dr. Brown’s code below.\n\n\nCode\n# fit linear regression model\nlanguageR::english %&gt;% \n  lm(RTlexdec ~ WrittenFrequency, data = .) -&gt; m1\nsummary(m1)\n\n\n\nCall:\nlm(formula = RTlexdec ~ WrittenFrequency, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45708 -0.11657 -0.00109  0.10403  0.56085 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       6.735931   0.006067 1110.19   &lt;2e-16 ***\nWrittenFrequency -0.037010   0.001134  -32.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1413 on 4566 degrees of freedom\nMultiple R-squared:  0.1891,    Adjusted R-squared:  0.1889 \nF-statistic:  1065 on 1 and 4566 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# save the intercept and slope to variables\nintercept &lt;- m1$coefficients[\"(Intercept)\"]\nslope &lt;- m1$coefficients[\"WrittenFrequency\"]\n\n# semi-manual math\nprint(intercept + slope * 5)\n\n\n(Intercept) \n    6.55088 \n\n\nCode\nprint(intercept + slope * 10)\n\n\n(Intercept) \n   6.365829 \n\n\nCode\n# use the predict.lm() function\npredict.lm(m1, newdata = tibble(WrittenFrequency = c(5, 10)))\n\n\n       1        2 \n6.550880 6.365829"
  },
  {
    "objectID": "lessons/linear_regression.html#more-than-one-explanatory-variable",
    "href": "lessons/linear_regression.html#more-than-one-explanatory-variable",
    "title": "Linear regression",
    "section": "More than one explanatory variable",
    "text": "More than one explanatory variable\nThe real usefulness of regression analysis is its ability to measure the influence of several explanatory variables at once in order to determine which variables significantly predict or explain the response variable.\nLet’s look at a multiple linear regression with three explanatory variables. Within the Rling package there is a dataset called ELP, which contains a subset of the English Lexicon Project. Let’s download the package from the companion website of the textbook How to Do Linguistics with R by Levshina. Then change the pathway in the following code and run it:\n\ninstall.packages(\"/pathway/to/Rling_1.0.tar.gz\", repos = NULL, type = \"source\")\n\nAnd then load up the dataset and inspect it a bit:\n\nlibrary(\"tidyverse\")\nlibrary(\"Rling\")\ndata(ELP)\nglimpse(ELP)\n\nRows: 880\nColumns: 5\n$ Word    &lt;fct&gt; rackets, stepmother, delineated, swimmers, umpire, cobra, vexe…\n$ Length  &lt;int&gt; 7, 10, 10, 8, 6, 5, 5, 8, 8, 6, 8, 12, 8, 6, 7, 3, 3, 10, 9, 4…\n$ SUBTLWF &lt;dbl&gt; 0.96, 4.24, 0.04, 1.49, 1.06, 3.33, 0.10, 0.06, 0.43, 5.41, 0.…\n$ POS     &lt;fct&gt; NN, NN, VB, NN, NN, NN, VB, NN, NN, NN, VB, NN, JJ, NN, NN, VB…\n$ Mean_RT &lt;dbl&gt; 790.87, 692.55, 960.45, 771.13, 882.50, 645.85, 760.29, 682.26…\n\nsummary(ELP)\n\n           Word         Length         SUBTLWF         POS     \n abbreviation:  1   Min.   : 3.00   Min.   :   0.020   JJ:159  \n abortions   :  1   1st Qu.: 7.00   1st Qu.:   0.180   NN:532  \n abrupt      :  1   Median : 8.00   Median :   0.570   VB:189  \n absentee    :  1   Mean   : 8.22   Mean   :   8.603           \n abutment    :  1   3rd Qu.:10.00   3rd Qu.:   2.105           \n accomplice  :  1   Max.   :20.00   Max.   :2556.730           \n (Other)     :874                                              \n    Mean_RT      \n Min.   : 517.5  \n 1st Qu.: 695.7  \n Median : 764.5  \n Mean   : 786.8  \n 3rd Qu.: 853.0  \n Max.   :1324.6  \n                 \n\n\nThe Word column is transparently named. The Length column is the length of the word in number of letters (as an integer). The SUBTLWF column gives the frequency (i.e., float or double or numeric) of the word normalized to per million words, as attested in a corpus of movie subtitles. The POS is transparently named (for a linguist, at least) and has three levels: JJ for adjective, NN for noun, and VB for verb. The Mean_RT column gives the mean reaction time in milliseconds (i.e., a float), and will the response variable in our regression.\nLet’s go with the linear regression!\nThe response variable (i.e., Mean_RT here) goes to the left of the formula operator (i.e., a tilde ~), and the explanatory variables to the right of that operator, separated by a plus sign +. The data frame with the dataset goes with the data argument. As is (very) common practice, we’ll take the log of frequency to put it on a more linear scale (from it’s original zipfian scale; see Vsauce’s video about lexical frequency).\nBehold, a linear regression and its results:\n\nm1 &lt;- lm(Mean_RT ~ Length + log(SUBTLWF) + POS, data = ELP)\nsummary(m1)\n\n\nCall:\nlm(formula = Mean_RT ~ Length + log(SUBTLWF) + POS, data = ELP)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-213.70  -62.55   -9.71   53.87  389.00 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   622.466     14.191  43.864  &lt; 2e-16 ***\nLength         19.555      1.433  13.645  &lt; 2e-16 ***\nlog(SUBTLWF)  -29.288      1.784 -16.420  &lt; 2e-16 ***\nPOSNN          -6.115      8.506  -0.719  0.47238    \nPOSVB         -29.184     10.154  -2.874  0.00415 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 93.29 on 875 degrees of freedom\nMultiple R-squared:  0.4565,    Adjusted R-squared:  0.454 \nF-statistic: 183.7 on 4 and 875 DF,  p-value: &lt; 2.2e-16\n\n\n\nInterpretation\nLet’s review the interpretation of the output. The Call section simply shows the lm() call. The Residuals section gives summary info about the residuals (but we could see the individual residuals by using the residuals() function). The Coefficients section gives the y-intercept and the slopes of the three explanatory variables.\nFirst, the Length row shows below the Estimate column that, on average, with each unit increases of the Length variable (i.e., each additional letter in word), there is a 19.555 milliseconds increase in Mean_RT (i.e., mean reaction time), and the p-value below the Pr(&gt;|t|) column is below 0.0000000000000002 (R’s default cutoff with p-values in this summary display). In simple terms, as the lengths of words increase, it takes humans significantly longer to react.\nThe log(SUBTLWF) row shows below the Estimate column that, on average, with each unit increase of frequency (on a log scale, given our use of log()), mean reaction times decrease by 29.288 milliseconds (note the negative sign before the number), and this happens at a statistically significant rate given the p-value.\nThe POS variable has two rows: POSNN and POSVB. But wait! What happened to JJ?! Because JJ comes first in the alphabet in comparison to NN and VB, it was chosen by R as the reference level against which the other levels in that categorical variable are compared. So, we compare each of the listed levels (i.e., NN and VB) to JJ, which is not listed. First things first, POSNN does not have a p-value below 0.05, so we conclude that there is no significant difference between NN and JJ in this dataset, and then we move on. Because POSVB’s p-value is below our alpha level of 0.05, we conclude that there is a significant difference between VB and JJ, such that, on average, reaction times to VB (i.e., verbs) is 29.184 milliseconds shorter (i.e., -29.184) than reaction times to JJ (i.e., adjectives).\nBy default, R chooses the level that comes first in alphabetical order to serve as the reference level. If we would like a different level to be the reference level, we can relevel the categorical variable so that another level is first with the fct_relevel() function. We saw this function previously in the context of boxplots, when we changed the order of the boxes."
  },
  {
    "objectID": "lessons/linear_regression.html#sum-contrasts",
    "href": "lessons/linear_regression.html#sum-contrasts",
    "title": "Linear regression",
    "section": "Sum contrasts",
    "text": "Sum contrasts\nThe comparison of a reference level to the other level(s) of a categorical variable is called “treatment contrasts”, and this is the default behavior of lm() in R. Another option is called “sum contrasts”, which is based on the mean of means of the response variable by each level in the categorical variable. The coefficients given indicate how much above or below that mean of means each level in the categorical is. For more info about sum contrasts, see this website or see p. 146 of Levshina (2015) or p. 201 of Regression Modeling for Linguistic Data v1.1 (among other good resources)."
  },
  {
    "objectID": "lessons/linear_regression.html#references",
    "href": "lessons/linear_regression.html#references",
    "title": "Linear regression",
    "section": "References",
    "text": "References\nLevshina, Natalia. 2015. How to do Linguistics with R: Data Exploration and Statistical Analysis. Amsterdam / Philadelphia: John Benjamins.\nSonderegger, Morgan. 2022. Regression Modeling for Linguistic Data. Available online.\nAlso, I thank the creators of these websites, which I relied on heavily: https://drsimonj.svbtle.com/visualising-residuals\nhttps://www.mathsisfun.com/data/least-squares-regression.html"
  },
  {
    "objectID": "lessons/regexes.html",
    "href": "lessons/regexes.html",
    "title": "Regular expressions",
    "section": "",
    "text": "Students will become proficient with writing regular expressions, including with capture groups and lookaround."
  },
  {
    "objectID": "lessons/regexes.html#objective",
    "href": "lessons/regexes.html#objective",
    "title": "Regular expressions",
    "section": "",
    "text": "Students will become proficient with writing regular expressions, including with capture groups and lookaround."
  },
  {
    "objectID": "lessons/regexes.html#regular-expressions-aka.-regexes",
    "href": "lessons/regexes.html#regular-expressions-aka.-regexes",
    "title": "Regular expressions",
    "section": "Regular expressions (aka. regexes)",
    "text": "Regular expressions (aka. regexes)\n\nRegular expressions are used to match strings. \n\nNote: In R, you must use double backslashes, e.g., \\\\w+\n\nOnline regex checker are useful, such as here (for Python), here, and here.\nLetters represent themselves: \"ed\" returns ed anywhere in the string, for example, Ed studied in the education building."
  },
  {
    "objectID": "lessons/regexes.html#character-classes",
    "href": "lessons/regexes.html#character-classes",
    "title": "Regular expressions",
    "section": "Character classes",
    "text": "Character classes\n\n\\\\w = alphanumeric character; \\\\W = non-alphanumeric character\n\\\\s = whitespace (i.e., spaces, tab breaks, newlines); \\\\S = non-whitespace\n\\\\d = Arabic numeral (i.e., 0-9); \\\\D = non-Arabic numeral\n[] = character class finds one of the characters between the square brackets: \n\n[aeiou] finds one of the five orthographic vowels\n[Aa] find either uppercase or lowercase a \n[a-z] finds one lowercase English character\n[a-zA-Z] returns one lowercase English character or one uppercase English character \nExample: \"latin[aox]\" returns latina, latino, latinx."
  },
  {
    "objectID": "lessons/regexes.html#the-pipe-which-is-just-above-the-return-key-on-my-keyboard-is-an-or-operator",
    "href": "lessons/regexes.html#the-pipe-which-is-just-above-the-return-key-on-my-keyboard-is-an-or-operator",
    "title": "Regular expressions",
    "section": "| (the “pipe” which is just above the return key on my keyboard) is an “or” operator:  ",
    "text": "| (the “pipe” which is just above the return key on my keyboard) is an “or” operator:  \n\nExample: \"\\\\bth(is|at|ese|ose) \\\\w+\" returns an English demonstrative determiner followed by a space, followed by a contiguous span or one or more of alphanumeric character, for example, this bag, that cat, these plants, those buildings.\nQuantifiers\n\n{min, max} = returns between min and max number of the previous character: \"\\\\w{2,5}\" returns between two and five alphanumeric characters. Note that \"\\\\w{,5}\" returns up to five alphanumeric characters, and \"\\\\w{2,}\" finds two or more alphanumeric characters.\n{integer} = returns the exact number of the previous character: \"\\\\d{4}\" returns exactly four Arabic numerals (for example, to find four-digit years in a text or corpus)\nShortcut quantifiers:\n\n? means the same as {0, 1}, meaning it returns zero or one of the previous pattern, that is, the previous character is optional\n* is the same as {0,} and returns zero or more of the previous pattern: yes\\\\!* returns yes, followed by any number of exclamation points, including none at all: yes, yes!, yes!!!, etc.. \n+ means {1,} and returns one or more of the previous pattern, for example, \"go+l\" returns gol, goool, gooooooool\n\n\n\n\nActivity\n\nWhat do the following regexes match? See example in Section 2.1 here.\n\n\"\\\\b[Tt]he\\\\b\\\\s+\\\\b[Ii]nternet\\\\b\"\n\"\\\\w+ed\\\\b\"\n\"\\\\bcent(er|re)\\\\b\"\n\"\\\\bwalk(s|ed|ing)?\\\\b\"\n\"\\\\b[^aeiou\\\\W]{2,}\\\\w+\"\n\"\\\\b[^aeiou\\\\W][aeiou]\\\\w+\""
  },
  {
    "objectID": "lessons/regexes.html#capture-groups",
    "href": "lessons/regexes.html#capture-groups",
    "title": "Regular expressions",
    "section": "Capture groups",
    "text": "Capture groups\n\nWarning: This gets wild. \nYou can have a regular expression remember what it captured in order to search for that same sequence of characters.\nYou can encapsulate a pattern in parentheses to capture, and then refer to that same sequence of characters with \\\\1 for the first capture group, or \\\\2 for the second capture group (if you have more than one capture group in the same regex), etc.\nExample: \"\\\\w+(\\\\w) \\\\1\\\\w+\" returns a bigram whose first word ends with the same letter that the second word begins with, e.g., walkeddown\n\n\nActivity\n\nWhat do the following regexes match?\n\n\"([aeiou])\\\\1\"\n\"\\\\w*([aeiou])\\\\1\\\\w*\"\n\"the (\\\\w+)er they were, the \\\\1er they will be\"\n\"[Tt]he (\\\\w+)er they (\\\\w+),? the \\\\1er we \\\\2\"\n\"\\\\w+(\\\\w{2,})\\\\W+\\\\w+\\\\1\\\\b\""
  },
  {
    "objectID": "lessons/regexes.html#lookaround",
    "href": "lessons/regexes.html#lookaround",
    "title": "Regular expressions",
    "section": "Lookaround",
    "text": "Lookaround\n\nLookaround allows you to use surrounding characters to find other characters, but to not consume those surrounding characters.\n\nSee lookahead examples in Section 2.1.7 here.\n\n\n\nActivity\n\nDownload at least several TXT files of your choice (perhaps from Project Gutenberg or Saints from the LMS).\nLoop over the files and search for a regex of your choice with a capture group, and print to screen the results. Use several different regex functions from the stringr package, for example, str_match_all(), str_extract_all(), str_locate_all().\nCreate a tabular dataset of your choice with a regex of your choice. As a first step, you might simple create a data frame with two columns: filename, and regex match.\nRamp it up by creating more columns, perhaps the number of characters in the match, or the number of (orthographic) vowels in the match, etc."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\nMy name is Earl Kjar Brown and I’m a Dane."
  },
  {
    "objectID": "demo.html",
    "href": "demo.html",
    "title": "demo",
    "section": "",
    "text": "Students will learn about Quarto."
  },
  {
    "objectID": "demo.html#objective",
    "href": "demo.html#objective",
    "title": "demo",
    "section": "",
    "text": "Students will learn about Quarto."
  },
  {
    "objectID": "demo.html#lets-get-rollin",
    "href": "demo.html#lets-get-rollin",
    "title": "demo",
    "section": "Let’s get rollin’!",
    "text": "Let’s get rollin’!\nWhat should I write? Here’s an example of some code:\n\n\nCode\nprint(\"hola mundo\")\n\n\n[1] \"hola mundo\"\n\n\nI just said “hello” in Spanish, with the code print(\"hola mundo\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ling_data_analysis",
    "section": "",
    "text": "Website with Earl Kjar Brown’s lesson plans for Linguistic Data Analysis with R. Click on the lesson plans in the navigation menu."
  },
  {
    "objectID": "lessons/webscrape_freq.html",
    "href": "lessons/webscrape_freq.html",
    "title": "Webscrape frequencies",
    "section": "",
    "text": "Students will use R to webscrape an already-created frequency list on the internet."
  },
  {
    "objectID": "lessons/webscrape_freq.html#objective",
    "href": "lessons/webscrape_freq.html#objective",
    "title": "Webscrape frequencies",
    "section": "",
    "text": "Students will use R to webscrape an already-created frequency list on the internet."
  },
  {
    "objectID": "lessons/webscrape_freq.html#overview-of-the-internet",
    "href": "lessons/webscrape_freq.html#overview-of-the-internet",
    "title": "Webscrape frequencies",
    "section": "Overview of the internet",
    "text": "Overview of the internet\nHere’s a super simplified overview of how the internet works: Clients (e.g., computers, smart phones) that are connected to the internet make HTTP requests to web servers (e.g., byu.edu, npr.org, instagram.com, etc.), and those web servers send back HTTP responses. Here’s a diagram to illustrate this basic idea."
  },
  {
    "objectID": "lessons/webscrape_freq.html#r-as-web-browser",
    "href": "lessons/webscrape_freq.html#r-as-web-browser",
    "title": "Webscrape frequencies",
    "section": "R as web browser",
    "text": "R as web browser\nR can act like a web browser by making HTTP requests and receiving HTTP responses from web servers. The rvest package here makes it easy to have R interact with the internet. That package also contains useful functions to parse the HTML in the HTTP response in order to extract information using either CSS selectors or XPath expressions.\nIt’s a good idea, and actually necessary with some websites, to make R look like a normal web browser when making the request to the web server. We do this by setting the user-agent to a common one that many web browsers use:\n\n# set the user-agent to make R look like a normal web browser to the web server\nhttr::set_config(httr::user_agent(\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"))"
  },
  {
    "objectID": "lessons/webscrape_freq.html#frequency-lists-on-the-internet",
    "href": "lessons/webscrape_freq.html#frequency-lists-on-the-internet",
    "title": "Webscrape frequencies",
    "section": "Frequency lists on the internet",
    "text": "Frequency lists on the internet\nThere are many frequency lists on the internet, and the free online dictionary wikitionary makes available a plethora of them here.\nSometimes, the frequency lists are in HTML tables, for example:\n\nThe 1,000 most frequent words in TV and Movie scripts and transcripts in English here.\nThe 1,900 most frequent Hindi words here.\nThe 10,000 most frequency Serbian words based on TV and Movie subtitles here.\n\nOther times, the frequency lists are presented as HTML lists (whether ordered or unordered), for example:\n\nThe 5,000 most frequent words in Danish here.\n1,000 Japanese basic words here.\nThe 2,000 most frequent words in fiction in English here."
  },
  {
    "objectID": "lessons/webscrape_freq.html#html-tables",
    "href": "lessons/webscrape_freq.html#html-tables",
    "title": "Webscrape frequencies",
    "section": "HTML tables",
    "text": "HTML tables\nLet’s webscrape a frequency list that is sitting in an HTML table. Let’s use the list of the 1,000 most frequent words in TV and Movie scripts in English here.\n\n# if need, use install.packages() first to download the following packages to your harddrive\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\nurl &lt;- \"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/TV/2006/1-1000\"\n\n# request the page\npage &lt;- read_html(url)\n\n# get the HTML table holding the frequency list\nfreqs_table &lt;- html_element(page, \"table\") \n\n# convert the HTML table into a data frame (i.e., tibble)\nfreqs_df &lt;- html_table(freqs_table, header = TRUE)\n\n# print the frequency table to the console\nprint(freqs_df)\n\nYou may have noticed that the above code could be put into a single pipeline by using the pipe operator repeatedly:\n\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/TV/2006/1-1000\" %&gt;% \n  read_html() %&gt;% \n  html_element(\"table\") %&gt;% \n  html_table(header = TRUE) %&gt;% \n  print()\n\n\nActivity\nYour turn! Find a frequency list of your choice online that is in an HTML table and webscrape it into a data frame in R. If you’ve having trouble finding one that is in an HTML table, try the 1,900 most frequent Hindi words here or the 10,000 most frequent Serbian words here.\nAfter a good-faith effort, if you need help, see Dr. Brown’s code below:\nHindi\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Hindi_1900\" %&gt;%  # put URL in a string\n  read_html() %&gt;%  # request page\n  html_element(\"table\") %&gt;%  # find first \"table\" HTML element\n  html_table() %&gt;%  # convert HTML table to data frame (i.e., tibble)\n  print()  # print data frame to console\n\n\nSerbian\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n# extra exercise: write in appropriate comments below\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Serbian_wordlist\" %&gt;% \n  read_html() %&gt;% \n  html_element(\"table\") %&gt;% \n  html_table() %&gt;% \n  print()"
  },
  {
    "objectID": "lessons/webscrape_freq.html#html-lists",
    "href": "lessons/webscrape_freq.html#html-lists",
    "title": "Webscrape frequencies",
    "section": "HTML lists",
    "text": "HTML lists\nWebscraping HTML lists, whether ordered (i.e., with numbers) or unordered (i.e., with bullets) is more work because we can’t use the slick html_table() function in rvest to convert an HTML table into a data frame in R. Rather, we have to identify which part of the data is the word and which part is the frequency. Further, we have to parse each frequency list separately, as we can’t assume that all frequency lists are formatted the same way.\nLet’s inspect the HTML of the frequency list of the 5,000 most frequency words in Danish in TV and Movie subtitles here. (Seriously, go inspect the HTML of that list before moving on.)\nWe see that the frequency list is an ordered list (HTML tag &lt;ol&gt;) and that each word and its corresponding frequency is in a list item (HTML tag &lt;li&gt;) and that the word and its frequency are separated by a space. We can use this information to scrape out the frequency list, and then use the space to identify the word and the frequency in order to put them into separate columns in a data frame in R.\nPro-tip: SelectorGadget (here) is an absolutely super helpful Google Chrome add-on extension that helps to quick identify the CSS Selector of elements of a webpage.\nLet’s get rolling with the code:\n\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n# put the URL in a string\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Danish_wordlist\" %&gt;% \n  \n  # request the HTML page\n  read_html() %&gt;% \n  \n  # extract the HTML elements &lt;li&gt; that are children of HTML elements &lt;ol&gt;\n  html_elements(css = \"ol li\") %&gt;% \n  \n  # keep only the text, that is, remove the HTML tags\n  html_text() %&gt;% \n  \n  # create a data frame with one column named \"both\"\n  # note: the dot below pipes the result of the previous step into where the dot is (rather than at the beginning of the function, which is the default behavior of the pipe operator)\n  tibble(both = .) %&gt;% \n  \n  # separate the \"both\" column into two columns (i.e., \"wd\" and \"freq\") on the space between the word and the frequency\n  separate_wider_delim(cols = \"both\", delim = \" \", names = c(\"wd\", \"freq\")) %&gt;% \n  \n  # print the data frame to the console\n  print()\n\n\nActivity\nIt’s that time of the class period: your turn!\nFind a frequency (or rank) list of your choice on the internet (probably at wikitionary.org) that is not in an HTML table and parse it into a data frame in R. If you need help finding one, try the 2,000 most frequent words in fiction in English here or the 1,000 Japanese basic words here.\nAfter a good-faith effort, if you need some help, see Dr. Brown’s code below:\nEnglish\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Contemporary_fiction\" %&gt;% \n  read_html() %&gt;% \n  html_elements(css = \"ol li\") %&gt;% \n  html_text() %&gt;% \n  tibble(wd = .) %&gt;% \n  print()\n\n\nJapanese\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\"https://en.wiktionary.org/wiki/Appendix:1000_Japanese_basic_words\" %&gt;% \n  read_html() %&gt;% \n  html_elements(css = \".mw-parser-output &gt; ul li\") %&gt;% \n  html_text() %&gt;% \n  tibble(entry = .) %&gt;% \n  print()"
  },
  {
    "objectID": "lessons/webscrape_freq.html#question",
    "href": "lessons/webscrape_freq.html#question",
    "title": "Webscrape frequencies",
    "section": "Question",
    "text": "Question\nIn most of the code in this lesson, the frequency or rank lists were simply printed to the console. How could we modify the code so that instead of printing to the console, we write the lists out to CSV files, one list per CSV file?"
  },
  {
    "objectID": "lessons/wrangling.html",
    "href": "lessons/wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Students will manipulate or wrangle a data frame."
  },
  {
    "objectID": "lessons/wrangling.html#objective",
    "href": "lessons/wrangling.html#objective",
    "title": "Data Wrangling",
    "section": "",
    "text": "Students will manipulate or wrangle a data frame."
  },
  {
    "objectID": "lessons/wrangling.html#the-dplyr-package",
    "href": "lessons/wrangling.html#the-dplyr-package",
    "title": "Data Wrangling",
    "section": "The dplyr package",
    "text": "The dplyr package\nThe dplyr R package is a core package of tidyverse, and has a handful of super useful functions for manipulating or wrangling a data frame. Most of the functions are verbs (e.g., filter(), select(), etc.) and so the documentation for the package often refers to the main functions of dplyr as verbs."
  },
  {
    "objectID": "lessons/wrangling.html#rows",
    "href": "lessons/wrangling.html#rows",
    "title": "Data Wrangling",
    "section": "Rows",
    "text": "Rows\nThere are three (I guess four) main functions (aka. verbs) for working with rows in dplyr: filter(), arrange() (and it’s helper function desc()), and distinct().\nPop (formative) quiz!\nInstructions: Match each function with its corresponding purpose.\n\n\n\n\n\n\n\nfilter()\nA. orders the rows based on values in one or more columns\n\n\narrange()\nB. inverts the order so that the rows are in big-to-small order\n\n\ndesc()\nC. keeps only unique rows\n\n\ndistinct()\nD. keeps rows that evaluate to TRUE in a conditional statement based on one or more columns\n\n\n\n\nActivity\nLet’s follow the examples given in Chapter 3 “Data transformation” of the book R for Data Science (2e). First, let’s download the nycflights13 R package to our harddrive:\n\ninstall.packages(\"nycflights13\", repos = \"https://cran.rstudio.com\")\n\nNow, let’s load the nycflights13 data frame into our R session or script, and our ol’ friend tidyverse:\n\nlibrary(\"nycflights13\")\nlibrary(\"tidyverse\")\n\nFinally, and most importantly, let’s try to solve the exercises in section 3.2.5 here. After some good-faith work, if you need some help, take a look at Dr. Brown’s code below:\n“Had an arrival delay of two or more hours”\n\n\nCode\nflights |&gt; \n  filter(arr_delay &gt;= 120)\n\n\n“Flew to Houston (IAH or HOU)”\n\n\nCode\nflights |&gt;  \n  filter(dest %in% c(\"IAH\", \"HOU\"))\n\n\n“Were operated by United, American, or Delta”\n\n\nCode\nflights |&gt;  \n  filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n\n“Departed in summer (July, August, and September)”\n\n\nCode\nflights |&gt;  \n  filter(month %in% c(7, 8, 9))  # note: the month column has a data type of integer, and therefore you don't need quotes around the numbers\n\n\nHere’s another way to filter for only summer flights:\n\n\nCode\nflights |&gt;  \n  filter(month &gt;= 7 & month &lt;= 9)\n\n\n“Arrived more than two hours late, but didn’t leave late”\n\n\nCode\nflights |&gt;  \n  filter(arr_delay &gt; 120 & dep_delay &lt;= 0)\n\n\n“Were delayed by at least an hour, but made up over 30 minutes in flight”\n\n\nCode\nflights |&gt;  \n  filter(dep_delay &gt;= 60 & arr_delay &lt; 30)\n\n\n“Sort flights to find the flights with longest departure delays.”\n\n\nCode\nflights |&gt;  \n  arrange(desc(dep_delay))\n\n\n“Find the flights that left earliest in the morning.”\n\n\nCode\nflights |&gt;  \n  arrange(dep_time)\n\n\n“Sort flights to find the fastest flights. (Hint: Try including a math calculation inside of your function.)”\n\n\nCode\nflights |&gt;  \n  arrange(desc(distance / air_time))\n\n\nHere’s another way, and a preview of the mutate function that we’ll see below:\n\n\nCode\nflights |&gt;  \n  mutate(speed = distance / air_time) %&gt;% \n  arrange(desc(speed))\n\n\n“Was there a flight on every day of 2013?”\n\n\nCode\nflights |&gt;  \n  distinct(year, month, day)\n\n\n“Which flights traveled the farthest distance?”\n\n\nCode\nflights |&gt;  \n  arrange(desc(distance))\n\n\n“Which traveled the least distance?”\n\n\nCode\nflights |&gt;  \n  arrange(distance)\n\n\n“Does it matter what order you used filter() and arrange() if you’re using both? Why/why not? Think about the results and how much work the functions would have to do.”\nEarl speculates that it’s probably best to filter first so that arrange has fewer rows to sort."
  },
  {
    "objectID": "lessons/wrangling.html#columns",
    "href": "lessons/wrangling.html#columns",
    "title": "Data Wrangling",
    "section": "Columns",
    "text": "Columns\nThere are four main functions (i.e., verbs) for working with columns: mutate(), select() (and its handful of helper functions here), rename(), and relocate().\nPop quiz time!\nInstructions: Match each function (i.e., verb) with its corresponding purpose.\n\n\n\n\n\n\n\nmutate()\nA. change the name of the specified column\n\n\nselect()\nB. creates new columns based on one or more already existing columns\n\n\nrename()\nC. moves the position of the specified columns\n\n\nrelocate()\nD. keeps only the specified columns\n\n\n\nLet’s try out the exercises provided in 3.3.5 of Chapter 3 “Data transformation” here. After a good-faith effort, if you need take a look at Dr. Brown’s code below each exercise.\n“Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?”\n“Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights”\n\n\nCode\nflights |&gt; \n  select(\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\")\n\n\n\n\nCode\nflights |&gt; \n  select(matches(\"^(dep_|arr_)\"))\n\n\n“Does the result of running the following code surprise you? How do the select helpers deal with upper and lower case by default? How can you change that default?”\n\n\nCode\nflights |&gt; \n  select(contains(\"TIME\"))\n\n\n“Rename air_time to air_time_min to indicate units of measurement and move it to the beginning of the data frame.”\n\n\nCode\nflights |&gt; \n  rename(air_time_min = air_time) |&gt; \n  relocate(air_time_min, .before = 1)\n\n\n“Why doesn’t the following work, and what does the error mean?”\n\nflights |&gt; \n  select(tailnum) |&gt; \n  arrange(arr_delay)\n\n\nActivity\nNow for some linguistic data. Download the “data_FRC_spch_rate.xlsx” Excel file from the Datasets module in the LMS (here). This dataset was used to write the article in the journal Corpus Linguistics and Linguistic Theory here.\nOpen it up in Excel (or Google Sheets) and inspect the columns in the data sheet and read what type of data each column holds in the legend sheet.\nNow, read in the data sheet into R as a data frame (probably a tibble). Take some time to practice using the functions (i.e., verbs) that work with rows and the functions that work with columns.\n\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\nsetwd(\"/pathway/to/dir\")\nsibilants &lt;- read_excel(\"data_FRC_spch_rate.xlsx\", sheet = \"data\")\n\nThen, try the following exercises. After a good-faith effort to complete these exercises on your own, if you need help take a look at Dr. Brown’s code.\n\nKeep only the rows with tokens of /s/ that were maintained as a sibilant.\n\n\nCode\nsibilants |&gt; \n  filter(s == \"maintained\")\n\n\nKeep only the rows of tokens of /s/ that were deleted (no sibilance) in word final position.\n\n\nCode\nsibilants |&gt; \n  filter(s == \"deleted\", wd_pos == \"wd_final\")\n\n\nCreate an alphabetized list of unique words with /s/ at the beginning of the word and following by high vowels.\n\n\nCode\nsibilants |&gt; \n  filter(wd_pos == \"wd_initial\", sound_post == \"HiV\") |&gt; \n  distinct(word) |&gt; \n  arrange(word)\n\n\nSort the data frame in descending order by lexical frequency, and then in ascending order by word.\n\n\nCode\nsibilants |&gt; \n  arrange(desc(lex_freq), word)\n\n\nKeep only rows with tokens with a speech rate of between 8 and 12 segments per second. Hint: The data type of the column with speech rate (i.e., spch_rate) may not have been read in as a numeric (aka. float) value. So, first you may need to coerce that column to a numeric data type with the base R function as.numeric().\n\n\nCode\nsibilants |&gt; \n  mutate(spch_rate = as.numeric(spch_rate)) |&gt; \n  filter(spch_rate &gt;= 8 & spch_rate &lt;= 12)\n\n\nCreate an alphabetized list of unique words with /s/ in which /s/ occurred in a tonic syllable.\n\n\nCode\nsibilants |&gt; \n  filter(stress == \"tonic\") |&gt; \n  distinct(word) |&gt; \n  arrange(word)"
  },
  {
    "objectID": "lessons/wrangling.html#group-by",
    "href": "lessons/wrangling.html#group-by",
    "title": "Data Wrangling",
    "section": "Group by",
    "text": "Group by\nAnother super useful function in dplyr is group_up(). It doesn’t change the data frame itself, but it performs subsequent functions based on the levels of a column or columns. It is often used in conjugation with the summarize() function to get group-level information.\nAs an example, let’s first calculate mean departure delay of all flights in the nycflights13 dataset, then next, let’s calculate the mean departure delay by airline (i.e., carrier column). However, as a preprocessing step, we need to remove rows what don’t have a number in dep_delay, but rather an NA, because the mean() function errs out with missing values. There are two ways to do this: (1) use filter() to keep only rows that don’t have an NA in the dep_delay column, or (2) tell the mean() function to remove the NAs with the na.rm argument.\nHere’s how to use filter() for that purpose:\n\nlibrary(\"nycflights13\")\nlibrary(\"tidyverse\")\n\nflights |&gt; \n  filter(!is.na(dep_delay)) |&gt;  # mind the exclamation point\n  summarize(average_departure_delay = mean(dep_delay))\n\nAnd here we use the na.rm argument within the mean() function:\n\nflights |&gt; \n  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE))\n\nSo, the mean departure delay across all airlines is 12.6 minutes. The natural follow-up question is whether some airlines have longer delays than others. Enter our new friend group_by(). Now, let’s perform this mean operation based on the levels (aka. values) of the carrier column in order to get the mean departure delay time by airline:\n\nflights |&gt; \n  group_by(carrier) |&gt; # we group before getting the mean\n  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE))\n\nMicro-activity: Modify the previous code block to sort the resulting data frame so that the airline with the shortest average delay is at the top of the resulting data frame and the airline with the longest average delay is at the bottom. After a good-faith effort, if you need some help, take a look at Dr. Brown’s code.\n\n\nCode\nflights |&gt; \n  group_by(carrier) |&gt; \n  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE)) |&gt; \n  arrange(average_departure_delay)\n\n\nIf you need to group by several columns, just add the additional column names to the group_by() call, for example:\n\nflights |&gt; \n  group_by(carrier, origin) |&gt; \n  summarize(average_departure_delay = mean(dep_delay, na.rm = TRUE))\n\nYou don’t have to use group_by() only with summarize(), but it’s a common use case. And, importantly, after summarize() has finished its work, it ungroups the data frame. If you were to use group_by() with other functions, they don’t automatically ungroup the data frame, and you would have explicitly do so with the ungroup() function. Here’s an example:\n\nflights |&gt; \n  group_by(carrier) |&gt; \n  mutate(aver_dep_delay_airline = mean(dep_delay, na.rm = TRUE)) |&gt; \n  select(carrier, dep_delay, aver_dep_delay_airline)\n\nNotice that the output says Groups: carrier [16]. This means that the data frame is still grouped by the 16 unique levels (aka. values) in the carrier column. In order to ungroup it, we can call ungroup() after we don’t need the groups anymore:\n\nflights |&gt; \n  group_by(carrier) |&gt; \n  mutate(aver_dep_delay_airline = mean(dep_delay, na.rm = TRUE)) |&gt; \n  select(carrier, dep_delay, aver_dep_delay_airline) |&gt; \n  ungroup()\n\n\nActivity\nUse the data sheet within the data_FRC_spch_rate.xlsx Excel workbook to get familiar with the group_by() and summarize().\n\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\nsetwd(\"/pathway/to/dir/\")\nsibilants &lt;- read_excel(\"data_FRC_spch_rate.xlsx\", sheet = \"data\")\n\nPerhaps you might try getting the mean and/or median speech rate (i.e., column name spch_rate) based on whether the /s/ was maintained as a sibilant or deleted (using the column s). Note: You’ll need to remove rows in the data frame that have NaN in spch_rate and then you need to convert spch_rate to a numeric data type. You can do both of these preprocessing steps right within the pipeline before getting the central tendency measures, if you want. If you get stuck, take a look at Dr. Brown’s code below.\n\n\nCode\nsibilants |&gt; \n  filter(spch_rate != \"NaN\") |&gt;  # more rows with \"NaN\"\n  mutate(spch_rate = as.numeric(spch_rate)) |&gt;  # convert data type from character to numeric\n  group_by(s) |&gt; \n  summarize(\n    mean_spch_rate = mean(spch_rate),\n    median_spch_rate = median(spch_rate))\n\n\nActivity: Try some other analyses with group_by() and summarize() and be prepared to share with a neighbor and/or the class."
  },
  {
    "objectID": "lessons/wrangling.html#joining-data-frames",
    "href": "lessons/wrangling.html#joining-data-frames",
    "title": "Data Wrangling",
    "section": "Joining data frames",
    "text": "Joining data frames\nJoining or merging two tables into one is often a necessary step in data analysis, including in linguistic data analysis. For example, let’s say we have one data frame (perhaps coming from an Excel file) that has tokens of sounds in words, and we have another data frame (perhaps from a CSV file) with frequencies of words from a large corpus. It would be slick to get the frequencies of word from the frequency data frame and put the appropriate frequency next to the words in the sound data frame. Enter the family of join functions in dplyr here.\nLet’s start with a super simple example in order to understand the concept, then we’ll apply this to bigger datasets. Let’s create a data frame (i.e., tibble) with two columns: a person, and the fruit that they said during a conversation:\n\nlibrary(tidyverse)\nmain_df &lt;- tibble(\n  speaker = c(\"Bob\", \"Bob\", \"Billy\", \"Billy\", \"Britta\", \"Britta\", \"Bonnie\"), \n  word = c(\"apple\", \"banana\", \"orange\", \"mango\", \"apple\", \"manzana\", \"kiwi\")\n)\nprint(main_df)\n\nNow, let’s create a data frame with some of the speakers’ ages:\n\nages &lt;- tibble(\n  person = c(\"Bonnie\", \"Billy\", \"Bob\"), \n  age = c(47, 6, 95)\n)\nprint(ages)\n\nYou may have noticed that while Britta said a couple words in our main_df data frame, she isn’t listed in the ages data frame. We’ll see what happens below when a value in a column in the main data frame doesn’t have a corresponding value in the to-be-joined data frame.\nIn order to create a new column with the age of each of the speakers in main_df, we can use the left_join() function to join main_df and ages. This function returns every row in the left-hand or first data frame in the function call (which, if we’re using a pipe operator, is the data frame to the left of the pipe), and only the rows in the right-hand or second data frame that have a corresponding value based on the columns specified with the by argument. By default, left_join joins by common column names, but we can explicitly specify which column in the left-hand data frame corresponds to which column in the right-hand data frame with the by argument. Let’s take a look:\n\nmain_df |&gt; \n  left_join(ages, by = join_by(speaker == person))\n\nQuestion: Why do we have speaker to the left of the double equal sign and person to the right?\nYou probably noticed that Britta’s age is NA because she isn’t listed in the ages data frame. If that’s a deal-breaker, then we can remove rows with NAs with filter() as a final step in the pipeline (mind the exclamation point before is.na()):\n\nmain_df |&gt; \n  left_join(ages, by = join_by(speaker == person)) |&gt; \n  filter(!is.na(age))\n\n\nToy activity\nLet’s create a toy data frame with frequencies of the words said by the speakers in main_df:\n\nfreqs &lt;- tibble(\n  wd = c(\"apple\", \"banana\", \"kiwi\", \"mango\"), \n  freq = c(123, 234, 345, 456)\n)\nprint(freqs)\n\nNow, join the freqs data frame to the main_df so that the frequency of each word is in a new column to the right of the column holding the age of the speakers, so that you end up with a data frame with four columns: speaker, word, age, freq. Give it a try, but if you get stuck, take a look at Dr. Brown’s code below. Hint: Make sure you are super aware of the column names in the various data frames that you need to join, so that you can give the correct names to the by argument.\n\n\nCode\nmain_df |&gt; \n  left_join(ages, by = join_by(speaker == person)) |&gt; \n  left_join(freqs, by = join_by(word == wd))\n\n\n\n\nActivity (with a review)\nLet’s ramp it up to a real activity. Try the following:\n\nReview: Create a data frame with at least one column named word (i.e., that has one word per row) using text files of your choice (e.g., Saints or texts from Project Gutenberg). You might like to use a regular expression to find different words (e.g., \"\\\\w+ed\\\\b\" or \"\\\\bre\\\\w+\"), as the next steps will be boring if all rows have the same word.\nReview: Create a data frame of frequencies of words from text files of your choice or webscrape a frequency list from the internet (e.g., Wikitionary).\nNote: If after a good-faith effort to complete the previous two steps, you can’t remember how to do these tasks, download and use two files in the LMS labeled Dataset_for_over_dan.csv and freqs_dan.csv. The first file has a keyword-in-context display of words in Danish that start with for or over while the second file has frequencies of words in Danish.\nHere’s the main exercise of the activity: Join the data frame with words to the data frame with frequencies, so that you have a new column (called freq) with the frequency of the word in the word (or node) column. Again, I can’t stress enough the importance of taking the time to double check that you know what the names are of the columns in the several data frame, so that you can correctly pass thme to the by argument."
  },
  {
    "objectID": "lessons/wrangling.html#separate-and-unite-columns",
    "href": "lessons/wrangling.html#separate-and-unite-columns",
    "title": "Data Wrangling",
    "section": "Separate and unite columns",
    "text": "Separate and unite columns\nOther useful functions, this time from the tidyr package within tidyverse, are the 3-member family of separate_* functions (i.e., separate_wider_delim(), separate_wider_position(), and separate_wider_regex()) and the unite() function.\nThe separate_* functions are transparently named and split up one column into two or more columns.\n\nThe separate_wider_delim() function splits on a delimiter given to the delim argument;\nThe separate_wider_position()function splits at fixed character widths given to the widths argument;\nThe separate_wider_regex() function splits on a regular expression given to the patterns argument.\n\nLet’s create a data frame with one column with two pieces of information in each row: the speaker who said something, following by a semi-colon, followed by what the speaker said:\n\nlibrary(tidyverse)\nhungry &lt;- tibble(\n  conversation = c(\"Bobby: I'm hungry!\", \"Cathy: Let's stop at In-n-Out. What do you say to that?!\", \"Bobby: Sounds good to me. What do you say João?\", \"João: Eu digo que sim!\", \"Roberto: Yo también.\")\n)\nprint(hungry)\n\nLet’s start with separate_wider_delim(). Pay attention to the arguments in the function.\n\nhungry |&gt; \n  separate_wider_delim(cols = conversation, delim = \": \", names = c(\"person\", \"utterance\"))\n\nThe function separate_wider_regex() takes a different approach. Rather than splitting on the given fixed-width delimiter given to the delim argument in the previous function, this function takes regular expressions that are used to extract matches out of the string in the row. Specifically, you give it a named character vector in which the names become column names and the elements of the vector are regular expressions to be matched. If no name is given, then that match doesn’t make it into the resulting data frame. Let’s use this function on the same hungry data frame from above. Notice that the regex \":\" doesn’t have a name, and therefore isn’t returned in the resulting data frame.\n\nhungry |&gt; \n  separate_wider_regex(cols = conversation, patterns = c(person = \"^[^:]+\", \":\", utterance = \"[^:]+$\"))\n\nFor good measure, we’d probably want to trim off that extra space at the beginning of each utterance:\n\nhungry |&gt; \n  separate_wider_regex(cols = conversation, patterns = c(person = \"^[^:]+\", \":\", utterance = \"[^:]+$\")) |&gt; \n  mutate(utterance = str_trim(utterance))\n\nDr. Brown hasn’t needed separate_wider_position() yet in his life, so let’s move on! If you think you might need it, take a look at the docs here.\nThe unite() function is the inverse of the separate_* functions, that is, it merges into one column two or more columns. It takes was input the data frame (probably piped in with the pipe operator), the col argument which specifies the name of the new column, then the names of the two or more columns to be merged, and the sep argument which specifies the character(s) used to join the values in the new column. There are other arguments; take a look at the docs here.\nLet’s continue with our hungry data frame from above. Let’s put the person and utterance columns back together, but this time with a dash separating the speakers and their corresponding utterances.\n\ndf_separated &lt;- hungry |&gt; \n  separate_wider_delim(cols = conversation, delim = \": \", names = c(\"person\", \"utterance\"))\nprint(df_separated)\n\ndf_separated |&gt; \n  unite(col = \"combined\", person:utterance, sep = \" - \")\n\n\nActivity\nUse some of the separate_*() functions and unite() on the data frame of your choice."
  },
  {
    "objectID": "lessons/wrangling.html#pivoting-data-frames",
    "href": "lessons/wrangling.html#pivoting-data-frames",
    "title": "Data Wrangling",
    "section": "Pivoting data frames",
    "text": "Pivoting data frames\nAnother useful, and often necessary, step in (linguistic) data analysis is changing the shape of a data frame before running an analysis. Some functions in R and in other languages (e.g., Python and Julia) and in other software (e.g., Excel) require the data frame to be in a specific shape in order to do certain analyses. Being able to transform the data to that shape is an important preprocessing step.\nThe two main functions for changing the shape of a data frame within tidyverse are pivot_longer() and pivot_wider(). The function pivot_longer() increases the number of rows and decreases the number of columns, while pivot_wider() decreases the number of rows and increases the number of columns.\nLet’s take a super simple example. Let’s create a toy data frame with one row and five columns with the ages of people in a sample:\n\nlibrary(\"tidyverse\")\nages &lt;- tibble(Sammy = 20, Edna = 18, Luisa = 16, Heidi = 14, Evelyn = 12)\nprint(ages)\n\nAs we see, each column represents one person, with the age in the row. Let’s reshape the data frame so that we end up with five rows and two columns, with each row giving info about one person, and the first column giving the person’s name and the second column giving the person’s age:\n\nages |&gt; \n  pivot_longer(cols = Sammy:Evelyn, names_to = \"person\", values_to = \"age\")\n\nThe resulting data frame is a “tidy” data frame because each column has only one variable (e.g., person or age) and the rows represent individual observations. The pre-transformed data frame above was not tidy because each column had more than one variable (e.g., person and age). Read Hadley Wickham’s paper for a full explanation of tidy data.\nThe pivot_wider() does the opposite, that is, it make a long(er) data frame wide(r). Let’s create another toy data frame, this time in long format and make it wide:\n\nheights &lt;- tibble(\n  person = c(\"Sammy\", \"Edna\", \"Luisa\", \"Heidi\", \"Evelyn\"),\n  height_in = c(72, 70, 69, 63, 64))\nprint(heights)\n\nLet’s pivot this data frame to a wide format so that there are five columns, with each person’s name was the column name, and the rows are the corresponding heights in inches:\n\nheights |&gt; \n  pivot_wider(names_from = person, values_from = height_in)\n\n\nToy activity and review\nCreate one data frame with the ages and heights of the five people in the above toy examples. Hint: In addition to pivot_longer() and/or pivot_wider(), you’ll also need our ol’ friend left_join() that we saw above.\nAfter a good-faith effort, if you need help take a look at Dr. Brown’s code below:\n\n\nCode\nages |&gt; \n  pivot_longer(cols = Sammy:Evelyn, names_to = \"person\", values_to = \"age\") |&gt; \n  left_join(heights, by = join_by(person == person))\n\n\n\n\nVowels\nLet’s take a look at a linguistic example from Dr. Stanley. Download the sample_vowel_data.csv file from the LMS &gt; Datasets. Currently, each row represents one time point for each vowel. There are 27 rows containing F1, F2, and F3 measurements from three tokens of three vowels each, at three timepoints.\nLet’s load the dataset into our R session as a data frame:\n\nlibrary(\"tidyverse\")\nvowels &lt;- read_csv(\"/pathway/to/sample_vowel_data.csv\")\nprint(vowels)\n\nLet’s change it into a wider format so that the unit of observation (i.e., row) is one vowel. There are 9 rows. Also, save it to a variable named widest so we can work with it later.\n\nwidest &lt;- vowels |&gt; \n  pivot_wider(names_from = percent, values_from = c(F1, F2, F3))\nprint(widest)\n\nLet’s change it to a longer format. Now the unit of measurement is one format measurement, per time point, per vowel token. There are 81 rows. Note that I’ll need to specify which columns should be affected with the cols argument.\n\nlongest &lt;- vowels |&gt; \n    pivot_longer(cols = c(F1, F2, F3), \n                 names_to = \"formant\", \n                 values_to = \"hz\") |&gt; \n    arrange(formant)\nprint(longest)\n\nLet’s use our widest data frame and pivot it longer. Also, as a shorthand for specifying the columns I want to pivot, I can use matches() and a regex:\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = \"formant_percent\", \n                 values_to = \"hz\")\n\nHere, because each column name contained two pieces of information (F1_25), the resulting data set after pivoting contains a single column with two pieces of information: formant and percent.\nEnter our ol’ friend separate():\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = \"formant_percent\", \n                 values_to = \"hz\") |&gt; \n    separate(formant_percent, \n             into = c(\"formant\", \"percent\"), \n             sep = \"_\")\n\nBut the separation above can happen within pivot functions directly. Below, we provide a vector of names as the value of the names_to argument. This then requires the use of the names_sep argument, which is where you specify the character that separates the two names.\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = c(\"formant\", \"percent\"), \n                 names_sep = \"_\", \n                 values_to = \"hz\")\n\n\n\nRamping it up\nLet’s say we start off with what we see as the widest version of the data in the widest data frame. This is not a hypothetical: this is how a lot of sociophonetics software returns the data to the user! We want to pivot it so that it looks like our original version in the vowels data frame. One way is to make it longer and then wider again.\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = c(\"formant\", \"percent\"),\n                 names_sep = \"_\",\n                 values_to = \"hz\") |&gt; \n    pivot_wider(names_from = formant,\n                values_from = hz)\n\nThis produces the correct output, but it’s a little clunky to do two pivots.\nEnter the black magic that is .value. So instead, we replace the formant column name (in the names_to argument) with .value (mind the period). This then, somehow, takes what would have been the formant column and pivots it wider. Note that when we do this, we don’t need the values_from argument anymore.\n\nwidest |&gt; \n    pivot_longer(cols = matches(\"F\\\\d_\\\\d\\\\d\"), \n                 names_to = c(\".value\", \"percent\"),\n                 names_sep = \"_\")\n\n\n\nBig badboy activity\nDownload the “RPM_2019_for_Reliability.xlsx” Excel file from LMS &gt; Datasets module and transform (aka. transpose) the dataset into the shape asked for in the email below that Dr. Brown received from his brother Alan:\n“I’m sending along that data set that I had originally sent that had those problems. Stayc cleaned it all up and we took some students out who spoke other languages at home. The sheet of interest is the first one labeled “For Earl”. As we had talked about, we just need the data oriented horizontally by item number so across the top the columns would run from 1 to 36 left to right. Each row, then, would represent a different student and ’1’s or ’0’s in the columns would indicate correct or incorrect for each item/column. The initial three columns to the left would be the demographic information like gender, grade, immersion/non-immersion. Let me know if that doesn’t make sense.”\nThe first ten rows (i.e., students, of the 104 students) of the transposed dataset should look like the following:\n\n\n\nTransposed dataset\n\n\nA couple words to the wise:\n\nAs is often the case in the wild, this dataset is squirrelly. Take a look at the bottom of the dataset (in Excel) and you’ll see that there is “104” below the last row of the first column. You’ll need to use the range argument in the read_excel() function (within the readxl package) to read in only the cells with actual student data, that is, rows with something in all five columns.\nThe “104” indicates that there are 104 students in the dataset. The answers to the 36 questions that each student responded to are in sets of 36 rows, with the question number indicated by in the Page/Item column. You’ll see that the numbers in that column go from 1 to 36, and then start over at 1 with the next student.\nThere is no unique identifier (e.g., ID number) for each student. Before transposing the data frame, you’ll need to create a new column with a unique identifier. You can use the rep() function, with its each argument, to do this. See Stack Overflow thread for an example.\nAfter transposing the data frame, you should move the column with the unique identifier to the far left.\n\nAfter a good-faith effort to complete this activity, if you need help take a look at Dr. Brown’s code below:\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"readxl\")\n\"/pathway/to/RPM_2019_for_Reliability.xlsx\" |&gt; \n  read_excel(range = \"For Earl!A1:E3745\") |&gt; \n  mutate(id = rep(1:104, each = 36)) |&gt; \n  pivot_wider(names_from = \"Page/Item\", values_from = \"Correct/incorrect\") |&gt; \n  relocate(id, .before = 1)"
  },
  {
    "objectID": "lessons/wrangling.html#working-with-categorical-variables",
    "href": "lessons/wrangling.html#working-with-categorical-variables",
    "title": "Data Wrangling",
    "section": "Working with categorical variables",
    "text": "Working with categorical variables\nThere are a few handy functions in the forcats package within tidyverse for working with categorical variables (aka. factors).\nThe fct_recode() function allows the user to manually change or combine levels (aka. values) of a categorical variable. Let’s create a toy data frame with words and vowels:\n\nlibrary(\"tidyverse\")\nwords &lt;- tibble(\n  word = c(\"wasp\", \"fleece\", \"beat\", \"bit\", \"pat\", \"bus\", \"hot\", \"cool\", \"firm\", \"father\"),\n  vowel = c(\"[ɑ]\", \"[i]\", \"[i]\", \"[ɪ]\", \"[æ]\", \"[ʌ]\", \"[ɔ]\", \"[u]\", \"[ɚ]\", \"[ɑ]\")\n)\nprint(words)\n\nNow, let’s create a new column in which we describe (some of) the vowels:\n\nwords |&gt; \n  mutate(description = fct_recode(vowel, \"high front closed\" = \"[i]\", \"r colored schwa\" = \"[ɚ]\", \"stressed schwa (wedge)\" = \"[ʌ]\"))\n\nThe fct_collapse() function allows the user to put levels into groups:\n\nwords |&gt; \n  mutate(height = fct_collapse(vowel, \n                               hi_V = c(\"[i]\", \"[ɪ]\", \"[u]\"), \n                               lo_V = c(\"[ɑ]\", \"[æ]\"),\n                               other_level = \"mid_V\"))\n\nQuestion: What does the other_level argument do in the function call above?\nThe fct_relevel() function allows the user to reorder the levels of a categorical variable. This is often useful when plotting data and the user wants an order of levels different from alphabetical order (the default).\nLet’s create a toy data frame of voiceless plosives and their voice onset times (VOT):\n\nlibrary(tidyverse)\nvoiceless_plosives &lt;- tibble(\n  plosive = c(\"t\", \"k\", \"p\", \"p\", \"k\", \"t\", \"p\", \"k\", \"t\"),\n  vot = c(23, 34, 45, 56, 67, 78, 89, 91, 21)\n)\nprint(voiceless_plosives)\n\nIf we create a boxplot (preview of what’s coming soon!), we see that “k” is on the left, followed by “p” and “t”, because the default way to sort levels is by alphabetical order:\n\nvoiceless_plosives |&gt; \n  ggplot(aes(x = plosive, y = vot)) +\n  geom_boxplot()\n\nWe probably want “p” followed by “t” and then “k”, as that’s how they are presented in linguistic literature, based on the place of articulation. Let’s use fct_relevel() to change the internal order of the levels, and then replot their VOTs:\n\nvoiceless_plosives |&gt;\n  mutate(plosive = fct_relevel(plosive, \"p\", \"t\", \"k\")) |&gt; \n  ggplot(aes(x = plosive, y = vot)) +\n  geom_boxplot()\n\n\nActivity\nLet’s create a toy data frame with Spanish words with a dental or alveolar word-final sibilant:\n\nlibrary(\"tidyverse\")\nsibilants &lt;- tibble(\n  person = c(\"Raúl\", \"Raúl\", \"José\", \"José\", \"María\"),\n  target_wd = c(\"árboles\", \"mesas\", \"lápiz\", \"es\", \"pues\"),\n  next_wd = c(\"de\", \"en\", \"y\", \"que\", \".\"),\n  next_segment = c(\"d\", \"e\", \"i\", \"k\", \"#\")\n)\nprint(sibilants)\n\nYour task is to create a new column that groups the vowels into one level, the consonants into their own group, and then all other following segments should be placed into an other level.\nAfter a good-faith effort, if you need help, take a look at Dr. Brown’s code below:\n\n\nCode\nsibilants |&gt;  \n  mutate(next_sound_type = fct_collapse(next_segment, vowel = c(\"i\",\"e\"), consonant = c(\"d\", \"k\"), other_level = \"other\"))"
  },
  {
    "objectID": "lessons/anova.html",
    "href": "lessons/anova.html",
    "title": "ANOVA",
    "section": "",
    "text": "Students perform an ANOVA analysis and interpret the results."
  },
  {
    "objectID": "lessons/anova.html#whats-in-a-name",
    "href": "lessons/anova.html#whats-in-a-name",
    "title": "ANOVA",
    "section": "What’s in a name?",
    "text": "What’s in a name?\nANOVA stands for Analysis of Variance. It’s very much like the t-test and Wilcoxon test, but with three or more levels (aka. groups) in the categorical variable.\n\n\n\n\n\n\n\nt-test or Wilcoxon test\nANOVA\n\n\nold v. young\nold v. middle age v. young\n\n\nwomen v. men\nwomen v. men. v. kids\n\n\nMandarin v. Cantonese\nMandarin v. Cantonese v. Wu v. Hokkien v. Gan v. Hunanese\n\n\n\nAn ANOVA calculates an F statistic which, at a high level is calculated with the following formula:\n\\[\n\\frac{variation\\ between\\ groups}{variation\\ within\\ groups}\n\\]\nThe hypotheses are:\n\\[\nH_0:μ_1=μ_2=μ_3...=μ_n\n\\]\n\\[\nH_a:At\\ least\\ one\\ of\\ the\\ groups\\ is\\ different\\ from\\ the\\ others.\n\\]\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"joeysvowels\")\nfront_vowels &lt;- joeysvowels::midpoints %&gt;%\n  filter(vowel %in% c(\"TRAP\", \"FACE\", \"DRESS\", \"FLEECE\", \"KIT\")) %&gt;% \n  mutate(dur = end - start)\nfront_vowels %&gt;% \n  ggplot(aes(x = vowel, y = dur))+\n  geom_boxplot(notch = TRUE)\n\nNotch went outside hinges\nℹ Do you want `notch = FALSE`?\n\n\n\n\n\nThose notches are trying to tell a story, amirite?!\nLet’s see what an ANOVA says:\n\nfront_vowels %&gt;% \n  aov(dur~vowel, data = .) -&gt; result\nsummary(result)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nvowel         4 0.2875 0.07188   31.23 &lt;2e-16 ***\nResiduals   211 0.4856 0.00230                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOkay, the ANOVA feels that there is something goin’ on here. Just look at that p-value well below 0.05. But, the immediate follow-up question is which groups are significantly different from other groups.\nEnter the post-hoc test. Actually, there are quite a few post-hoc test, but we’ll use the Tukey Honest Significant Difference post-hoc test (because that’s what Dr. Brown sees a lot in publications):\n\nTukeyHSD(result)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = dur ~ vowel, data = .)\n\n$vowel\n                     diff         lwr          upr     p adj\nFACE-TRAP    -0.016746918 -0.04303958  0.009545744 0.4042499\nDRESS-TRAP   -0.076731562 -0.10441226 -0.049050863 0.0000000\nFLEECE-TRAP  -0.022437110 -0.05725583  0.012381610 0.3920587\nKIT-TRAP     -0.084665180 -0.10918029 -0.060150069 0.0000000\nDRESS-FACE   -0.059984644 -0.08942775 -0.030541539 0.0000006\nFLEECE-FACE  -0.005690191 -0.04192579  0.030545407 0.9927010\nKIT-FACE     -0.067918262 -0.09440724 -0.041429282 0.0000000\nFLEECE-DRESS  0.054294452  0.01703945  0.091549460 0.0007958\nKIT-DRESS    -0.007933618 -0.03580086  0.019933621 0.9352192\nKIT-FLEECE   -0.062228070 -0.09719527 -0.027260869 0.0000190\n\n\nSimple enough. The p-value tell of which pairwise comparisons are statistically significantly different from each other."
  },
  {
    "objectID": "lessons/anova.html#activity",
    "href": "lessons/anova.html#activity",
    "title": "ANOVA",
    "section": "Activity",
    "text": "Activity\nYour turn! Find a dataset of your choice and load it up, and find a continuous variable and a categorical variable with at least three levels, and … you guessed, run an ANOVA."
  },
  {
    "objectID": "lessons/quarto.html",
    "href": "lessons/quarto.html",
    "title": "Quarto documents",
    "section": "",
    "text": "Students will write a Quarto document that integrates prose and code, and the output of the code."
  },
  {
    "objectID": "lessons/quarto.html#objective",
    "href": "lessons/quarto.html#objective",
    "title": "Quarto documents",
    "section": "",
    "text": "Students will write a Quarto document that integrates prose and code, and the output of the code."
  },
  {
    "objectID": "lessons/quarto.html#reproducible-research",
    "href": "lessons/quarto.html#reproducible-research",
    "title": "Quarto documents",
    "section": "Reproducible research",
    "text": "Reproducible research\nClearing laying out methods is an important consideration when publishing original research so that others can reproduce or replicate your research. One useful way to do that is to make available the computer code that goes into the data collection and data analysis parts of the research process.\nEnter documents that knit together code and prose. The current cool-kid-on-the-block is the Quarto document. All the lesson plans that Dr. Brown has created have been produced as Quarto documents. The RStudio IDE makes it super easy to write Quarto document and integrate R code (and Python and Julia code too!) with prose around the code. Also, the writer of the document can specify whether the output of the code is rendered. For example, Dr. Brown usually does not render the output of the R code."
  },
  {
    "objectID": "lessons/quarto.html#getting-started",
    "href": "lessons/quarto.html#getting-started",
    "title": "Quarto documents",
    "section": "Getting started",
    "text": "Getting started\nSteps:\n\nWithin RStudio, click on File &gt; New File &gt; Quarto Document…\nGive the document a name\nStart writing prose and/or code!"
  },
  {
    "objectID": "lessons/quarto.html#choose-your-own-adventure",
    "href": "lessons/quarto.html#choose-your-own-adventure",
    "title": "Quarto documents",
    "section": "Choose your own adventure",
    "text": "Choose your own adventure\n…Or jump between adventures, as needed.\nWithin RStudio, you can use the Source editor or the Visual editor, or jump back and forth as needed. For example, Dr. Brown uses the Visual editor the vast majority of the time, but occasionally jumps over to the Source editor in order to have more control over formatting.\nIn order to insert a code block on a new line, you can type a forward slash “/”, and a drop-down menu will appear from which you can select the type of code you want to insert (most commonly R code). There’s also a keyboard short to insert an R code block: Mac &gt; Cmd + Opt + i; Windows &gt; Ctrl + Alt + i."
  },
  {
    "objectID": "lessons/quarto.html#markdown-syntax",
    "href": "lessons/quarto.html#markdown-syntax",
    "title": "Quarto documents",
    "section": "Markdown syntax",
    "text": "Markdown syntax\nQuarto documents accept Markdown syntax to format the prose. It provides the basics of word processing documents, like different levels of headers, bold font, italics, underlined, etc. Importantly and usefully, it also provides a way to format code within the prose, for example, print(\"hello world\") returns hello world.\n\nActivity\nTake a look at the two cheatsheets that RStudio has about Markdown syntax (Help &gt; Cheat Sheets &gt; R Markdown Cheat Sheet & R Markdown Reference Guide) or websites about Markdown, for example, markdownguide.org.\nNow, create a Quarto document and use the Source editor only to create a (simple) HTML output document.\nNext, create another Quarto document, but this time, use the Visual editor as much as possible.\nBe prepared to share your documents with the members of the class."
  },
  {
    "objectID": "lessons/t-test.html",
    "href": "lessons/t-test.html",
    "title": "t-test, Wilcoxon test",
    "section": "",
    "text": "Students will perform a t-test and a Wilcoxon test"
  },
  {
    "objectID": "lessons/t-test.html#objective",
    "href": "lessons/t-test.html#objective",
    "title": "t-test, Wilcoxon test",
    "section": "",
    "text": "Students will perform a t-test and a Wilcoxon test"
  },
  {
    "objectID": "lessons/t-test.html#formative-quiz",
    "href": "lessons/t-test.html#formative-quiz",
    "title": "t-test, Wilcoxon test",
    "section": "Formative quiz",
    "text": "Formative quiz\nMatch each term with its corresponding definition.\n\nGood ol’ fashioned matching quiz\n\n\n\n\n\n\nsample\nThe mean of the sample\n\n\npopulation\nA value that is the result of converting a value into units of standard deviations from the mean\n\n\npoint estimate\nA Greek letter that usually stands for the standard deviation of a continuous variable of the population\n\n\nconfidence interval\nA Romanized letter that usually stands for the explanatory variable\n\n\nstandard error\nSomething that might affect the thing under study\n\n\nt-test\nThe thing that is being studied (aka. dependent thing of study)\n\n\nWilcoxon test\na Greek letter that usually stands for the mean of a continuous variable of a population\n\n\nsigma\nAn ideal spread of data points around the central tendency; looks like a good ol’ fashioned bell (aka. bell curve)\n\n\nmeu\nthe number of data points (aka. observations)\n\n\nn\nThe standard deviation of the sample\n\n\nx\nThe group that the researcher want to know something about\n\n\ny\nA non-parametric test\n\n\nresponse variable\nThe group that is used to infer about a larger group\n\n\nexplanatory variable\nThe proposition that is likely an effect of the explanatory variable on the response variable\n\n\nz-score\nA Romanized letter that usually stands for the response variable.\n\n\nmeu hat\na single number that characterizes an aspect of a dataset\n\n\nsigma hat\nA range of values that is likely to contain the population value\n\n\nalpha level\nThe threshold below which the null hypothesis can be rejected\n\n\nnull hypothesis\nA parametric test with a continuous response variable and a categorical variable with exactly two levels\n\n\nalternative hypothesis\nA measure of how much uncertainty there is in the estimate of the sample mean\n\n\nnormal distribution\nThe proposition that there is no effect of the explanatory variable(s) on the response variable"
  },
  {
    "objectID": "lessons/t-test.html#t-test",
    "href": "lessons/t-test.html#t-test",
    "title": "t-test, Wilcoxon test",
    "section": "t-test",
    "text": "t-test\nThe response variable is continuous (aka. quantitative) and the explanatory variable is categorical with exactly two levels (aka. values or groups).\n\nAssumptions\nThe distribution of the response is normal, and/or there are more than 30 data points in each level (aka. group) of the explanatory variable.\nThe variances of the response variables in both levels of the explanatory are equal. However, this is often not the case and our new friend Mr. Welch proposed an adjustment that provides a correction for unequal variances. The default behavior of the t.test() in R (i.e., the var.equal argument) is to use Welch’s adjustment.\n\n\nOne-tailed or two-tailed\nA one-tailed test should be used when the alternative hypothesis is directional, that is, it hypothesizes that the mean of one group is greater than the mean of the other group, or less than the other group. For example, the null and alternative hypotheses would look something like this:\n\\[\nH_0:μ_A = μ_B\n\\] \\[\nH_a:μ_A&gt;μ_B\n\\]\nWe use a two-tailed t-test when our alternative hypothesis is not directional, that is, it hypothesizes that the mean of one group does not equal the mean of the other group, but it doesn’t say anything about direction. For example:\n\\[\nH_0:μ_A = μ_B\n\\] \\[\nH_a:μ_A ≠ μ_B\n\\]"
  },
  {
    "objectID": "lessons/t-test.html#wilcoxon-test",
    "href": "lessons/t-test.html#wilcoxon-test",
    "title": "t-test, Wilcoxon test",
    "section": "Wilcoxon test",
    "text": "Wilcoxon test\nWhen the data are not normally distributed (and you don’t have &gt;30 observations in each level of the categorical explanatory variable, so that you can use the t-test), the Wilcoxon test is the way to go. This test is also the way to go when you have ordinal data, like responses to a Likert-scale survey.\nThe Wilcoxon test is apparently super similar to, or in most cases identifical to, the Mann-Whitney test; see Q&A thread. Our ol’ friend Jamovi has a check box for the “Mann-Whitney” test, while the function in R run these test is wilcox.test()."
  },
  {
    "objectID": "lessons/t-test.html#question",
    "href": "lessons/t-test.html#question",
    "title": "t-test, Wilcoxon test",
    "section": "Question",
    "text": "Question\nLet’s look at the speech rate in syllables per second of 25 Spanish speakers and 25 Italian speakers. First, let’s propose a null hypothesis and an alternative hypothesis. As we don’t have any previous information about the speech rate of Spanish and Italian speakers in comparison to each other, our alternative hypothesis will simply be “not equal to” rather than something directional like “greater than” or “less than”:\n\\[\nH_0:μ_S=μ_I\n\\] \\[\nH_a:μ_S≠μ_I\n\\]\nFirst, a good ol’ fashioned boxplot:\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nesit &lt;- read_csv(\"/Users/ekb5/Documents/data_analysis/datasets/span_ital_speaking_rate.csv\")\n\nRows: 50 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): language\ndbl (2): participant_ID, syllables\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nesit %&gt;% \n  ggplot(aes(x = language, y = syllables))+\n  geom_boxplot(notch = TRUE)\n\n\n\n\nThe non-overlapping notches on the boxes suggest that the difference in the means of the two groups will likely be statistically significantly different.\nEnter an inferential statistical test.\nFirst, let’s see if the data points are normally distributed. A simple Shapiro-Wilk normality test:\n\nesit %&gt;% \n  group_by(language) %&gt;% \n  do(\n    shapiro.test(.$syllables) %&gt;% \n      broom::tidy()\n  )\n\n# A tibble: 2 × 4\n# Groups:   language [2]\n  language statistic p.value method                     \n  &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                      \n1 Italian      0.971  0.663  Shapiro-Wilk normality test\n2 Spanish      0.926  0.0710 Shapiro-Wilk normality test\n\n\nBoth p-values are above 0.05, so that’s good.\nNow, let’s also plot the distribution in a density plot:\n\nesit %&gt;% \n  ggplot(aes(syllables, color = language))+\n  geom_density()\n\n\n\n\nNot the prettiest (normal) density plots.\nAnd for good measure, let’s calculate the skewness of each group:\n\ninstall.packages(\"moments\", repos = \"http://rstudio.org\")\n\nWarning: unable to access index for repository http://rstudio.org/src/contrib:\n  cannot open URL 'http://rstudio.org/src/contrib/PACKAGES'\n\n\nWarning: package 'moments' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n\nWarning: unable to access index for repository http://rstudio.org/bin/macosx/big-sur-arm64/contrib/4.3:\n  cannot open URL 'http://rstudio.org/bin/macosx/big-sur-arm64/contrib/4.3/PACKAGES'\n\n\n\nesit %&gt;% \n  group_by(language) %&gt;% \n  do(\n    moments::skewness(.$syllables) %&gt;% \n      broom::tidy()\n  )\n\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\n\n# A tibble: 2 × 2\n# Groups:   language [2]\n  language      x\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Italian  -0.216\n2 Spanish  -0.923\n\n\nA good rule of thumb is that if the skewness score is between -0.5 and 0.5, the data is normal enough to continue with a parametric test.\nBecause the Shapiro-Wilk gave us the green light to continue with a t-test, let’s go!\n\nesit %&gt;% \n  t.test(syllables~language, data = ., alternative = \"two.sided\") -&gt; result\nprint(result)\n\n\n    Welch Two Sample t-test\n\ndata:  syllables by language\nt = -6.34, df = 47.793, p-value = 7.695e-08\nalternative hypothesis: true difference in means between group Italian and group Spanish is not equal to 0\n95 percent confidence interval:\n -1.1748003 -0.6090245\nsample estimates:\nmean in group Italian mean in group Spanish \n             7.013432              7.905345 \n\nprint(result$stderr)\n\n[1] 0.1406801\n\n\nThe p-value suggests that the mean speech rate of Spanish speakers is different from the mean speech rate of Italian speakers.\n\nHow to report in paper\n“An independent two-tailed t-test compared the speech rate of Spanish speakers (n = 25) and Italian speakers (n = 25). We reject the null hypothesis and conclude that there is a signficant difference in speech rate between these two groups: t(47.793) = -6.34, p ≤ 0.001.”"
  },
  {
    "objectID": "lessons/t-test.html#activity",
    "href": "lessons/t-test.html#activity",
    "title": "t-test, Wilcoxon test",
    "section": "Activity",
    "text": "Activity\nYour turn! Use a dataset of your choice and see if there is a statistical difference between the means of a continuous variable based on two categories in a categorical variable."
  },
  {
    "objectID": "lessons/r_rstudio.html",
    "href": "lessons/r_rstudio.html",
    "title": "R and RStudio",
    "section": "",
    "text": "Students will become familiar with the R programming language a bit and the RStudio Integrated Development Environment (IDE)"
  },
  {
    "objectID": "lessons/r_rstudio.html#objective",
    "href": "lessons/r_rstudio.html#objective",
    "title": "R and RStudio",
    "section": "",
    "text": "Students will become familiar with the R programming language a bit and the RStudio Integrated Development Environment (IDE)"
  },
  {
    "objectID": "lessons/r_rstudio.html#the-r-programming-language",
    "href": "lessons/r_rstudio.html#the-r-programming-language",
    "title": "R and RStudio",
    "section": "The R programming language",
    "text": "The R programming language\n\nR (here) is a programming language specifically designed for statistical analysis and visualization.\nAs an open-source language, there are many third-party add-on packages (here) that extend the use of R that are available on the Comprehensive R Archive Network (aka. CRAN here).\nSo called “Task Views” (here) collect and briefly describe packages related to specific fields, including one for Natural Language Processing (here)."
  },
  {
    "objectID": "lessons/r_rstudio.html#rstudio-ide",
    "href": "lessons/r_rstudio.html#rstudio-ide",
    "title": "R and RStudio",
    "section": "RStudio IDE",
    "text": "RStudio IDE\n\nIntegrated Development Environments (IDEs) facilitate writing computer code.\nRStudio (here) from the company Posit (here) is the go-to IDE for R.\n\nHas many useful keyboard shortcuts, cf. Help &gt; Keyboard Shortcuts Help\n\nEarl’s favorites are:\n\nAssignment operator: ALT + -\nPipe operator in tidyverse: CTRL/CMD + SHIFT + m\n\n\nHas many cheat sheets for various tools within R and RStudio, cf. Help &gt; Cheat Sheets"
  },
  {
    "objectID": "lessons/r_rstudio.html#r-projects",
    "href": "lessons/r_rstudio.html#r-projects",
    "title": "R and RStudio",
    "section": "R projects",
    "text": "R projects\n\nR projects help keep things organized:\n\nData files, like CSV (.csv) and/or Excel (.xlsx) files\nSource code files, like R scripts (.r or .R files)\n\nTo create an R project:\n\nFile &gt; New Project..."
  },
  {
    "objectID": "lessons/r_rstudio.html#activity",
    "href": "lessons/r_rstudio.html#activity",
    "title": "R and RStudio",
    "section": "Activity",
    "text": "Activity\n\nStudents explore R and RStudio IDE, perhaps using the RStudio cheat sheet (Help &gt; Cheat Sheets &gt; RStudio IDE Cheat Sheet)"
  },
  {
    "objectID": "lessons/interaction.html",
    "href": "lessons/interaction.html",
    "title": "Interaction between explanatory variables",
    "section": "",
    "text": "In the CMS, there’s a fictitious dataset named “regionalisms.csv” in the LMS with the number of regionalisms (e.g., potato bug for rolly polly, coke for any soda, scallions for green onions, etc.) normalized to N per hour of speech, as spoken by 200 speakers grouped by age and sex. Download the CSV file to your harddrive and load it into R.\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nregion &lt;- read_csv(\"/Users/ekb5/Documents/LING_440/regionalisms.csv\")\n\nRows: 200 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): age, sex\ndbl (1): n\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "lessons/interaction.html#load-data",
    "href": "lessons/interaction.html#load-data",
    "title": "Interaction between explanatory variables",
    "section": "",
    "text": "In the CMS, there’s a fictitious dataset named “regionalisms.csv” in the LMS with the number of regionalisms (e.g., potato bug for rolly polly, coke for any soda, scallions for green onions, etc.) normalized to N per hour of speech, as spoken by 200 speakers grouped by age and sex. Download the CSV file to your harddrive and load it into R.\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nregion &lt;- read_csv(\"/Users/ekb5/Documents/LING_440/regionalisms.csv\")\n\nRows: 200 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): age, sex\ndbl (1): n\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "lessons/interaction.html#plot-age",
    "href": "lessons/interaction.html#plot-age",
    "title": "Interaction between explanatory variables",
    "section": "Plot age",
    "text": "Plot age\nWhat is the effect of age group on number of regionalisms spoken per hour? Let’s draw a boxplot and see.\n\np1 &lt;- region %&gt;% \n  ggplot(aes(x = age, y = n))+\n  geom_boxplot(notch = TRUE)+\n  stat_summary(fun = mean)+\n  theme_bw()\np1\n\nWarning: Removed 2 rows containing missing values (`geom_segment()`).\n\n\n\n\n\nWow! Looks like we have a serious effect from age group on number of regionalisms, with older speakers using more regionalisms."
  },
  {
    "objectID": "lessons/interaction.html#plot-sex",
    "href": "lessons/interaction.html#plot-sex",
    "title": "Interaction between explanatory variables",
    "section": "Plot sex",
    "text": "Plot sex\nNow let’s see the effect of biological sex on the number of regionalisms spoken per hour.\n\np2 &lt;- region %&gt;% \n  ggplot(aes(x = sex, y = n))+\n  geom_boxplot(notch = TRUE)+\n  stat_summary(fun = mean)+\n  theme_bw()\np2\n\nWarning: Removed 2 rows containing missing values (`geom_segment()`).\n\n\n\n\n\nAnother boxplot that seems to suggest that there is a big effect from sex on number of regionalisms per hour of speech, with men using more regionalisms than women."
  },
  {
    "objectID": "lessons/interaction.html#plot-age-and-sex-together",
    "href": "lessons/interaction.html#plot-age-and-sex-together",
    "title": "Interaction between explanatory variables",
    "section": "Plot age and sex together",
    "text": "Plot age and sex together\nWait! Hold up! Could it be more complicated than the above boxplots suggest? Could there be an interaction between age and sex, such that the effect of one variable is affected by the other variable? Let’s take a look at a faceted boxplot of age group and sex.\n\np3 &lt;- region %&gt;% \n  ggplot(aes(x = sex, y = n))+\n  geom_boxplot(notch = TRUE)+\n  stat_summary(fun = mean)+\n  facet_wrap(~age)+\n  theme_bw()\np3\n\nWarning: Removed 2 rows containing missing values (`geom_segment()`).\nRemoved 2 rows containing missing values (`geom_segment()`).\n\n\n\n\n\nOh! Now we see what’s going on. It’s not that older speakers in general produce more regionalisms than younger speaker in general, nor that men in general produce more regionalisms than women in general. We see that the effect is really only from older men who produce lots of regionalisms, while all other speakers use many fewer regionalisms. The takehome message: We have an interaction between age group and sex because the effect of one variable is affected by the other variable."
  },
  {
    "objectID": "lessons/interaction.html#only-main-effects",
    "href": "lessons/interaction.html#only-main-effects",
    "title": "Interaction between explanatory variables",
    "section": "Only main effects",
    "text": "Only main effects\nLet’s fit a linear regression with only the two main effects (i.e., age group and sex). Look at the adjusted \\(R^{2}\\) value and the AIC (Akaike information criterion).\n\nm1 &lt;- lm(n ~ age + sex, data = region)\nsummary(m1)\n\n\nCall:\nlm(formula = n ~ age + sex, data = region)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.505 -2.505 -0.145  2.855  7.945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  22.0550     0.4739  46.541  &lt; 2e-16 ***\nageyoung     -4.9100     0.5472  -8.973 2.29e-16 ***\nsexwomen     -4.5500     0.5472  -8.315 1.49e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.869 on 197 degrees of freedom\nMultiple R-squared:  0.4317,    Adjusted R-squared:  0.4259 \nF-statistic: 74.83 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\nAIC(m1)\n\n[1] 1113.78"
  },
  {
    "objectID": "lessons/interaction.html#with-interaction-term",
    "href": "lessons/interaction.html#with-interaction-term",
    "title": "Interaction between explanatory variables",
    "section": "With interaction term",
    "text": "With interaction term\nNow, let’s fit a linear regression with the two main effects (i.e., age group and sex) as well as a interaction term between the two main effect. Again, pay attention to the adjusted \\(R^{2}\\) value (the bigger the better with \\(R^{2}\\)) and the AIC value (the smaller the better with AIC), and compare them to the previous linear regression model.\n\nm2 &lt;- lm(n ~ age + sex + age:sex, data = region)\nsummary(m2)\n\n\nCall:\nlm(formula = n ~ age + sex + age:sex, data = region)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -5.38  -3.02   0.28   2.62   5.82 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        24.1800     0.4569  52.917   &lt;2e-16 ***\nageyoung           -9.1600     0.6462 -14.175   &lt;2e-16 ***\nsexwomen           -8.8000     0.6462 -13.618   &lt;2e-16 ***\nageyoung:sexwomen   8.5000     0.9139   9.301   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.231 on 196 degrees of freedom\nMultiple R-squared:  0.6057,    Adjusted R-squared:  0.5997 \nF-statistic: 100.4 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\nAIC(m2)\n\n[1] 1042.662\n\n\nOkay, now we’re getting somewhere! The \\(R^{2}\\) has increased and the AIC has decreased, both of which are good. That is, the model with an interaction term explains more of the variability in the response variable, and the estimator of the prediction error has gone down (i.e., AIC)."
  },
  {
    "objectID": "lessons/interaction.html#no-interaction",
    "href": "lessons/interaction.html#no-interaction",
    "title": "Interaction between explanatory variables",
    "section": "No interaction",
    "text": "No interaction\nFor comparison, if there were an monotonic effect from age group and another monotonic effect from sex, with no interaction effect between these two variables, the boxplot would look something like the following one.\n\nsize_per_group &lt;- 50\nold_men &lt;- tibble(\n  n_regionalisms = sample(seq(20, 30), size = size_per_group, replace = T),\n  age = sample(seq(35, 55), size = size_per_group, replace = T),\n  sex = rep(\"men\", size_per_group)\n)\nold_women &lt;- tibble(\n  n_regionalisms = sample(seq(15, 25), size = size_per_group, replace = T),\n  age = sample(seq(35, 55), size = size_per_group, replace = T),\n  sex = rep(\"women\", size_per_group)\n)\nyoung_men &lt;- tibble(\n  n_regionalisms = sample(seq(10, 20), size = size_per_group, replace = T),\n  age = sample(seq(18, 34), size = size_per_group, replace = T),\n  sex = rep(\"men\", size_per_group)\n)\nyoung_women &lt;- tibble(\n  n_regionalisms = sample(seq(5, 15), size = size_per_group, replace = T),\n  age = sample(seq(18, 34), size = size_per_group, replace = T),\n  sex = rep(\"women\", size_per_group)\n)\n  \nfake_data &lt;- tibble() %&gt;% \n  bind_rows(old_men) %&gt;% \n  bind_rows(old_women) %&gt;% \n  bind_rows(young_men) %&gt;% \n  bind_rows(young_women)\n\nfake_data &lt;- fake_data %&gt;% \n  mutate(age_group = ifelse(age &lt;= 34, \"young\", \"old\"))\n\np4 &lt;- fake_data %&gt;% \n  ggplot(aes(x = sex, y = n_regionalisms))+\n  geom_boxplot(notch = TRUE)+\n  stat_summary(fun = mean)+\n  facet_wrap(~age_group)+\n  theme_bw()\np4\n\nWarning: Removed 2 rows containing missing values (`geom_segment()`).\nRemoved 2 rows containing missing values (`geom_segment()`).\n\n\n\n\n\n\nm3 &lt;- lm(n_regionalisms ~ age_group + sex + age_group:sex, data = fake_data)\nsummary(m3)\n\n\nCall:\nlm(formula = n_regionalisms ~ age_group + sex + age_group:sex, \n    data = fake_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -5.36  -2.48  -0.36   2.52   5.52 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              24.6400     0.4314  57.121  &lt; 2e-16 ***\nage_groupyoung           -9.2800     0.6100 -15.212  &lt; 2e-16 ***\nsexwomen                 -5.1600     0.6100  -8.458 6.19e-15 ***\nage_groupyoung:sexwomen  -0.3400     0.8627  -0.394    0.694    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.05 on 196 degrees of freedom\nMultiple R-squared:  0.7635,    Adjusted R-squared:  0.7599 \nF-statistic: 210.9 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\nAIC(m3)\n\n[1] 1019.619\n\n\nThe interaction term was not selected as making a significant contribution to the prediction of the number of regionalisms in this second set of fictitious data."
  },
  {
    "objectID": "lessons/programming_basics.html",
    "href": "lessons/programming_basics.html",
    "title": "Programming basics",
    "section": "",
    "text": "Students will learn to code the basic building blocks of programming in R."
  },
  {
    "objectID": "lessons/programming_basics.html#objective",
    "href": "lessons/programming_basics.html#objective",
    "title": "Programming basics",
    "section": "",
    "text": "Students will learn to code the basic building blocks of programming in R."
  },
  {
    "objectID": "lessons/programming_basics.html#primitive-data-types-in-r",
    "href": "lessons/programming_basics.html#primitive-data-types-in-r",
    "title": "Programming basics",
    "section": "Primitive data types in R",
    "text": "Primitive data types in R\n\ninteger\n\nThis a whole number, i.e., there is no decimal component, for example “5”.\nTo specify an integer in R, type an uppercase “L” immediately to the right of the number (i.e., no space between the number and the “L”).\n\nE.g., 5L\n\n\nnumeric\n\nThis is a number with a decimal component, for example, “3.14”. Note: Unless an “L” is placed to the right of a whole number, R treats it as a numeric.\nTo specific a numeric in R, just the type the good ol’ fashioned number.\n\nE.g., 3.14\n\nNote: Dr. Brown may refer to this data type as a “float” because of language transfer from Python and Julia.\n\ncharacter\n\nE.g., \"hello world\" and c(\"hello\", \"hola\", \"hej\")\n\nNote: Dr. Brown will likely refer to this data type as “string” because of language transfer from Python and Julia.\nYou can extract part of a string with the sub_str() function (doc here).\n\n\nlogical\n\nThis data type has one of two values, either TRUE or FALSE (or T or F for shorthand).\nDr. Brown may refer to this data type as a “Boolean” because of language transfer from Python and Julia.\n\nThe class() function returns the data type of a variable or value.\n\nEg. class(5L) returns integer, while class(\"hello world\") returns character.\n\n\nActivity\n\nStudents use the class() function to become familiar with the data type of the values that they type."
  },
  {
    "objectID": "lessons/programming_basics.html#operators-in-r",
    "href": "lessons/programming_basics.html#operators-in-r",
    "title": "Programming basics",
    "section": "Operators in R",
    "text": "Operators in R\n\nAssignment operator: There are two assignment operators in R. The most common is &lt;- but = also works. For example:\n\nfruit &lt;- \"apple\"\nfruits &lt;- c(\"apple\", \"banana\", \"orange\", \"mango\")\n\nKeyboard shortcut in RStudio: ALT/OPT + -\n\npets = c(\"dog\", \"cat\", \"fish\", \"Madagascar hissing cockroach\")\nage = 46 (note: this creates a numeric rather than an integer; if an integer is wanted: age = 46L)\n\nInclusion operator: %in% tests for inclusion of a value in a collection of values (e.g., a vector), for example:\n\n\"apple\" %in% c(\"banana\", \"apple\", \"mango\") returns TRUE\n\"kiwi\" %in% c(\"banana\", \"apple\", \"mango\") returns FALSE\n\nEqual operator: == (i.e., two equal signs together with no space between them) tests whether the left-hand value and the right-hand value are identical, for example:\n\n\"mango\" == \"mango\" returns TRUE\n\"apple\" == \"manzana\" returns FALSE\n\"Hannah\" == \"HANNAH\" returns FALSE\n\nSuper important note: Computers treat lowercase and uppercase letters differently.\n\nThe equal operator can be used with a string on the left-hand side an a vector of strings on the right-hand side, for example:\n\n\"apple\" == c(\"banana\", \"apple\", \"mango\") returns FALSE TRUE FALSE\nQuick discussion: Speculate with a neighbor about the reason the above expression returns FALSE TRUE FALSE.\n\n\n\nActivity\n\nStudents use these three operators to create variables and vectors, and test for inclusion of a string in a vector of strings."
  },
  {
    "objectID": "lessons/programming_basics.html#comments",
    "href": "lessons/programming_basics.html#comments",
    "title": "Programming basics",
    "section": "Comments",
    "text": "Comments\n\nComments within computer code helps the human readers, whether other humans or your later self, to quickly understand what the various parts of a computer script do.\nComments in R are specified with a hashtag, for example:\n\n\n# assign a value to a variable\ndog &lt;- \"fido\"\n\n# create a vector of multiple elements\nkids &lt;- c(\"Bobby\", \"Luisa\", \"José\")"
  },
  {
    "objectID": "lessons/programming_basics.html#if-else-in-r",
    "href": "lessons/programming_basics.html#if-else-in-r",
    "title": "Programming basics",
    "section": "if else in R",
    "text": "if else in R\n\nThe logic is simple: Ask a question, and if the answer is TRUE, then do this thing, but if the answer is FALSE, then do that thing.\n\n\n\nTwo approaches to if else in R:\n\nThe most common approach is to use a code block. See an example in the Stack Overflow answer here.\nA less common approach, but super useful for simple if else cases, is to use a function:\n\nbase R ifelse() function here;\ndpylr (part of the tidyverse ecosystem) if_else() function here.\n\n\n\nActivity\n\nStudents create a string with a single word, and then use if else (either a code block of a function) to print to the user whether the word begins with a vowel or a consonant.\n\nHint 1: The print() and cat() can be used to print to the console.\nHint 2: The sub_str() function can be used to extract a sub part of a string.\nHint 3: The %in% operator tests whether the left-hand value is within the right-hand collection."
  },
  {
    "objectID": "lessons/programming_basics.html#loops-in-r",
    "href": "lessons/programming_basics.html#loops-in-r",
    "title": "Programming basics",
    "section": "Loops in R",
    "text": "Loops in R\n\nThe mighty and super useful for loop iterates over all elements of a collection (e.g., a vector), for example see below (and see another example here):\n\n\n# create a vector\nfruits &lt;- c(\"apple\", \"mango\", \"banana\", \"orange\")\n\n# loop over the elements of the vector\nfor (fruit in fruits) {\n  print(fruit)  # print the current element to the console\n}\n\n\nThe less-common-but-still-useful while loop tests the conditional statement at the beginning of each iteration and runs the body of the loop if the statement evaluates to TRUE. See an example here.\nUseful keywords for both for loops and while loops:\n\nThe next keyword skips the rest of the current iteration and continues to the next iteration. This is very much like continue in Python and Julia.\nThe break keyword stops the loop completely, regardless of which iteration it was in, and no further iteration are executed.\n\n\nActivity\n\nStudents create a for loop to iterate from 1 to 10, skipping even numbers and printing out odd numbers.\n\nHint: The modulus operator %% will be helpful (see here)."
  },
  {
    "objectID": "lessons/programming_basics.html#defining-functions-in-r",
    "href": "lessons/programming_basics.html#defining-functions-in-r",
    "title": "Programming basics",
    "section": "Defining functions in R",
    "text": "Defining functions in R\n\nA very useful ability in R (and all programming languages) is for a user to define their own custom function.\nThe function() function does the trick.\n\nSee a tutorial here.\n\n\nActivity\n\nStudents define a function that takes as input a word and returns as output a logical value (aka. Boolean value) indicating whether the word begins with one of the five orthographic vowels (i.e., a, e, i, o, u).\n\nHint: The %in% keyword will be helpful here.\n\nNow for a little fun and to put these basic programming skills together: Students define a function (likely with smaller helper functions) that translates a sentence from English into Pig Latin. A little refresher on Pig Latin: If a word begins with a vowel, the word yay is added to the end of it; if a word begins with a consonant or consonant cluster (e.g., ch, gr), that consonant or consonant cluster is moved to the end of the word followed by ay.\n\nHint: the stringr package (part of the tidyverse) will be useful here, especially the str_c(), str_sub(), str_split() (and unlist()) functions, as will the letters and LETTERS built-in constants.\nHint: After a good-faith effort, if you need help, see the script written by Dr. Brown by clicking on “▶ Code” below.\n\n\n\n\nCode\nsuppressPackageStartupMessages(library(\"tidyverse\"))\n\n# helper function 1, for vowel words\ntrans_v &lt;-  function(wd, vowels) {\n  return(str_c(wd, \"yay\"))\n}\n\n# helper function 2, for consonant words\ntrans_c &lt;- function(wd, vowels, first_let) {\n  second_let &lt;- str_sub(wd, 2, 2)\n  if (!str_to_lower(second_let) %in% vowels) {\n    first_two &lt;- str_sub(wd, 1, 2)\n    rest_wd &lt;- str_sub(wd, 3, str_length(wd))\n    return(str_c(rest_wd, first_two, \"ay\"))\n  } else {\n    rest_wd &lt;- str_sub(wd, 2, str_length(wd))\n    return(str_c(rest_wd, first_let, \"ay\"))\n  }\n}\n\n# the main function\ntrans_pig &lt;- function(sentence, vowels) {\n  wds &lt;- unlist(str_split(sentence, \"\\\\s+\"))\n  trans_sent &lt;- \"\"\n  for (wd in wds) {\n    first_let &lt;- str_sub(wd, 1, 1)\n    if (str_to_lower(first_let) %in% vowels) {\n      # this is a vowel word\n      trans_sent &lt;- str_c(trans_sent, trans_v(wd, vowels), \" \")\n    } else {\n      # the current word is a consonant word\n      trans_sent &lt;- str_c(trans_sent, trans_c(wd, vowels, first_let), \" \")\n    }\n  }\n  return(str_trim(trans_sent))\n}\n\n### test the function\nsentence &lt;- \"I do not like green eggs and ham.\"\nvowels &lt;- c(\"a\", \"e\", \"i\", \"o\", \"u\")\nprint(trans_pig(sentence, vowels))"
  },
  {
    "objectID": "lessons/quanteda.html",
    "href": "lessons/quanteda.html",
    "title": "quanteda",
    "section": "",
    "text": "Students will analyze textual data (aka. texts) with the quanteda R package."
  },
  {
    "objectID": "lessons/quanteda.html#objective",
    "href": "lessons/quanteda.html#objective",
    "title": "quanteda",
    "section": "",
    "text": "Students will analyze textual data (aka. texts) with the quanteda R package."
  },
  {
    "objectID": "lessons/quanteda.html#using-r-for-textual-analysis",
    "href": "lessons/quanteda.html#using-r-for-textual-analysis",
    "title": "quanteda",
    "section": "Using R for textual analysis",
    "text": "Using R for textual analysis\nThe quanteda R package is a super useful package for analyzing texts in R. Some of the techniques are corpus linguistic techniques through and through: tokenizing into words or sentences, keyword-in-context, removing stopwords. Other techniques might be better considered computational linguistic techniques: sentiment analysis, document feature matrix. Regardless, we need to get to know this package, as I can’t call myself a good professor of Linguistics Data Analysis with R without looking at this package."
  },
  {
    "objectID": "lessons/quanteda.html#install-the-package-and-its-friends",
    "href": "lessons/quanteda.html#install-the-package-and-its-friends",
    "title": "quanteda",
    "section": "Install the package and its friends",
    "text": "Install the package and its friends\nStep #1: Install the quanteda package and the other related package that the quanteda creators recommend:\n\ninstall.packages(c(\"quanteda\", \"quanteda.textmodels\", \"quanteda.textstats\", \"quanteda.textplots\", \"readtext\", \"spacyr\", \"remotes\"), repos = \"http://cran.rstudio.com\")\n\nThen, we need to install two packages on a Github:\n\nremotes::install_github(\"quanteda/quanteda.corpora\")\nremotes::install_github(\"kbenoit/quanteda.dictionaries\")"
  },
  {
    "objectID": "lessons/quanteda.html#create-a-corpus",
    "href": "lessons/quanteda.html#create-a-corpus",
    "title": "quanteda",
    "section": "Create a corpus",
    "text": "Create a corpus\nLet’s create a corpus in quanteda! First, let’s get their example code here working on our computers.\nLet’s read in the three currently published volumes of Saints. First, download the zipped file Saints.zip from the LMS. Then, unzip (aka. decompress or extract) the zipped file so that you end up with a directory with several subdirectories organized by volume number.\nLet’s read in the corpus such that we add a column with metadata about which volume each file is from. There are three ways (that Dr. Brown can think of) to do that.\n\nOption 1: Each volume individually (most verbose)\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# load each volume as a separate corpus\nsaints01 &lt;- readtext(\"Volume01/*.txt\") %&gt;% corpus()  # same as: saints01 &lt;- corpus(readtext(\"Volume01/*.txt\"))\ndocvars(saints01, \"Volume\") &lt;- \"01\"\n\nsaints02 &lt;- readtext(\"Volume02/*.txt\") %&gt;% corpus()\ndocvars(saints02, \"Volume\") &lt;- \"02\"\n\nsaints03 &lt;- readtext(\"Volume03/*.txt\") %&gt;% corpus()\ndocvars(saints03, \"Volume\") &lt;- \"03\"\n\n# combine the three corpora into one, with the docvar column identifying which volume each file comes from\nsaints &lt;- saints01 + saints02 + saints03\n\nprint(summary(saints))\n\n\n\nOption 2: Automate the previous approach (less verbose)\nWe can use the eval() and parse() functions in base R to automate the previous approach. Shall we?\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# our ol' friend the for loop \nfor (i in 1:3) {\n  to_str &lt;- str_glue(\"saints0{i} &lt;- readtext('Volume0{i}/*.txt') %&gt;% corpus(); docvars(saints0{i}, 'Volume') &lt;- '0{i}'\")\n  eval(parse(text = to_str))\n}\n\n# combine the three corpora into a new fourth one\nsaints &lt;- saints01 + saints02 + saints03\n\nprint(summary(saints))\n\n\n\nOption 3: Use the docvarfrom, dvsep, and docvarnames arguments (least verbose)\nThe readtext() function has a handful of arguments. The docvarfrom, dvsep, and the docvarnames arguments can create a new docvar (i.e., metadata about the corpus) from the filenames and/or the filepaths. Take a look at the documentation here.\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# one pipeline will do the trick!\nsaints &lt;- readtext(file = \"*/*.txt\", docvarsfrom = \"filepaths\", dvsep = \"/\", docvarnames = c(\"Volume\", \"filename\")) %&gt;% corpus()\n\nprint(summary(saints))\n\n\n\nActivity\nYou guessed it! It’s your turn. Create a corpus of your choice with files of your choice (e.g., perhaps from Project Gutenberg). Add a docvar of your choice, which may mean you need to preprocess the files a bit by specifying filenames or directory structure in a certain way."
  },
  {
    "objectID": "lessons/quanteda.html#creating-subcorpora",
    "href": "lessons/quanteda.html#creating-subcorpora",
    "title": "quanteda",
    "section": "Creating subcorpora",
    "text": "Creating subcorpora\nIt is super simple to create a subcorpus using a docvar (i.e., metadata about the files in the corpus). For example, let’s say we have our one Saints corpus with a docvar of Volume with values of “01”, “02”, and “03”. In order to create a subcorpus of just the files in Volume 1, we could run the following code. See the docs here.\n\nvol01 &lt;- corpus_subset(saints, Volume == \"01\") \n\n\nActivity\nGive it a try! Create a subcorpus from a larger corpus of your choice using the docvar of your choice."
  },
  {
    "objectID": "lessons/quanteda.html#keyword-in-context-kwic",
    "href": "lessons/quanteda.html#keyword-in-context-kwic",
    "title": "quanteda",
    "section": "Keyword-in-context (KWIC)",
    "text": "Keyword-in-context (KWIC)\nLet’s perform into a corpus linguistic technique: the keyword-in-context display or concordance lines or concordances. It is super easy to do so with the tokens() and kwic() functions. See example here.\nLet’s start out slowly with a simple word search using our Saints corpus:\n\n# Assuming packages are loaded and corpus has been created\n\n# tokenize and then get concordances\ntoks &lt;- tokens(saints)\nconcordances &lt;- kwic(toks, pattern = \"tree\")\nprint(concordances)\n\nLet’s ramp it up with some regular expression pizzazz. Question: What does the regular expression find?\n\ntoks &lt;- tokens(saints)\nconcordances &lt;- kwic(toks, pattern = \"\\\\w+(\\\\w{2,})\\\\W+\\\\w+\\\\1\\\\b\", valuetype = \"regex\")\nprint(concordances)\n\nThe main tokenizer function tokens() has a handful of arguments that are worth inspecting. Take a gander here. Also, there many other tokenizer functions that can be useful. Take a look here.\n\nActivity\nGet to know the tokenizing functions and the kwic() function (example here and docs here) with the corpus of your choice."
  },
  {
    "objectID": "lessons/quanteda.html#searching-with-a-dictionary",
    "href": "lessons/quanteda.html#searching-with-a-dictionary",
    "title": "quanteda",
    "section": "Searching with a dictionary",
    "text": "Searching with a dictionary\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# one pipeline will do the trick!\nsaints &lt;- readtext(file = \"*/*.txt\", docvarsfrom = \"filepaths\", dvsep = \"/\", docvarnames = c(\"Volume\", \"filename\")) %&gt;% corpus()\n\n# create a dictionary with named vectors\ndict &lt;- dictionary(list(temple = c(\"temple\", \"sealing\", \"spire\", \"open house\"),\n                missionary = c(\"missionary\", \"mission\", \"labor\\\\b\")))\n\n# search with the dictionary\nkwic(tokens(saints), pattern = dict, valuetype = \"regex\") %&gt;% print()"
  },
  {
    "objectID": "lessons/quanteda.html#collocations",
    "href": "lessons/quanteda.html#collocations",
    "title": "quanteda",
    "section": "Collocations",
    "text": "Collocations\nYou can retrieve collocations with the textstat_collocations() function here. At the time of making this lesson plan, it only calculates the lambda metric proposed by Blaheta and Johnson (2001):\nBlaheta, D. & Johnson, M. (2001). Unsupervised learning of multi-word verbs. Presented at the ACLEACL Workshop on the Computational Extraction, Analysis and Exploitation of Collocations.\nHowever, the doc page says that there are plans to add more measures. Hopefully log-dice is in the works, as it’s a great word association metric.\nLet’s retrieve the collocations in the first three volumes of Saints:\n\nlibrary(quanteda)\nlibrary(readtext)\nlibrary(tidyverse)\nlibrary(quanteda.textstats)\n\n# change directories into the Saints directories\nsetwd(\"/pathway/to/Saints/txt\")\n\n# load the Saints corpus, creating a docvar with the Volume that each file belongs to\nsaints &lt;- readtext(file = \"*/*.txt\", docvarsfrom = \"filepaths\", dvsep = \"/\", docvarnames = c(\"Volume\", \"filename\")) %&gt;%  corpus() \n# retrieve collocations\nsaints %&gt;% \n  tokens() %&gt;% \n  textstat_collocations()  %&gt;% \n  arrange(desc(lambda)) %&gt;%  # arrange them in descending order by lambda\n  head(50)  # show only first 50 collocations\n\nThat’s okay, but there are lots of proper nouns and low-frequency collocations. We can remove those with the min_count argument: textstat_collocations(min_count = 25).\nIf we want, we can also remove function words, which have little semantic value, with a stopword list:\n\nlibrary(\"stopwords\")\nsaints %&gt;% \n  tokens() %&gt;% \n  tokens_remove(stopwords(\"en\")) %&gt;% \n  textstat_collocations(min_count = 25) %&gt;% \n  arrange(desc(lambda)) %&gt;% \n  head(50)\n\n\nActivity\nGo for it! That is, retrieve collocations in a corpus of your choice. Look at and possibly use some of the other arguments in the textstat_collocations() function. Reminder: In order to pull up the doc page of a function within the RStudio IDE, you can type a question mark and then the name of the function (no space between them) in the console, e.g., ?textstat_collocations. And here’s that same doc page on the internet."
  },
  {
    "objectID": "lessons/quanteda.html#document-feature-matrix",
    "href": "lessons/quanteda.html#document-feature-matrix",
    "title": "quanteda",
    "section": "Document-feature matrix",
    "text": "Document-feature matrix\nWithin quanteda, a document-feature matrix is very similar to a document-term matrix (if you know what that is). It’s a tabular dataset (hence the name “matrix”) and has the name of the files (aka. documents) as rows, and the features (i.e., words and punctuation) are the columns. Here’s an example. Let’s take a look with the Saints corpus:\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"readtext\")\nsetwd(\"/pathway/to/Saints/txt/\")\n\n# one pipeline will do the trick!\nsaints &lt;- readtext(file = \"*/*.txt\", docvarsfrom = \"filepaths\", dvsep = \"/\", docvarnames = c(\"Volume\", \"filename\")) %&gt;% corpus()\n\nsaints %&gt;% \n  tokens() %&gt;% \n  dfm()\n\nWe can use a dictionary of patterns to limit the number of features (i.e., columns) that are returned in the matrix:\n\ndict &lt;- dictionary(list(\n  temple = c(\"temple\", \"sealing\", \"spire\", \"open house\"),\n  missionary = c(\"missionary\", \"mission\", \"labor\"))\n  )\n\nsaints %&gt;% \n  tokens() %&gt;% \n  tokens_lookup(dictionary = dict) %&gt;% \n  dfm() %&gt;% \n  as_tibble() %&gt;%  # cast to tibble in order to use arrange below\n  arrange(desc(temple))"
  },
  {
    "objectID": "lessons/quanteda.html#similarities-between-texts",
    "href": "lessons/quanteda.html#similarities-between-texts",
    "title": "quanteda",
    "section": "Similarities between texts",
    "text": "Similarities between texts\nYou can plot similarities between texts in a corpus. Let’s run the example code in the Quick Guide here.\nNow, let’s make this work with our Saints corpus."
  },
  {
    "objectID": "lessons/quanteda.html#keyness-analysis",
    "href": "lessons/quanteda.html#keyness-analysis",
    "title": "quanteda",
    "section": "Keyness analysis",
    "text": "Keyness analysis\nWe can use quanteda to perform keyness analysis, that is, to identify the keywords that typify a section of the corpus from the rest of the corpus.\nLet’s get the creators’ example working in the doc page (in console ?textstat_keyness) and here.\n\nActivity\nTake some time to look over the other analysis abilities of quanteda and try to get one or two working with a corpus of your choice."
  }
]